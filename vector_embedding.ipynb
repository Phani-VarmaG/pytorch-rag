{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to your MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"myDatabase\"]\n",
    "collection = db[\"myCollection\"]\n",
    "\n",
    "# Fetch your scraped documents (assume stored under 'content' field)\n",
    "documents = [doc['page_text'] for doc in collection.find()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Chunk and Preprocess the data\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "docs = splitter.create_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Buffer\\n¶\\nclass\\ntorch.nn.parameter.\\nBuffer\\n(\\ndata\\n=\\nNone\\n,\\n*\\n,\\npersistent\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nA kind of Tensor that should not be considered a model\\nparameter. For example, BatchNorm’s\\nrunning_mean\\nis not a parameter, but is part of the module’s state.\\nBuffers are\\nTensor\\nsubclasses, that have a\\nvery special property when used with\\nModule\\ns – when they’re\\nassigned as Module attributes they are automatically added to the list of\\nits buffers, and will appear e.g. in\\nbuffers()\\niterator.'),\n",
       " Document(metadata={}, page_content='buffers()\\niterator.\\nAssigning a Tensor doesn’t have such effect. One can still assign a Tensor as explicitly by using\\nthe\\nregister_buffer()\\nfunction.\\nParameters\\ndata\\n(\\nTensor\\n) – buffer tensor.\\npersistent\\n(\\nbool\\n,\\noptional\\n) – whether the buffer is part of the module’s\\nstate_dict\\n. Default:\\nTrue\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Parameter\\n¶\\nclass\\ntorch.nn.parameter.\\nParameter\\n(\\ndata\\n=\\nNone\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nA kind of Tensor that is to be considered a module parameter.\\nParameters are\\nTensor\\nsubclasses, that have a\\nvery special property when used with\\nModule\\ns - when they’re\\nassigned as Module attributes they are automatically added to the list of\\nits parameters, and will appear e.g. in\\nparameters()\\niterator.\\nAssigning a Tensor doesn’t have such effect. This is because one might'),\n",
       " Document(metadata={}, page_content='want to cache some temporary state, like last hidden state of the RNN, in\\nthe model. If there was no such class as\\nParameter\\n, these\\ntemporaries would get registered too.\\nParameters\\ndata\\n(\\nTensor\\n) – parameter tensor.\\nrequires_grad\\n(\\nbool\\n,\\noptional\\n) – if the parameter requires gradient. Note that\\nthe torch.no_grad() context does NOT affect the default behavior of\\nParameter creation–the Parameter will still have\\nrequires_grad=True\\nin\\nno_grad\\nmode. See\\nLocally disabling gradient computation'),\n",
       " Document(metadata={}, page_content='mode. See\\nLocally disabling gradient computation\\nfor more\\ndetails. Default:\\nTrue\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='UninitializedParameter\\n¶\\nclass\\ntorch.nn.parameter.\\nUninitializedParameter\\n(\\nrequires_grad\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA parameter that is not initialized.\\nUninitialized Parameters are a special case of\\ntorch.nn.Parameter\\nwhere the shape of the data is still unknown.\\nUnlike a\\ntorch.nn.Parameter\\n, uninitialized parameters\\nhold no data and attempting to access some properties, like their shape,'),\n",
       " Document(metadata={}, page_content=\"will throw a runtime error. The only operations that can be performed on a uninitialized\\nparameter are changing its datatype, moving it to a different device and\\nconverting it to a regular\\ntorch.nn.Parameter\\n.\\nThe default device or dtype to use when the parameter is materialized can be set\\nduring construction using e.g.\\ndevice='cuda'\\n.\\ncls_to_become\\n[source]\\n¶\\nalias of\\nParameter\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.\"),\n",
       " Document(metadata={}, page_content='UninitializedBuffer\\n¶\\nclass\\ntorch.nn.parameter.\\nUninitializedBuffer\\n(\\nrequires_grad\\n=\\nFalse\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n,\\npersistent\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nA buffer that is not initialized.\\nUninitialized Buffer is a a special case of\\ntorch.Tensor\\nwhere the shape of the data is still unknown.\\nUnlike a\\ntorch.Tensor\\n, uninitialized parameters\\nhold no data and attempting to access some properties, like their shape,'),\n",
       " Document(metadata={}, page_content=\"will throw a runtime error. The only operations that can be performed on a uninitialized\\nparameter are changing its datatype, moving it to a different device and\\nconverting it to a regular\\ntorch.Tensor\\n.\\nThe default device or dtype to use when the buffer is materialized can be set\\nduring construction using e.g.\\ndevice='cuda'\\n.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.\"),\n",
       " Document(metadata={}, page_content='Module\\n¶\\nclass\\ntorch.nn.\\nModule\\n(\\n*\\nargs\\n,\\n**\\nkwargs\\n)\\n[source]\\n[source]\\n¶\\nBase class for all neural network modules.\\nYour models should also subclass this class.\\nModules can also contain other Modules, allowing them to be nested in\\na tree structure. You can assign the submodules as regular attributes:\\nimport\\ntorch.nn\\nas\\nnn\\nimport\\ntorch.nn.functional\\nas\\nF\\nclass\\nModel\\n(\\nnn\\n.\\nModule\\n):\\ndef\\n__init__\\n(\\nself\\n)\\n->\\nNone\\n:\\nsuper\\n()\\n.\\n__init__\\n()\\nself\\n.\\nconv1\\n=\\nnn\\n.\\nConv2d\\n(\\n1\\n,\\n20\\n,\\n5\\n)\\nself\\n.\\nconv2\\n='),\n",
       " Document(metadata={}, page_content='conv1\\n=\\nnn\\n.\\nConv2d\\n(\\n1\\n,\\n20\\n,\\n5\\n)\\nself\\n.\\nconv2\\n=\\nnn\\n.\\nConv2d\\n(\\n20\\n,\\n20\\n,\\n5\\n)\\ndef\\nforward\\n(\\nself\\n,\\nx\\n):\\nx\\n=\\nF\\n.\\nrelu\\n(\\nself\\n.\\nconv1\\n(\\nx\\n))\\nreturn\\nF\\n.\\nrelu\\n(\\nself\\n.\\nconv2\\n(\\nx\\n))\\nSubmodules assigned in this way will be registered, and will also have their\\nparameters converted when you call\\nto()\\n, etc.\\nNote\\nAs per the example above, an\\n__init__()\\ncall to the parent class\\nmust be made before assignment on the child.\\nVariables\\ntraining\\n(\\nbool'),\n",
       " Document(metadata={}, page_content='Variables\\ntraining\\n(\\nbool\\n) – Boolean represents whether this module is in training or\\nevaluation mode.\\nadd_module\\n(\\nname\\n,\\nmodule\\n)\\n[source]\\n[source]\\n¶\\nAdd a child module to the current module.\\nThe module can be accessed as an attribute using the given name.\\nParameters\\nname\\n(\\nstr\\n) – name of the child module. The child module can be\\naccessed from this module using the given name\\nmodule\\n(\\nModule\\n) – child module to be added to the module.\\napply\\n(\\nfn\\n)\\n[source]\\n[source]\\n¶\\nApply\\nfn'),\n",
       " Document(metadata={}, page_content='apply\\n(\\nfn\\n)\\n[source]\\n[source]\\n¶\\nApply\\nfn\\nrecursively to every submodule (as returned by\\n.children()\\n) as well as self.\\nTypical use includes initializing the parameters of a model\\n(see also\\ntorch.nn.init\\n).\\nParameters\\nfn\\n(\\nModule\\n-> None) – function to be applied to each submodule\\nReturns\\nself\\nReturn type\\nModule\\nExample:\\n>>>\\n@torch\\n.\\nno_grad\\n()\\n>>>\\ndef\\ninit_weights\\n(\\nm\\n):\\n>>>\\nprint\\n(\\nm\\n)\\n>>>\\nif\\ntype\\n(\\nm\\n)\\n==\\nnn\\n.\\nLinear\\n:\\n>>>\\nm\\n.\\nweight\\n.\\nfill_\\n(\\n1.0\\n)\\n>>>\\nprint\\n(\\nm\\n.\\nweight\\n)\\n>>>\\nnet\\n=\\nnn\\n.'),\n",
       " Document(metadata={}, page_content='(\\n1.0\\n)\\n>>>\\nprint\\n(\\nm\\n.\\nweight\\n)\\n>>>\\nnet\\n=\\nnn\\n.\\nSequential\\n(\\nnn\\n.\\nLinear\\n(\\n2\\n,\\n2\\n),\\nnn\\n.\\nLinear\\n(\\n2\\n,\\n2\\n))\\n>>>\\nnet\\n.\\napply\\n(\\ninit_weights\\n)\\nLinear(in_features=2, out_features=2, bias=True)\\nParameter containing:\\ntensor([[1., 1.],\\n[1., 1.]], requires_grad=True)\\nLinear(in_features=2, out_features=2, bias=True)\\nParameter containing:\\ntensor([[1., 1.],\\n[1., 1.]], requires_grad=True)\\nSequential(\\n(0): Linear(in_features=2, out_features=2, bias=True)\\n(1): Linear(in_features=2, out_features=2, bias=True)'),\n",
       " Document(metadata={}, page_content=')\\nbfloat16\\n(\\n)\\n[source]\\n[source]\\n¶\\nCasts all floating point parameters and buffers to\\nbfloat16\\ndatatype.\\nNote\\nThis method modifies the module in-place.\\nReturns\\nself\\nReturn type\\nModule\\nbuffers\\n(\\nrecurse\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterator over module buffers.\\nParameters\\nrecurse\\n(\\nbool\\n) – if True, then yields buffers of this module\\nand all submodules. Otherwise, yields only buffers that\\nare direct members of this module.\\nYields\\ntorch.Tensor\\n– module buffer\\nReturn type\\nIterator\\n['),\n",
       " Document(metadata={}, page_content=\"– module buffer\\nReturn type\\nIterator\\n[\\nTensor\\n]\\nExample:\\n>>>\\nfor\\nbuf\\nin\\nmodel\\n.\\nbuffers\\n():\\n>>>\\nprint\\n(\\ntype\\n(\\nbuf\\n),\\nbuf\\n.\\nsize\\n())\\n<class 'torch.Tensor'> (20L,)\\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\\nchildren\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterator over immediate children modules.\\nYields\\nModule\\n– a child module\\nReturn type\\nIterator\\n[\\nModule\\n]\\ncompile\\n(\\n*\\nargs\\n,\\n**\\nkwargs\\n)\\n[source]\\n[source]\\n¶\\nCompile this Module’s forward using\\ntorch.compile()\\n.\\nThis Module’s\\n__call__\"),\n",
       " Document(metadata={}, page_content='torch.compile()\\n.\\nThis Module’s\\n__call__\\nmethod is compiled and all arguments are passed as-is\\nto\\ntorch.compile()\\n.\\nSee\\ntorch.compile()\\nfor details on the arguments for this function.\\ncpu\\n(\\n)\\n[source]\\n[source]\\n¶\\nMove all model parameters and buffers to the CPU.\\nNote\\nThis method modifies the module in-place.\\nReturns\\nself\\nReturn type\\nModule\\ncuda\\n(\\ndevice\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nMove all model parameters and buffers to the GPU.'),\n",
       " Document(metadata={}, page_content='Move all model parameters and buffers to the GPU.\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on GPU while being optimized.\\nNote\\nThis method modifies the module in-place.\\nParameters\\ndevice\\n(\\nint\\n,\\noptional\\n) – if specified, all parameters will be\\ncopied to that device\\nReturns\\nself\\nReturn type\\nModule\\ndouble\\n(\\n)\\n[source]\\n[source]\\n¶\\nCasts all floating point parameters and buffers to\\ndouble'),\n",
       " Document(metadata={}, page_content='double\\ndatatype.\\nNote\\nThis method modifies the module in-place.\\nReturns\\nself\\nReturn type\\nModule\\neval\\n(\\n)\\n[source]\\n[source]\\n¶\\nSet the module in evaluation mode.\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e. whether they are affected, e.g.\\nDropout\\n,\\nBatchNorm\\n,\\netc.\\nThis is equivalent with\\nself.train(False)\\n.\\nSee\\nLocally disabling gradient computation\\nfor a comparison between\\n.eval()'),\n",
       " Document(metadata={}, page_content='for a comparison between\\n.eval()\\nand several similar mechanisms that may be confused with it.\\nReturns\\nself\\nReturn type\\nModule\\nextra_repr\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn the extra representation of the module.\\nTo print customized extra information, you should re-implement\\nthis method in your own modules. Both single-line and multi-line\\nstrings are acceptable.\\nReturn type\\nstr\\nfloat\\n(\\n)\\n[source]\\n[source]\\n¶\\nCasts all floating point parameters and buffers to\\nfloat\\ndatatype.\\nNote'),\n",
       " Document(metadata={}, page_content='float\\ndatatype.\\nNote\\nThis method modifies the module in-place.\\nReturns\\nself\\nReturn type\\nModule\\nforward\\n(\\n*\\ninput\\n)\\n[source]\\n¶\\nDefine the computation performed at every call.\\nShould be overridden by all subclasses.\\nNote\\nAlthough the recipe for forward pass needs to be defined within\\nthis function, one should call the\\nModule\\ninstance afterwards\\ninstead of this since the former takes care of running the\\nregistered hooks while the latter silently ignores them.\\nget_buffer\\n(\\ntarget\\n)\\n[source]'),\n",
       " Document(metadata={}, page_content='get_buffer\\n(\\ntarget\\n)\\n[source]\\n[source]\\n¶\\nReturn the buffer given by\\ntarget\\nif it exists, otherwise throw an error.\\nSee the docstring for\\nget_submodule\\nfor a more detailed\\nexplanation of this method’s functionality as well as how to\\ncorrectly specify\\ntarget\\n.\\nParameters\\ntarget\\n(\\nstr\\n) – The fully-qualified string name of the buffer\\nto look for. (See\\nget_submodule\\nfor how to specify a\\nfully-qualified string.)\\nReturns\\nThe buffer referenced by\\ntarget\\nReturn type\\ntorch.Tensor\\nRaises\\nAttributeError'),\n",
       " Document(metadata={}, page_content='Return type\\ntorch.Tensor\\nRaises\\nAttributeError\\n– If the target string references an invalid\\n    path or resolves to something that is not a\\n    buffer\\nget_extra_state\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn any extra state to include in the module’s state_dict.\\nImplement this and a corresponding\\nset_extra_state()\\nfor your module\\nif you need to store extra state. This function is called when building the\\nmodule’s\\nstate_dict()\\n.\\nNote that extra state should be picklable to ensure working serialization'),\n",
       " Document(metadata={}, page_content='of the state_dict. We only provide backwards compatibility guarantees\\nfor serializing Tensors; other objects may break backwards compatibility if\\ntheir serialized pickled form changes.\\nReturns\\nAny extra state to store in the module’s state_dict\\nReturn type\\nobject\\nget_parameter\\n(\\ntarget\\n)\\n[source]\\n[source]\\n¶\\nReturn the parameter given by\\ntarget\\nif it exists, otherwise throw an error.\\nSee the docstring for\\nget_submodule\\nfor a more detailed'),\n",
       " Document(metadata={}, page_content='get_submodule\\nfor a more detailed\\nexplanation of this method’s functionality as well as how to\\ncorrectly specify\\ntarget\\n.\\nParameters\\ntarget\\n(\\nstr\\n) – The fully-qualified string name of the Parameter\\nto look for. (See\\nget_submodule\\nfor how to specify a\\nfully-qualified string.)\\nReturns\\nThe Parameter referenced by\\ntarget\\nReturn type\\ntorch.nn.Parameter\\nRaises\\nAttributeError\\n– If the target string references an invalid\\n    path or resolves to something that is not an\\nnn.Parameter\\nget_submodule\\n('),\n",
       " Document(metadata={}, page_content='nn.Parameter\\nget_submodule\\n(\\ntarget\\n)\\n[source]\\n[source]\\n¶\\nReturn the submodule given by\\ntarget\\nif it exists, otherwise throw an error.\\nFor example, let’s say you have an\\nnn.Module\\nA\\nthat\\nlooks like this:\\nA(\\n    (net_b): Module(\\n        (net_c): Module(\\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n        )\\n        (linear): Linear(in_features=100, out_features=200, bias=True)\\n    )\\n)\\n(The diagram shows an\\nnn.Module\\nA\\n.\\nA\\nwhich has a nested\\nsubmodule\\nnet_b'),\n",
       " Document(metadata={}, page_content='A\\n.\\nA\\nwhich has a nested\\nsubmodule\\nnet_b\\n, which itself has two submodules\\nnet_c\\nand\\nlinear\\n.\\nnet_c\\nthen has a submodule\\nconv\\n.)\\nTo check whether or not we have the\\nlinear\\nsubmodule, we\\nwould call\\nget_submodule(\"net_b.linear\")\\n. To check whether\\nwe have the\\nconv\\nsubmodule, we would call\\nget_submodule(\"net_b.net_c.conv\")\\n.\\nThe runtime of\\nget_submodule\\nis bounded by the degree\\nof module nesting in\\ntarget\\n. A query against\\nnamed_modules\\nachieves the same result, but it is O(N) in'),\n",
       " Document(metadata={}, page_content='achieves the same result, but it is O(N) in\\nthe number of transitive modules. So, for a simple check to see\\nif some submodule exists,\\nget_submodule\\nshould always be\\nused.\\nParameters\\ntarget\\n(\\nstr\\n) – The fully-qualified string name of the submodule\\nto look for. (See above example for how to specify a\\nfully-qualified string.)\\nReturns\\nThe submodule referenced by\\ntarget\\nReturn type\\ntorch.nn.Module\\nRaises\\nAttributeError\\n– If the target string references an invalid'),\n",
       " Document(metadata={}, page_content='– If the target string references an invalid\\n    path or resolves to something that is not an\\nnn.Module\\nhalf\\n(\\n)\\n[source]\\n[source]\\n¶\\nCasts all floating point parameters and buffers to\\nhalf\\ndatatype.\\nNote\\nThis method modifies the module in-place.\\nReturns\\nself\\nReturn type\\nModule\\nipu\\n(\\ndevice\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nMove all model parameters and buffers to the IPU.\\nThis also makes associated parameters and buffers different objects. So'),\n",
       " Document(metadata={}, page_content='it should be called before constructing the optimizer if the module will\\nlive on IPU while being optimized.\\nNote\\nThis method modifies the module in-place.\\nParameters\\ndevice\\n(\\nint\\n,\\noptional\\n) – if specified, all parameters will be\\ncopied to that device\\nReturns\\nself\\nReturn type\\nModule\\nload_state_dict\\n(\\nstate_dict\\n,\\nstrict\\n=\\nTrue\\n,\\nassign\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nCopy parameters and buffers from\\nstate_dict\\ninto this module and its descendants.\\nIf\\nstrict\\nis\\nTrue\\n, then\\nthe keys of\\nstate_dict'),\n",
       " Document(metadata={}, page_content='If\\nstrict\\nis\\nTrue\\n, then\\nthe keys of\\nstate_dict\\nmust exactly match the keys returned\\nby this module’s\\nstate_dict()\\nfunction.\\nWarning\\nIf\\nassign\\nis\\nTrue\\nthe optimizer must be created after\\nthe call to\\nload_state_dict\\nunless\\nget_swap_module_params_on_conversion()\\nis\\nTrue\\n.\\nParameters\\nstate_dict\\n(\\ndict\\n) – a dict containing parameters and\\npersistent buffers.\\nstrict\\n(\\nbool\\n,\\noptional\\n) – whether to strictly enforce that the keys\\nin\\nstate_dict\\nmatch the keys returned by this module’s\\nstate_dict()'),\n",
       " Document(metadata={}, page_content='state_dict()\\nfunction. Default:\\nTrue\\nassign\\n(\\nbool\\n,\\noptional\\n) – When set to\\nFalse\\n, the properties of the tensors\\nin the current module are preserved whereas setting it to\\nTrue\\npreserves\\nproperties of the Tensors in the state dict. The only\\nexception is the\\nrequires_grad\\nfield of\\nDefault:\\n``False`\\nReturns\\nmissing_keys\\nis a list of str containing any keys that are expected\\nby this module but missing from the provided\\nstate_dict\\n.\\nunexpected_keys'),\n",
       " Document(metadata={}, page_content='state_dict\\n.\\nunexpected_keys\\nis a list of str containing the keys that are not\\nexpected by this module but present in the provided\\nstate_dict\\n.\\nReturn type\\nNamedTuple\\nwith\\nmissing_keys\\nand\\nunexpected_keys\\nfields\\nNote\\nIf a parameter or buffer is registered as\\nNone\\nand its corresponding key\\nexists in\\nstate_dict\\n,\\nload_state_dict()\\nwill raise a\\nRuntimeError\\n.\\nmodules\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterator over all modules in the network.\\nYields\\nModule\\n– a module in the network\\nReturn type'),\n",
       " Document(metadata={}, page_content=\"Module\\n– a module in the network\\nReturn type\\nIterator\\n[\\nModule\\n]\\nNote\\nDuplicate modules are returned only once. In the following\\nexample,\\nl\\nwill be returned only once.\\nExample:\\n>>>\\nl\\n=\\nnn\\n.\\nLinear\\n(\\n2\\n,\\n2\\n)\\n>>>\\nnet\\n=\\nnn\\n.\\nSequential\\n(\\nl\\n,\\nl\\n)\\n>>>\\nfor\\nidx\\n,\\nm\\nin\\nenumerate\\n(\\nnet\\n.\\nmodules\\n()):\\n...\\nprint\\n(\\nidx\\n,\\n'->'\\n,\\nm\\n)\\n0 -> Sequential(\\n(0): Linear(in_features=2, out_features=2, bias=True)\\n(1): Linear(in_features=2, out_features=2, bias=True)\\n)\"),\n",
       " Document(metadata={}, page_content=')\\n1 -> Linear(in_features=2, out_features=2, bias=True)\\nmtia\\n(\\ndevice\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nMove all model parameters and buffers to the MTIA.\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing the optimizer if the module will\\nlive on MTIA while being optimized.\\nNote\\nThis method modifies the module in-place.\\nParameters\\ndevice\\n(\\nint\\n,\\noptional\\n) – if specified, all parameters will be\\ncopied to that device\\nReturns\\nself'),\n",
       " Document(metadata={}, page_content=\"copied to that device\\nReturns\\nself\\nReturn type\\nModule\\nnamed_buffers\\n(\\nprefix\\n=\\n''\\n,\\nrecurse\\n=\\nTrue\\n,\\nremove_duplicate\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\\nParameters\\nprefix\\n(\\nstr\\n) – prefix to prepend to all buffer names.\\nrecurse\\n(\\nbool\\n,\\noptional\\n) – if True, then yields buffers of this module\\nand all submodules. Otherwise, yields only buffers that\"),\n",
       " Document(metadata={}, page_content=\"are direct members of this module. Defaults to True.\\nremove_duplicate\\n(\\nbool\\n,\\noptional\\n) – whether to remove the duplicated buffers in the result. Defaults to True.\\nYields\\n(str, torch.Tensor)\\n– Tuple containing the name and buffer\\nReturn type\\nIterator\\n[\\nTuple\\n[\\nstr\\n,\\nTensor\\n]]\\nExample:\\n>>>\\nfor\\nname\\n,\\nbuf\\nin\\nself\\n.\\nnamed_buffers\\n():\\n>>>\\nif\\nname\\nin\\n[\\n'running_var'\\n]:\\n>>>\\nprint\\n(\\nbuf\\n.\\nsize\\n())\\nnamed_children\\n(\\n)\\n[source]\\n[source]\\n¶\"),\n",
       " Document(metadata={}, page_content=\".\\nsize\\n())\\nnamed_children\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\\nYields\\n(str, Module)\\n– Tuple containing a name and child module\\nReturn type\\nIterator\\n[\\nTuple\\n[\\nstr\\n,\\nModule\\n]]\\nExample:\\n>>>\\nfor\\nname\\n,\\nmodule\\nin\\nmodel\\n.\\nnamed_children\\n():\\n>>>\\nif\\nname\\nin\\n[\\n'conv4'\\n,\\n'conv5'\\n]:\\n>>>\\nprint\\n(\\nmodule\\n)\\nnamed_modules\\n(\\nmemo\\n=\\nNone\\n,\\nprefix\\n=\\n''\\n,\\nremove_duplicate\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\"),\n",
       " Document(metadata={}, page_content=',\\nremove_duplicate\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\\nParameters\\nmemo\\n(\\nOptional\\n[\\nSet\\n[\\nModule\\n]\\n]\\n) – a memo to store the set of modules already added to the result\\nprefix\\n(\\nstr\\n) – a prefix that will be added to the name of the module\\nremove_duplicate\\n(\\nbool\\n) – whether to remove the duplicated module instances in the result\\nor not\\nYields\\n(str, Module)\\n– Tuple of name and module'),\n",
       " Document(metadata={}, page_content=\"Yields\\n(str, Module)\\n– Tuple of name and module\\nNote\\nDuplicate modules are returned only once. In the following\\nexample,\\nl\\nwill be returned only once.\\nExample:\\n>>>\\nl\\n=\\nnn\\n.\\nLinear\\n(\\n2\\n,\\n2\\n)\\n>>>\\nnet\\n=\\nnn\\n.\\nSequential\\n(\\nl\\n,\\nl\\n)\\n>>>\\nfor\\nidx\\n,\\nm\\nin\\nenumerate\\n(\\nnet\\n.\\nnamed_modules\\n()):\\n...\\nprint\\n(\\nidx\\n,\\n'->'\\n,\\nm\\n)\\n0 -> ('', Sequential(\\n(0): Linear(in_features=2, out_features=2, bias=True)\\n(1): Linear(in_features=2, out_features=2, bias=True)\\n))\"),\n",
       " Document(metadata={}, page_content=\"))\\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\\nnamed_parameters\\n(\\nprefix\\n=\\n''\\n,\\nrecurse\\n=\\nTrue\\n,\\nremove_duplicate\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\\nParameters\\nprefix\\n(\\nstr\\n) – prefix to prepend to all parameter names.\\nrecurse\\n(\\nbool\\n) – if True, then yields parameters of this module\\nand all submodules. Otherwise, yields only parameters that\"),\n",
       " Document(metadata={}, page_content=\"are direct members of this module.\\nremove_duplicate\\n(\\nbool\\n,\\noptional\\n) – whether to remove the duplicated\\nparameters in the result. Defaults to True.\\nYields\\n(str, Parameter)\\n– Tuple containing the name and parameter\\nReturn type\\nIterator\\n[\\nTuple\\n[\\nstr\\n,\\nParameter\\n]]\\nExample:\\n>>>\\nfor\\nname\\n,\\nparam\\nin\\nself\\n.\\nnamed_parameters\\n():\\n>>>\\nif\\nname\\nin\\n[\\n'bias'\\n]:\\n>>>\\nprint\\n(\\nparam\\n.\\nsize\\n())\\nparameters\\n(\\nrecurse\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterator over module parameters.\"),\n",
       " Document(metadata={}, page_content=\"¶\\nReturn an iterator over module parameters.\\nThis is typically passed to an optimizer.\\nParameters\\nrecurse\\n(\\nbool\\n) – if True, then yields parameters of this module\\nand all submodules. Otherwise, yields only parameters that\\nare direct members of this module.\\nYields\\nParameter\\n– module parameter\\nReturn type\\nIterator\\n[\\nParameter\\n]\\nExample:\\n>>>\\nfor\\nparam\\nin\\nmodel\\n.\\nparameters\\n():\\n>>>\\nprint\\n(\\ntype\\n(\\nparam\\n),\\nparam\\n.\\nsize\\n())\\n<class 'torch.Tensor'> (20L,)\\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\"),\n",
       " Document(metadata={}, page_content=\"<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\\nregister_backward_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a backward hook on the module.\\nThis function is deprecated in favor of\\nregister_full_backward_hook()\\nand\\nthe behavior of this function will change in future versions.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nregister_buffer\\n(\\nname\\n,\\ntensor\\n,\\npersistent\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\"),\n",
       " Document(metadata={}, page_content='tensor\\n,\\npersistent\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nAdd a buffer to the module.\\nThis is typically used to register a buffer that should not to be\\nconsidered a model parameter. For example, BatchNorm’s\\nrunning_mean\\nis not a parameter, but is part of the module’s state. Buffers, by\\ndefault, are persistent and will be saved alongside parameters. This\\nbehavior can be changed by setting\\npersistent\\nto\\nFalse\\n. The\\nonly difference between a persistent buffer and a non-persistent buffer'),\n",
       " Document(metadata={}, page_content='is that the latter will not be a part of this module’s\\nstate_dict\\n.\\nBuffers can be accessed as attributes using given names.\\nParameters\\nname\\n(\\nstr\\n) – name of the buffer. The buffer can be accessed\\nfrom this module using the given name\\ntensor\\n(\\nTensor\\nor\\nNone\\n) – buffer to be registered. If\\nNone\\n, then operations\\nthat run on buffers, such as\\ncuda\\n, are ignored. If\\nNone\\n,\\nthe buffer is\\nnot\\nincluded in the module’s\\nstate_dict\\n.\\npersistent\\n(\\nbool\\n) – whether the buffer is part of this module’s'),\n",
       " Document(metadata={}, page_content=\") – whether the buffer is part of this module’s\\nstate_dict\\n.\\nExample:\\n>>>\\nself\\n.\\nregister_buffer\\n(\\n'running_mean'\\n,\\ntorch\\n.\\nzeros\\n(\\nnum_features\\n))\\nregister_forward_hook\\n(\\nhook\\n,\\n*\\n,\\nprepend\\n=\\nFalse\\n,\\nwith_kwargs\\n=\\nFalse\\n,\\nalways_call\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nRegister a forward hook on the module.\\nThe hook will be called every time after\\nforward()\\nhas computed an output.\\nIf\\nwith_kwargs\\nis\\nFalse\\nor not specified, the input contains only\"),\n",
       " Document(metadata={}, page_content='False\\nor not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won’t be\\npassed to the hooks and only to the\\nforward\\n. The hook can modify the\\noutput. It can modify the input inplace but it will not have effect on\\nforward since this is called after\\nforward()\\nis called. The hook\\nshould have the following signature:\\nhook\\n(\\nmodule\\n,\\nargs\\n,\\noutput\\n)\\n->\\nNone\\nor\\nmodified\\noutput\\nIf\\nwith_kwargs\\nis\\nTrue\\n, the forward hook will be passed the\\nkwargs'),\n",
       " Document(metadata={}, page_content='True\\n, the forward hook will be passed the\\nkwargs\\ngiven to the forward function and be expected to return the\\noutput possibly modified. The hook should have the following signature:\\nhook\\n(\\nmodule\\n,\\nargs\\n,\\nkwargs\\n,\\noutput\\n)\\n->\\nNone\\nor\\nmodified\\noutput\\nParameters\\nhook\\n(\\nCallable\\n) – The user defined hook to be registered.\\nprepend\\n(\\nbool\\n) – If\\nTrue\\n, the provided\\nhook\\nwill be fired\\nbefore all existing\\nforward\\nhooks on this\\ntorch.nn.modules.Module\\n. Otherwise, the provided\\nhook'),\n",
       " Document(metadata={}, page_content='. Otherwise, the provided\\nhook\\nwill be fired after all existing\\nforward\\nhooks on\\nthis\\ntorch.nn.modules.Module\\n. Note that global\\nforward\\nhooks registered with\\nregister_module_forward_hook()\\nwill fire before all hooks\\nregistered by this method.\\nDefault:\\nFalse\\nwith_kwargs\\n(\\nbool\\n) – If\\nTrue\\n, the\\nhook\\nwill be passed the\\nkwargs given to the forward function.\\nDefault:\\nFalse\\nalways_call\\n(\\nbool\\n) – If\\nTrue\\nthe\\nhook\\nwill be run regardless of\\nwhether an exception is raised while calling the Module.'),\n",
       " Document(metadata={}, page_content='Default:\\nFalse\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nregister_forward_pre_hook\\n(\\nhook\\n,\\n*\\n,\\nprepend\\n=\\nFalse\\n,\\nwith_kwargs\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nRegister a forward pre-hook on the module.\\nThe hook will be called every time before\\nforward()\\nis invoked.\\nIf\\nwith_kwargs\\nis false or not specified, the input contains only\\nthe positional arguments given to the module. Keyword arguments won’t be'),\n",
       " Document(metadata={}, page_content='passed to the hooks and only to the\\nforward\\n. The hook can modify the\\ninput. User can either return a tuple or a single modified value in the\\nhook. We will wrap the value into a tuple if a single value is returned\\n(unless that value is already a tuple). The hook should have the\\nfollowing signature:\\nhook\\n(\\nmodule\\n,\\nargs\\n)\\n->\\nNone\\nor\\nmodified\\ninput\\nIf\\nwith_kwargs\\nis true, the forward pre-hook will be passed the\\nkwargs given to the forward function. And if the hook modifies the'),\n",
       " Document(metadata={}, page_content='input, both the args and kwargs should be returned. The hook should have\\nthe following signature:\\nhook\\n(\\nmodule\\n,\\nargs\\n,\\nkwargs\\n)\\n->\\nNone\\nor\\na\\ntuple\\nof\\nmodified\\ninput\\nand\\nkwargs\\nParameters\\nhook\\n(\\nCallable\\n) – The user defined hook to be registered.\\nprepend\\n(\\nbool\\n) – If true, the provided\\nhook\\nwill be fired before\\nall existing\\nforward_pre\\nhooks on this\\ntorch.nn.modules.Module\\n. Otherwise, the provided\\nhook\\nwill be fired after all existing\\nforward_pre\\nhooks\\non this\\ntorch.nn.modules.Module'),\n",
       " Document(metadata={}, page_content='forward_pre\\nhooks\\non this\\ntorch.nn.modules.Module\\n. Note that global\\nforward_pre\\nhooks registered with\\nregister_module_forward_pre_hook()\\nwill fire before all\\nhooks registered by this method.\\nDefault:\\nFalse\\nwith_kwargs\\n(\\nbool\\n) – If true, the\\nhook\\nwill be passed the kwargs\\ngiven to the forward function.\\nDefault:\\nFalse\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nregister_full_backward_hook\\n(\\nhook\\n,\\nprepend\\n='),\n",
       " Document(metadata={}, page_content='register_full_backward_hook\\n(\\nhook\\n,\\nprepend\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nRegister a backward hook on the module.\\nThe hook will be called every time the gradients with respect to a module\\nare computed, i.e. the hook will execute if and only if the gradients with\\nrespect to module outputs are computed. The hook should have the following\\nsignature:\\nhook\\n(\\nmodule\\n,\\ngrad_input\\n,\\ngrad_output\\n)\\n->\\ntuple\\n(\\nTensor\\n)\\nor\\nNone\\nThe\\ngrad_input\\nand\\ngrad_output\\nare tuples that contain the gradients'),\n",
       " Document(metadata={}, page_content='grad_output\\nare tuples that contain the gradients\\nwith respect to the inputs and outputs respectively. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with\\nrespect to the input that will be used in place of\\ngrad_input\\nin\\nsubsequent computations.\\ngrad_input\\nwill only correspond to the inputs given\\nas positional arguments and all kwarg arguments are ignored. Entries\\nin\\ngrad_input\\nand\\ngrad_output\\nwill be\\nNone\\nfor all non-Tensor\\narguments.'),\n",
       " Document(metadata={}, page_content='will be\\nNone\\nfor all non-Tensor\\narguments.\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module’s forward function.\\nWarning\\nModifying inputs or outputs inplace is not allowed when using backward hooks and\\nwill raise an error.\\nParameters\\nhook\\n(\\nCallable\\n) – The user-defined hook to be registered.\\nprepend\\n(\\nbool'),\n",
       " Document(metadata={}, page_content='prepend\\n(\\nbool\\n) – If true, the provided\\nhook\\nwill be fired before\\nall existing\\nbackward\\nhooks on this\\ntorch.nn.modules.Module\\n. Otherwise, the provided\\nhook\\nwill be fired after all existing\\nbackward\\nhooks on\\nthis\\ntorch.nn.modules.Module\\n. Note that global\\nbackward\\nhooks registered with\\nregister_module_full_backward_hook()\\nwill fire before\\nall hooks registered by this method.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type'),\n",
       " Document(metadata={}, page_content='handle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nregister_full_backward_pre_hook\\n(\\nhook\\n,\\nprepend\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nRegister a backward pre-hook on the module.\\nThe hook will be called every time the gradients for the module are computed.\\nThe hook should have the following signature:\\nhook\\n(\\nmodule\\n,\\ngrad_output\\n)\\n->\\ntuple\\n[\\nTensor\\n]\\nor\\nNone\\nThe\\ngrad_output\\nis a tuple. The hook should\\nnot modify its arguments, but it can optionally return a new gradient with'),\n",
       " Document(metadata={}, page_content='respect to the output that will be used in place of\\ngrad_output\\nin\\nsubsequent computations. Entries in\\ngrad_output\\nwill be\\nNone\\nfor\\nall non-Tensor arguments.\\nFor technical reasons, when this hook is applied to a Module, its forward function will\\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\\nof each Tensor returned by the Module’s forward function.\\nWarning\\nModifying inputs inplace is not allowed when using backward hooks and\\nwill raise an error.'),\n",
       " Document(metadata={}, page_content='will raise an error.\\nParameters\\nhook\\n(\\nCallable\\n) – The user-defined hook to be registered.\\nprepend\\n(\\nbool\\n) – If true, the provided\\nhook\\nwill be fired before\\nall existing\\nbackward_pre\\nhooks on this\\ntorch.nn.modules.Module\\n. Otherwise, the provided\\nhook\\nwill be fired after all existing\\nbackward_pre\\nhooks\\non this\\ntorch.nn.modules.Module\\n. Note that global\\nbackward_pre\\nhooks registered with\\nregister_module_full_backward_pre_hook()\\nwill fire before\\nall hooks registered by this method.\\nReturns'),\n",
       " Document(metadata={}, page_content='all hooks registered by this method.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nregister_load_state_dict_post_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a post-hook to be run after module’s\\nload_state_dict()\\nis called.\\nIt should have the following signature::\\nhook(module, incompatible_keys) -> None\\nThe\\nmodule\\nargument is the current module that this hook is registered\\non, and the\\nincompatible_keys'),\n",
       " Document(metadata={}, page_content='on, and the\\nincompatible_keys\\nargument is a\\nNamedTuple\\nconsisting\\nof attributes\\nmissing_keys\\nand\\nunexpected_keys\\n.\\nmissing_keys\\nis a\\nlist\\nof\\nstr\\ncontaining the missing keys and\\nunexpected_keys\\nis a\\nlist\\nof\\nstr\\ncontaining the unexpected keys.\\nThe given incompatible_keys can be modified inplace if needed.\\nNote that the checks performed when calling\\nload_state_dict()\\nwith\\nstrict=True\\nare affected by modifications the hook makes to\\nmissing_keys\\nor\\nunexpected_keys\\n, as expected. Additions to either'),\n",
       " Document(metadata={}, page_content=', as expected. Additions to either\\nset of keys will result in an error being thrown when\\nstrict=True\\n, and\\nclearing out both missing and unexpected keys will avoid an error.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nregister_load_state_dict_pre_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a pre-hook to be run before module’s\\nload_state_dict()\\nis called.\\nIt should have the following signature::'),\n",
       " Document(metadata={}, page_content='It should have the following signature::\\nhook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\\nParameters\\nhook\\n(\\nCallable\\n) – Callable hook that will be invoked before\\nloading the state dict.\\nregister_module\\n(\\nname\\n,\\nmodule\\n)\\n[source]\\n[source]\\n¶\\nAlias for\\nadd_module()\\n.\\nregister_parameter\\n(\\nname\\n,\\nparam\\n)\\n[source]\\n[source]\\n¶\\nAdd a parameter to the module.\\nThe parameter can be accessed as an attribute using given name.'),\n",
       " Document(metadata={}, page_content='Parameters\\nname\\n(\\nstr\\n) – name of the parameter. The parameter can be accessed\\nfrom this module using the given name\\nparam\\n(\\nParameter\\nor\\nNone\\n) – parameter to be added to the module. If\\nNone\\n, then operations that run on parameters, such as\\ncuda\\n,\\nare ignored. If\\nNone\\n, the parameter is\\nnot\\nincluded in the\\nmodule’s\\nstate_dict\\n.\\nregister_state_dict_post_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a post-hook for the\\nstate_dict()\\nmethod.\\nIt should have the following signature::'),\n",
       " Document(metadata={}, page_content='method.\\nIt should have the following signature::\\nhook(module, state_dict, prefix, local_metadata) -> None\\nThe registered hooks can modify the\\nstate_dict\\ninplace.\\nregister_state_dict_pre_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a pre-hook for the\\nstate_dict()\\nmethod.\\nIt should have the following signature::\\nhook(module, prefix, keep_vars) -> None\\nThe registered hooks can be used to perform pre-processing before the\\nstate_dict\\ncall is made.\\nrequires_grad_\\n(\\nrequires_grad\\n=\\nTrue\\n)\\n[source]'),\n",
       " Document(metadata={}, page_content='requires_grad_\\n(\\nrequires_grad\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nChange if autograd should record operations on parameters in this module.\\nThis method sets the parameters’\\nrequires_grad\\nattributes\\nin-place.\\nThis method is helpful for freezing part of the module for finetuning\\nor training parts of a model individually (e.g., GAN training).\\nSee\\nLocally disabling gradient computation\\nfor a comparison between\\n.requires_grad_()\\nand several similar mechanisms that may be confused with it.\\nParameters'),\n",
       " Document(metadata={}, page_content='Parameters\\nrequires_grad\\n(\\nbool\\n) – whether autograd should record operations on\\nparameters in this module. Default:\\nTrue\\n.\\nReturns\\nself\\nReturn type\\nModule\\nset_extra_state\\n(\\nstate\\n)\\n[source]\\n[source]\\n¶\\nSet extra state contained in the loaded\\nstate_dict\\n.\\nThis function is called from\\nload_state_dict()\\nto handle any extra state\\nfound within the\\nstate_dict\\n. Implement this function and a corresponding\\nget_extra_state()\\nfor your module if you need to store extra state within its\\nstate_dict\\n.'),\n",
       " Document(metadata={}, page_content='state_dict\\n.\\nParameters\\nstate\\n(\\ndict\\n) – Extra state from the\\nstate_dict\\nset_submodule\\n(\\ntarget\\n,\\nmodule\\n)\\n[source]\\n[source]\\n¶\\nSet the submodule given by\\ntarget\\nif it exists, otherwise throw an error.\\nFor example, let’s say you have an\\nnn.Module\\nA\\nthat\\nlooks like this:\\nA(\\n    (net_b): Module(\\n        (net_c): Module(\\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\\n        )\\n        (linear): Linear(in_features=100, out_features=200, bias=True)\\n    )\\n)\\n(The diagram shows an'),\n",
       " Document(metadata={}, page_content=')\\n)\\n(The diagram shows an\\nnn.Module\\nA\\n.\\nA\\nhas a nested\\nsubmodule\\nnet_b\\n, which itself has two submodules\\nnet_c\\nand\\nlinear\\n.\\nnet_c\\nthen has a submodule\\nconv\\n.)\\nTo overide the\\nConv2d\\nwith a new submodule\\nLinear\\n, you\\nwould call\\nset_submodule(\"net_b.net_c.conv\",\\nnn.Linear(33,\\n16))\\n.\\nParameters\\ntarget\\n(\\nstr\\n) – The fully-qualified string name of the submodule\\nto look for. (See above example for how to specify a\\nfully-qualified string.)\\nmodule\\n(\\nModule\\n) – The module to set the submodule to.'),\n",
       " Document(metadata={}, page_content=\"(\\nModule\\n) – The module to set the submodule to.\\nRaises\\nValueError\\n– If the target string is empty\\nAttributeError\\n– If the target string references an invalid\\n    path or resolves to something that is not an\\nnn.Module\\nshare_memory\\n(\\n)\\n[source]\\n[source]\\n¶\\nSee\\ntorch.Tensor.share_memory_()\\n.\\nReturn type\\nT\\nstate_dict\\n(\\n*\\n,\\ndestination\\n:\\nT_destination\\n,\\nprefix\\n:\\nstr\\n=\\n''\\n,\\nkeep_vars\\n:\\nbool\\n=\\nFalse\\n)\\n→\\nT_destination\\n[source]\\n[source]\\n¶\\nstate_dict\\n(\\n*\\n,\\nprefix\\n:\\nstr\\n=\\n''\\n,\\nkeep_vars\\n:\\nbool\\n=\\nFalse\\n)\\n→\"),\n",
       " Document(metadata={}, page_content=\"prefix\\n:\\nstr\\n=\\n''\\n,\\nkeep_vars\\n:\\nbool\\n=\\nFalse\\n)\\n→\\nDict\\n[\\nstr\\n,\\nAny\\n]\\nReturn a dictionary containing references to the whole state of the module.\\nBoth parameters and persistent buffers (e.g. running averages) are\\nincluded. Keys are corresponding parameter and buffer names.\\nParameters and buffers set to\\nNone\\nare not included.\\nNote\\nThe returned object is a shallow copy. It contains references\\nto the module’s parameters and buffers.\\nWarning\\nCurrently\\nstate_dict()\"),\n",
       " Document(metadata={}, page_content='Warning\\nCurrently\\nstate_dict()\\nalso accepts positional arguments for\\ndestination\\n,\\nprefix\\nand\\nkeep_vars\\nin order. However,\\nthis is being deprecated and keyword arguments will be enforced in\\nfuture releases.\\nWarning\\nPlease avoid the use of argument\\ndestination\\nas it is not\\ndesigned for end-users.\\nParameters\\ndestination\\n(\\ndict\\n,\\noptional\\n) – If provided, the state of module will\\nbe updated into the dict and the same object is returned.\\nOtherwise, an\\nOrderedDict\\nwill be created and returned.'),\n",
       " Document(metadata={}, page_content=\"OrderedDict\\nwill be created and returned.\\nDefault:\\nNone\\n.\\nprefix\\n(\\nstr\\n,\\noptional\\n) – a prefix added to parameter and buffer\\nnames to compose the keys in state_dict. Default:\\n''\\n.\\nkeep_vars\\n(\\nbool\\n,\\noptional\\n) – by default the\\nTensor\\ns\\nreturned in the state dict are detached from autograd. If it’s\\nset to\\nTrue\\n, detaching will not be performed.\\nDefault:\\nFalse\\n.\\nReturns\\na dictionary containing a whole state of the module\\nReturn type\\ndict\\nExample:\\n>>>\\nmodule\\n.\\nstate_dict\\n()\\n.\\nkeys\\n()\"),\n",
       " Document(metadata={}, page_content=\"Example:\\n>>>\\nmodule\\n.\\nstate_dict\\n()\\n.\\nkeys\\n()\\n['bias', 'weight']\\nto\\n(\\ndevice\\n:\\nOptional\\n[\\nUnion\\n[\\nstr\\n,\\ndevice\\n,\\nint\\n]\\n]\\n=\\n...\\n,\\ndtype\\n:\\nOptional\\n[\\ndtype\\n]\\n=\\n...\\n,\\nnon_blocking\\n:\\nbool\\n=\\n...\\n)\\n→\\nSelf\\n[source]\\n[source]\\n¶\\nto\\n(\\ndtype\\n:\\ndtype\\n,\\nnon_blocking\\n:\\nbool\\n=\\n...\\n)\\n→\\nSelf\\nto\\n(\\ntensor\\n:\\nTensor\\n,\\nnon_blocking\\n:\\nbool\\n=\\n...\\n)\\n→\\nSelf\\nMove and/or cast the parameters and buffers.\\nThis can be called as\\nto\\n(\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n,\\nnon_blocking\\n=\\nFalse\\n)\\n[source]\\n[source]\\nto\\n(\\ndtype\\n,\"),\n",
       " Document(metadata={}, page_content='=\\nFalse\\n)\\n[source]\\n[source]\\nto\\n(\\ndtype\\n,\\nnon_blocking\\n=\\nFalse\\n)\\n[source]\\n[source]\\nto\\n(\\ntensor\\n,\\nnon_blocking\\n=\\nFalse\\n)\\n[source]\\n[source]\\nto\\n(\\nmemory_format\\n=\\ntorch.channels_last\\n)\\n[source]\\n[source]\\nIts signature is similar to\\ntorch.Tensor.to()\\n, but only accepts\\nfloating point or complex\\ndtype\\ns. In addition, this method will\\nonly cast the floating point or complex parameters and buffers to\\ndtype\\n(if given). The integral parameters and buffers will be moved\\ndevice'),\n",
       " Document(metadata={}, page_content='device\\n, if that is given, but with dtypes unchanged. When\\nnon_blocking\\nis set, it tries to convert/move asynchronously\\nwith respect to the host if possible, e.g., moving CPU Tensors with\\npinned memory to CUDA devices.\\nSee below for examples.\\nNote\\nThis method modifies the module in-place.\\nParameters\\ndevice\\n(\\ntorch.device\\n) – the desired device of the parameters\\nand buffers in this module\\ndtype\\n(\\ntorch.dtype\\n) – the desired floating point or complex dtype of'),\n",
       " Document(metadata={}, page_content='the parameters and buffers in this module\\ntensor\\n(\\ntorch.Tensor\\n) – Tensor whose dtype and device are the desired\\ndtype and device for all parameters and buffers in this module\\nmemory_format\\n(\\ntorch.memory_format\\n) – the desired memory\\nformat for 4D parameters and buffers in this module (keyword\\nonly argument)\\nReturns\\nself\\nReturn type\\nModule\\nExamples:\\n>>>\\nlinear\\n=\\nnn\\n.\\nLinear\\n(\\n2\\n,\\n2\\n)\\n>>>\\nlinear\\n.\\nweight\\nParameter containing:\\ntensor([[ 0.1913, -0.3420],\\n[-0.5113, -0.2325]])\\n>>>\\nlinear\\n.\\nto\\n('),\n",
       " Document(metadata={}, page_content='[-0.5113, -0.2325]])\\n>>>\\nlinear\\n.\\nto\\n(\\ntorch\\n.\\ndouble\\n)\\nLinear(in_features=2, out_features=2, bias=True)\\n>>>\\nlinear\\n.\\nweight\\nParameter containing:\\ntensor([[ 0.1913, -0.3420],\\n[-0.5113, -0.2325]], dtype=torch.float64)\\n>>>\\ngpu1\\n=\\ntorch\\n.\\ndevice\\n(\\n\"cuda:1\"\\n)\\n>>>\\nlinear\\n.\\nto\\n(\\ngpu1\\n,\\ndtype\\n=\\ntorch\\n.\\nhalf\\n,\\nnon_blocking\\n=\\nTrue\\n)\\nLinear(in_features=2, out_features=2, bias=True)\\n>>>\\nlinear\\n.\\nweight\\nParameter containing:\\ntensor([[ 0.1914, -0.3420],'),\n",
       " Document(metadata={}, page_content='Parameter containing:\\ntensor([[ 0.1914, -0.3420],\\n[-0.5112, -0.2324]], dtype=torch.float16, device=\\'cuda:1\\')\\n>>>\\ncpu\\n=\\ntorch\\n.\\ndevice\\n(\\n\"cpu\"\\n)\\n>>>\\nlinear\\n.\\nto\\n(\\ncpu\\n)\\nLinear(in_features=2, out_features=2, bias=True)\\n>>>\\nlinear\\n.\\nweight\\nParameter containing:\\ntensor([[ 0.1914, -0.3420],\\n[-0.5112, -0.2324]], dtype=torch.float16)\\n>>>\\nlinear\\n=\\nnn\\n.\\nLinear\\n(\\n2\\n,\\n2\\n,\\nbias\\n=\\nNone\\n)\\n.\\nto\\n(\\ntorch\\n.\\ncdouble\\n)\\n>>>\\nlinear\\n.\\nweight\\nParameter containing:\\ntensor([[ 0.3741+0.j,  0.2382+0.j],'),\n",
       " Document(metadata={}, page_content='tensor([[ 0.3741+0.j,  0.2382+0.j],\\n[ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\\n>>>\\nlinear\\n(\\ntorch\\n.\\nones\\n(\\n3\\n,\\n2\\n,\\ndtype\\n=\\ntorch\\n.\\ncdouble\\n))\\ntensor([[0.6122+0.j, 0.1150+0.j],\\n[0.6122+0.j, 0.1150+0.j],\\n[0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\\nto_empty\\n(\\n*\\n,\\ndevice\\n,\\nrecurse\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nMove the parameters and buffers to the specified device without copying storage.\\nParameters\\ndevice\\n(\\ntorch.device\\n) – The desired device of the parameters'),\n",
       " Document(metadata={}, page_content=') – The desired device of the parameters\\nand buffers in this module.\\nrecurse\\n(\\nbool\\n) – Whether parameters and buffers of submodules should\\nbe recursively moved to the specified device.\\nReturns\\nself\\nReturn type\\nModule\\ntrain\\n(\\nmode\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nSet the module in training mode.\\nThis has an effect only on certain modules. See the documentation of\\nparticular modules for details of their behaviors in training/evaluation\\nmode, i.e., whether they are affected, e.g.\\nDropout\\n,\\nBatchNorm'),\n",
       " Document(metadata={}, page_content='Dropout\\n,\\nBatchNorm\\n,\\netc.\\nParameters\\nmode\\n(\\nbool\\n) – whether to set training mode (\\nTrue\\n) or evaluation\\nmode (\\nFalse\\n). Default:\\nTrue\\n.\\nReturns\\nself\\nReturn type\\nModule\\ntype\\n(\\ndst_type\\n)\\n[source]\\n[source]\\n¶\\nCasts all parameters and buffers to\\ndst_type\\n.\\nNote\\nThis method modifies the module in-place.\\nParameters\\ndst_type\\n(\\ntype\\nor\\nstring\\n) – the desired type\\nReturns\\nself\\nReturn type\\nModule\\nxpu\\n(\\ndevice\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nMove all model parameters and buffers to the XPU.'),\n",
       " Document(metadata={}, page_content='Move all model parameters and buffers to the XPU.\\nThis also makes associated parameters and buffers different objects. So\\nit should be called before constructing optimizer if the module will\\nlive on XPU while being optimized.\\nNote\\nThis method modifies the module in-place.\\nParameters\\ndevice\\n(\\nint\\n,\\noptional\\n) – if specified, all parameters will be\\ncopied to that device\\nReturns\\nself\\nReturn type\\nModule\\nzero_grad\\n(\\nset_to_none\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nReset gradients of all model parameters.'),\n",
       " Document(metadata={}, page_content='¶\\nReset gradients of all model parameters.\\nSee similar function under\\ntorch.optim.Optimizer\\nfor more context.\\nParameters\\nset_to_none\\n(\\nbool\\n) – instead of setting to zero, set the grads to None.\\nSee\\ntorch.optim.Optimizer.zero_grad()\\nfor details.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Sequential\\n¶\\nclass\\ntorch.nn.\\nSequential\\n(\\n*\\nargs\\n:\\nModule\\n)\\n[source]\\n[source]\\n¶\\nclass\\ntorch.nn.\\nSequential\\n(\\narg\\n:\\nOrderedDict\\n[\\nstr\\n,\\nModule\\n]\\n)\\nA sequential container.\\nModules will be added to it in the order they are passed in the\\nconstructor. Alternatively, an\\nOrderedDict\\nof modules can be\\npassed in. The\\nforward()\\nmethod of\\nSequential\\naccepts any\\ninput and forwards it to the first module it contains. It then\\n“chains” outputs to inputs sequentially for each subsequent module,'),\n",
       " Document(metadata={}, page_content='finally returning the output of the last module.\\nThe value a\\nSequential\\nprovides over manually calling a sequence\\nof modules is that it allows treating the whole container as a\\nsingle module, such that performing a transformation on the\\nSequential\\napplies to each of the modules it stores (which are\\neach a registered submodule of the\\nSequential\\n).\\nWhat’s the difference between a\\nSequential\\nand a\\ntorch.nn.ModuleList\\n? A\\nModuleList\\nis exactly what it\\nsounds like–a list for storing\\nModule'),\n",
       " Document(metadata={}, page_content='sounds like–a list for storing\\nModule\\ns! On the other hand,\\nthe layers in a\\nSequential\\nare connected in a cascading way.\\nExample:\\n# Using Sequential to create a small model. When `model` is run,\\n# input will first be passed to `Conv2d(1,20,5)`. The output of\\n# `Conv2d(1,20,5)` will be used as the input to the first\\n# `ReLU`; the output of the first `ReLU` will become the input\\n# for `Conv2d(20,64,5)`. Finally, the output of\\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\\nmodel\\n='),\n",
       " Document(metadata={}, page_content=\"model\\n=\\nnn\\n.\\nSequential\\n(\\nnn\\n.\\nConv2d\\n(\\n1\\n,\\n20\\n,\\n5\\n),\\nnn\\n.\\nReLU\\n(),\\nnn\\n.\\nConv2d\\n(\\n20\\n,\\n64\\n,\\n5\\n),\\nnn\\n.\\nReLU\\n()\\n)\\n# Using Sequential with OrderedDict. This is functionally the\\n# same as the above code\\nmodel\\n=\\nnn\\n.\\nSequential\\n(\\nOrderedDict\\n([\\n(\\n'conv1'\\n,\\nnn\\n.\\nConv2d\\n(\\n1\\n,\\n20\\n,\\n5\\n)),\\n(\\n'relu1'\\n,\\nnn\\n.\\nReLU\\n()),\\n(\\n'conv2'\\n,\\nnn\\n.\\nConv2d\\n(\\n20\\n,\\n64\\n,\\n5\\n)),\\n(\\n'relu2'\\n,\\nnn\\n.\\nReLU\\n())\\n]))\\nappend\\n(\\nmodule\\n)\\n[source]\\n[source]\\n¶\\nAppend a given module to the end.\\nParameters\\nmodule\\n(\\nnn.Module\"),\n",
       " Document(metadata={}, page_content='Parameters\\nmodule\\n(\\nnn.Module\\n) – module to append\\nReturn type\\nSequential\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ModuleList\\n¶\\nclass\\ntorch.nn.\\nModuleList\\n(\\nmodules\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nHolds submodules in a list.\\nModuleList\\ncan be indexed like a regular Python list, but\\nmodules it contains are properly registered, and will be visible by all\\nModule\\nmethods.\\nParameters\\nmodules\\n(\\niterable\\n,\\noptional\\n) – an iterable of modules to add\\nExample:\\nclass\\nMyModule\\n(\\nnn\\n.\\nModule\\n):\\ndef\\n__init__\\n(\\nself\\n)\\n->\\nNone\\n:\\nsuper\\n()\\n.\\n__init__\\n()\\nself\\n.\\nlinears\\n=\\nnn\\n.\\nModuleList\\n([\\nnn\\n.\\nLinear\\n(\\n10\\n,\\n10\\n)\\nfor\\ni\\nin\\nrange\\n('),\n",
       " Document(metadata={}, page_content='([\\nnn\\n.\\nLinear\\n(\\n10\\n,\\n10\\n)\\nfor\\ni\\nin\\nrange\\n(\\n10\\n)])\\ndef\\nforward\\n(\\nself\\n,\\nx\\n):\\n# ModuleList can act as an iterable, or be indexed using ints\\nfor\\ni\\n,\\nl\\nin\\nenumerate\\n(\\nself\\n.\\nlinears\\n):\\nx\\n=\\nself\\n.\\nlinears\\n[\\ni\\n//\\n2\\n](\\nx\\n)\\n+\\nl\\n(\\nx\\n)\\nreturn\\nx\\nappend\\n(\\nmodule\\n)\\n[source]\\n[source]\\n¶\\nAppend a given module to the end of the list.\\nParameters\\nmodule\\n(\\nnn.Module\\n) – module to append\\nReturn type\\nModuleList\\nextend\\n(\\nmodules\\n)\\n[source]\\n[source]\\n¶\\nAppend modules from a Python iterable to the end of the list.'),\n",
       " Document(metadata={}, page_content='Parameters\\nmodules\\n(\\niterable\\n) – iterable of modules to append\\nReturn type\\nSelf\\ninsert\\n(\\nindex\\n,\\nmodule\\n)\\n[source]\\n[source]\\n¶\\nInsert a given module before a given index in the list.\\nParameters\\nindex\\n(\\nint\\n) – index to insert.\\nmodule\\n(\\nnn.Module\\n) – module to insert\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ModuleDict\\n¶\\nclass\\ntorch.nn.\\nModuleDict\\n(\\nmodules\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nHolds submodules in a dictionary.\\nModuleDict\\ncan be indexed like a regular Python dictionary,\\nbut modules it contains are properly registered, and will be visible by all\\nModule\\nmethods.\\nModuleDict\\nis an\\nordered\\ndictionary that respects\\nthe order of insertion, and\\nin\\nupdate()\\n, the order of the merged\\nOrderedDict\\n,\\ndict\\n(started from Python 3.6) or another\\nModuleDict\\n(the argument to\\nupdate()\\n).\\nNote that\\nupdate()'),\n",
       " Document(metadata={}, page_content=\"(the argument to\\nupdate()\\n).\\nNote that\\nupdate()\\nwith other unordered mapping\\ntypes (e.g., Python’s plain\\ndict\\nbefore Python version 3.6) does not\\npreserve the order of the merged mapping.\\nParameters\\nmodules\\n(\\niterable\\n,\\noptional\\n) – a mapping (dictionary) of (string: module)\\nor an iterable of key-value pairs of type (string, module)\\nExample:\\nclass\\nMyModule\\n(\\nnn\\n.\\nModule\\n):\\ndef\\n__init__\\n(\\nself\\n)\\n->\\nNone\\n:\\nsuper\\n()\\n.\\n__init__\\n()\\nself\\n.\\nchoices\\n=\\nnn\\n.\\nModuleDict\\n({\\n'conv'\\n:\\nnn\\n.\\nConv2d\\n(\\n10\\n,\\n10\\n,\"),\n",
       " Document(metadata={}, page_content=\".\\nModuleDict\\n({\\n'conv'\\n:\\nnn\\n.\\nConv2d\\n(\\n10\\n,\\n10\\n,\\n3\\n),\\n'pool'\\n:\\nnn\\n.\\nMaxPool2d\\n(\\n3\\n)\\n})\\nself\\n.\\nactivations\\n=\\nnn\\n.\\nModuleDict\\n([\\n[\\n'lrelu'\\n,\\nnn\\n.\\nLeakyReLU\\n()],\\n[\\n'prelu'\\n,\\nnn\\n.\\nPReLU\\n()]\\n])\\ndef\\nforward\\n(\\nself\\n,\\nx\\n,\\nchoice\\n,\\nact\\n):\\nx\\n=\\nself\\n.\\nchoices\\n[\\nchoice\\n](\\nx\\n)\\nx\\n=\\nself\\n.\\nactivations\\n[\\nact\\n](\\nx\\n)\\nreturn\\nx\\nclear\\n(\\n)\\n[source]\\n[source]\\n¶\\nRemove all items from the ModuleDict.\\nitems\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterable of the ModuleDict key/value pairs.\\nReturn type\\nIterable\\n[\\nTuple\\n[\\nstr\\n,\"),\n",
       " Document(metadata={}, page_content='Return type\\nIterable\\n[\\nTuple\\n[\\nstr\\n,\\nModule\\n]]\\nkeys\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterable of the ModuleDict keys.\\nReturn type\\nIterable\\n[\\nstr\\n]\\npop\\n(\\nkey\\n)\\n[source]\\n[source]\\n¶\\nRemove key from the ModuleDict and return its module.\\nParameters\\nkey\\n(\\nstr\\n) – key to pop from the ModuleDict\\nReturn type\\nModule\\nupdate\\n(\\nmodules\\n)\\n[source]\\n[source]\\n¶\\nUpdate the\\nModuleDict\\nwith key-value pairs from a mapping, overwriting existing keys.\\nNote\\nIf\\nmodules\\nis an\\nOrderedDict\\n, a\\nModuleDict\\n, or'),\n",
       " Document(metadata={}, page_content='If\\nmodules\\nis an\\nOrderedDict\\n, a\\nModuleDict\\n, or\\nan iterable of key-value pairs, the order of new elements in it is preserved.\\nParameters\\nmodules\\n(\\niterable\\n) – a mapping (dictionary) from string to\\nModule\\n,\\nor an iterable of key-value pairs of type (string,\\nModule\\n)\\nvalues\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterable of the ModuleDict values.\\nReturn type\\nIterable\\n[\\nModule\\n]\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ParameterList\\n¶\\nclass\\ntorch.nn.\\nParameterList\\n(\\nvalues\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nHolds parameters in a list.\\nParameterList\\ncan be used like a regular Python\\nlist, but Tensors that are\\nParameter\\nare properly registered,\\nand will be visible by all\\nModule\\nmethods.\\nNote that the constructor, assigning an element of the list, the\\nappend()\\nmethod and the\\nextend()\\nmethod will convert any\\nTensor\\ninto\\nParameter\\n.\\nParameters\\nparameters\\n(\\niterable\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='.\\nParameters\\nparameters\\n(\\niterable\\n,\\noptional\\n) – an iterable of elements to add to the list.\\nExample:\\nclass\\nMyModule\\n(\\nnn\\n.\\nModule\\n):\\ndef\\n__init__\\n(\\nself\\n)\\n->\\nNone\\n:\\nsuper\\n()\\n.\\n__init__\\n()\\nself\\n.\\nparams\\n=\\nnn\\n.\\nParameterList\\n([\\nnn\\n.\\nParameter\\n(\\ntorch\\n.\\nrandn\\n(\\n10\\n,\\n10\\n))\\nfor\\ni\\nin\\nrange\\n(\\n10\\n)])\\ndef\\nforward\\n(\\nself\\n,\\nx\\n):\\n# ParameterList can act as an iterable, or be indexed using ints\\nfor\\ni\\n,\\np\\nin\\nenumerate\\n(\\nself\\n.\\nparams\\n):\\nx\\n=\\nself\\n.\\nparams\\n[\\ni\\n//\\n2\\n]\\n.\\nmm\\n(\\nx\\n)\\n+\\np\\n.\\nmm\\n(\\nx\\n)\\nreturn\\nx\\nappend'),\n",
       " Document(metadata={}, page_content='//\\n2\\n]\\n.\\nmm\\n(\\nx\\n)\\n+\\np\\n.\\nmm\\n(\\nx\\n)\\nreturn\\nx\\nappend\\n(\\nvalue\\n)\\n[source]\\n[source]\\n¶\\nAppend a given value at the end of the list.\\nParameters\\nvalue\\n(\\nAny\\n) – value to append\\nReturn type\\nParameterList\\nextend\\n(\\nvalues\\n)\\n[source]\\n[source]\\n¶\\nAppend values from a Python iterable to the end of the list.\\nParameters\\nvalues\\n(\\niterable\\n) – iterable of values to append\\nReturn type\\nSelf\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ParameterDict\\n¶\\nclass\\ntorch.nn.\\nParameterDict\\n(\\nparameters\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nHolds parameters in a dictionary.\\nParameterDict can be indexed like a regular Python dictionary, but Parameters it\\ncontains are properly registered, and will be visible by all Module methods.\\nOther objects are treated as would be done by a regular Python dictionary\\nParameterDict\\nis an\\nordered\\ndictionary.\\nupdate()\\nwith other unordered mapping\\ntypes (e.g., Python’s plain\\ndict'),\n",
       " Document(metadata={}, page_content='types (e.g., Python’s plain\\ndict\\n) does not preserve the order of the\\nmerged mapping. On the other hand,\\nOrderedDict\\nor another\\nParameterDict\\nwill preserve their ordering.\\nNote that the constructor, assigning an element of the dictionary and the\\nupdate()\\nmethod will convert any\\nTensor\\ninto\\nParameter\\n.\\nParameters\\nvalues\\n(\\niterable\\n,\\noptional\\n) – a mapping (dictionary) of\\n(string : Any) or an iterable of key-value pairs\\nof type (string, Any)\\nExample:\\nclass\\nMyModule\\n(\\nnn\\n.\\nModule\\n):\\ndef\\n__init__\\n('),\n",
       " Document(metadata={}, page_content=\"class\\nMyModule\\n(\\nnn\\n.\\nModule\\n):\\ndef\\n__init__\\n(\\nself\\n)\\n->\\nNone\\n:\\nsuper\\n()\\n.\\n__init__\\n()\\nself\\n.\\nparams\\n=\\nnn\\n.\\nParameterDict\\n({\\n'left'\\n:\\nnn\\n.\\nParameter\\n(\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n10\\n)),\\n'right'\\n:\\nnn\\n.\\nParameter\\n(\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n10\\n))\\n})\\ndef\\nforward\\n(\\nself\\n,\\nx\\n,\\nchoice\\n):\\nx\\n=\\nself\\n.\\nparams\\n[\\nchoice\\n]\\n.\\nmm\\n(\\nx\\n)\\nreturn\\nx\\nclear\\n(\\n)\\n[source]\\n[source]\\n¶\\nRemove all items from the ParameterDict.\\ncopy\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn a copy of this\\nParameterDict\\ninstance.\\nReturn type\\nParameterDict\\nfromkeys\"),\n",
       " Document(metadata={}, page_content='instance.\\nReturn type\\nParameterDict\\nfromkeys\\n(\\nkeys\\n,\\ndefault\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nReturn a new ParameterDict with the keys provided.\\nParameters\\nkeys\\n(\\niterable\\n,\\nstring\\n) – keys to make the new ParameterDict from\\ndefault\\n(\\nParameter\\n,\\noptional\\n) – value to set for all keys\\nReturn type\\nParameterDict\\nget\\n(\\nkey\\n,\\ndefault\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nReturn the parameter associated with key if present. Otherwise return default if provided, None if not.\\nParameters\\nkey\\n(\\nstr'),\n",
       " Document(metadata={}, page_content='Parameters\\nkey\\n(\\nstr\\n) – key to get from the ParameterDict\\ndefault\\n(\\nParameter\\n,\\noptional\\n) – value to return if key not present\\nReturn type\\nAny\\nitems\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterable of the ParameterDict key/value pairs.\\nReturn type\\nIterable\\n[\\nTuple\\n[\\nstr\\n,\\nAny\\n]]\\nkeys\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterable of the ParameterDict keys.\\nReturn type\\nIterable\\n[\\nstr\\n]\\npop\\n(\\nkey\\n)\\n[source]\\n[source]\\n¶\\nRemove key from the ParameterDict and return its parameter.\\nParameters\\nkey\\n(\\nstr'),\n",
       " Document(metadata={}, page_content='Parameters\\nkey\\n(\\nstr\\n) – key to pop from the ParameterDict\\nReturn type\\nAny\\npopitem\\n(\\n)\\n[source]\\n[source]\\n¶\\nRemove and return the last inserted\\n(key, parameter)\\npair from the ParameterDict.\\nReturn type\\nTuple\\n[\\nstr\\n,\\nAny\\n]\\nsetdefault\\n(\\nkey\\n,\\ndefault\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nSet the default for a key in the Parameterdict.\\nIf key is in the ParameterDict, return its value.\\nIf not, insert\\nkey\\nwith a parameter\\ndefault\\nand return\\ndefault\\n.\\ndefault\\ndefaults to\\nNone\\n.\\nParameters\\nkey\\n(\\nstr'),\n",
       " Document(metadata={}, page_content='.\\ndefault\\ndefaults to\\nNone\\n.\\nParameters\\nkey\\n(\\nstr\\n) – key to set default for\\ndefault\\n(\\nAny\\n) – the parameter set to the key\\nReturn type\\nAny\\nupdate\\n(\\nparameters\\n)\\n[source]\\n[source]\\n¶\\nUpdate the\\nParameterDict\\nwith key-value pairs from\\nparameters\\n, overwriting existing keys.\\nNote\\nIf\\nparameters\\nis an\\nOrderedDict\\n, a\\nParameterDict\\n, or\\nan iterable of key-value pairs, the order of new elements in it is preserved.\\nParameters\\nparameters\\n(\\niterable\\n) – a mapping (dictionary) from string to\\nParameter'),\n",
       " Document(metadata={}, page_content='Parameter\\n, or an iterable of\\nkey-value pairs of type (string,\\nParameter\\n)\\nvalues\\n(\\n)\\n[source]\\n[source]\\n¶\\nReturn an iterable of the ParameterDict values.\\nReturn type\\nIterable\\n[\\nAny\\n]\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_forward_pre_hook\\n¶\\ntorch.nn.modules.module.\\nregister_module_forward_pre_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a forward pre-hook common to all modules.\\nWarning\\nThis adds global state to the\\nnn.module\\nmodule\\nand it is only intended for debugging/profiling purposes.\\nThe hook will be called every time before\\nforward()\\nis invoked.\\nIt should have the following signature:\\nhook\\n(\\nmodule\\n,\\ninput\\n)\\n->\\nNone\\nor\\nmodified\\ninput'),\n",
       " Document(metadata={}, page_content='hook\\n(\\nmodule\\n,\\ninput\\n)\\n->\\nNone\\nor\\nmodified\\ninput\\nThe input contains only the positional arguments given to the module.\\nKeyword arguments won’t be passed to the hooks and only to the\\nforward\\n.\\nThe hook can modify the input. User can either return a tuple or a\\nsingle modified value in the hook. We will wrap the value into a tuple\\nif a single value is returned(unless that value is already a tuple).\\nThis hook has precedence over the specific module hooks registered with\\nregister_forward_pre_hook\\n.'),\n",
       " Document(metadata={}, page_content='register_forward_pre_hook\\n.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_forward_hook\\n¶\\ntorch.nn.modules.module.\\nregister_module_forward_hook\\n(\\nhook\\n,\\n*\\n,\\nwith_kwargs\\n=\\nFalse\\n,\\nalways_call\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nRegister a global forward hook for all the modules.\\nWarning\\nThis adds global state to the\\nnn.module\\nmodule\\nand it is only intended for debugging/profiling purposes.\\nThe hook will be called every time after\\nforward()\\nhas computed an output.\\nIt should have the following signature:\\nhook\\n(\\nmodule\\n,\\ninput\\n,\\noutput\\n)'),\n",
       " Document(metadata={}, page_content='hook\\n(\\nmodule\\n,\\ninput\\n,\\noutput\\n)\\n->\\nNone\\nor\\nmodified\\noutput\\nThe input contains only the positional arguments given to the module.\\nKeyword arguments won’t be passed to the hooks and only to the\\nforward\\n.\\nYou can optionally modify the output of the module by returning a new value\\nthat will replace the output from the\\nforward()\\nfunction.\\nParameters\\nhook\\n(\\nCallable\\n) – The user defined hook to be registered.\\nalways_call\\n(\\nbool\\n) – If\\nTrue\\nthe\\nhook\\nwill be run regardless of'),\n",
       " Document(metadata={}, page_content=') – If\\nTrue\\nthe\\nhook\\nwill be run regardless of\\nwhether an exception is raised while calling the Module.\\nDefault:\\nFalse\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nThis hook will be executed before specific module hooks registered with\\nregister_forward_hook\\n.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_backward_hook\\n¶\\ntorch.nn.modules.module.\\nregister_module_backward_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a backward hook common to all the modules.\\nThis function is deprecated in favor of\\ntorch.nn.modules.module.register_module_full_backward_hook()\\nand the behavior of this function will change in future versions.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nNext'),\n",
       " Document(metadata={}, page_content='torch.utils.hooks.RemovableHandle\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_full_backward_pre_hook\\n¶\\ntorch.nn.modules.module.\\nregister_module_full_backward_pre_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a backward pre-hook common to all the modules.\\nWarning\\nThis adds global state to the\\nnn.module\\nmodule\\nand it is only intended for debugging/profiling purposes.\\nHooks registered using this function behave in the same way as those\\nregistered by\\ntorch.nn.Module.register_full_backward_pre_hook()\\n.'),\n",
       " Document(metadata={}, page_content='.\\nRefer to its documentation for more details.\\nHooks registered using this function will be called before hooks registered\\nusing\\ntorch.nn.Module.register_full_backward_pre_hook()\\n.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_full_backward_hook\\n¶\\ntorch.nn.modules.module.\\nregister_module_full_backward_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a backward hook common to all the modules.\\nWarning\\nThis adds global state to the\\nnn.module\\nmodule\\nand it is only intended for debugging/profiling purposes.\\nHooks registered using this function behave in the same way as those\\nregistered by\\ntorch.nn.Module.register_full_backward_hook()\\n.\\nRefer to its documentation for more details.'),\n",
       " Document(metadata={}, page_content='.\\nRefer to its documentation for more details.\\nHooks registered using this function will be called before hooks registered\\nusing\\ntorch.nn.Module.register_full_backward_hook()\\n.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_buffer_registration_hook\\n¶\\ntorch.nn.modules.module.\\nregister_module_buffer_registration_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a buffer registration hook common to all modules.\\nWarning\\nThis adds global state to the\\nnn.Module\\nmodule\\nThe hook will be called every time\\nregister_buffer()\\nis invoked.\\nIt should have the following signature:\\nhook\\n(\\nmodule\\n,\\nname\\n,\\nbuffer\\n)\\n->\\nNone\\nor\\nnew\\nbuffer'),\n",
       " Document(metadata={}, page_content='(\\nmodule\\n,\\nname\\n,\\nbuffer\\n)\\n->\\nNone\\nor\\nnew\\nbuffer\\nThe hook can modify the input or return a single modified value in the hook.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_module_registration_hook\\n¶\\ntorch.nn.modules.module.\\nregister_module_module_registration_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a module registration hook common to all modules.\\nWarning\\nThis adds global state to the\\nnn.Module\\nmodule\\nThe hook will be called every time\\nregister_module()\\nis invoked.\\nIt should have the following signature:\\nhook\\n(\\nmodule\\n,\\nname\\n,\\nsubmodule\\n)\\n->\\nNone\\nor\\nnew\\nsubmodule'),\n",
       " Document(metadata={}, page_content=',\\nname\\n,\\nsubmodule\\n)\\n->\\nNone\\nor\\nnew\\nsubmodule\\nThe hook can modify the input or return a single modified value in the hook.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='torch.nn.modules.module.register_module_parameter_registration_hook\\n¶\\ntorch.nn.modules.module.\\nregister_module_parameter_registration_hook\\n(\\nhook\\n)\\n[source]\\n[source]\\n¶\\nRegister a parameter registration hook common to all modules.\\nWarning\\nThis adds global state to the\\nnn.Module\\nmodule\\nThe hook will be called every time\\nregister_parameter()\\nis invoked.\\nIt should have the following signature:\\nhook\\n(\\nmodule\\n,\\nname\\n,\\nparam\\n)\\n->\\nNone\\nor\\nnew\\nparameter'),\n",
       " Document(metadata={}, page_content='module\\n,\\nname\\n,\\nparam\\n)\\n->\\nNone\\nor\\nnew\\nparameter\\nThe hook can modify the input or return a single modified value in the hook.\\nReturns\\na handle that can be used to remove the added hook by calling\\nhandle.remove()\\nReturn type\\ntorch.utils.hooks.RemovableHandle\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"Conv1d\\n¶\\nclass\\ntorch.nn.\\nConv1d\\n(\\nin_channels\\n,\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\ndilation\\n=\\n1\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 1D convolution over an input signal composed of several input\\nplanes.\\nIn the simplest case, the output value of the layer with input size\\n(\\nN\\n,\\nC\\nin\\n,\\nL\\n)\\n(N, C_{\\\\text{in}}, L)\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nL\\n)\\nand output\\n(\\nN\\n,\\nC\\nout\\n,\\nL\\nout\\n)\\n(N, C_{\\\\text{out}}, L_{\\\\text{out}})\\n(\\nN\"),\n",
       " Document(metadata={}, page_content=',\\nL\\nout\\n)\\n(N, C_{\\\\text{out}}, L_{\\\\text{out}})\\n(\\nN\\n,\\nC\\nout\\n\\u200b\\n,\\nL\\nout\\n\\u200b\\n)\\ncan be\\nprecisely described as:\\nout\\n(\\nN\\ni\\n,\\nC\\nout\\nj\\n)\\n=\\nbias\\n(\\nC\\nout\\nj\\n)\\n+\\n∑\\nk\\n=\\n0\\nC\\ni\\nn\\n−\\n1\\nweight\\n(\\nC\\nout\\nj\\n,\\nk\\n)\\n⋆\\ninput\\n(\\nN\\ni\\n,\\nk\\n)\\n\\\\text{out}(N_i, C_{\\\\text{out}_j}) = \\\\text{bias}(C_{\\\\text{out}_j}) +\\n\\\\sum_{k = 0}^{C_{in} - 1} \\\\text{weight}(C_{\\\\text{out}_j}, k)\\n\\\\star \\\\text{input}(N_i, k)\\nout\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nout\\nj\\n\\u200b\\n\\u200b\\n)\\n=\\nbias\\n(\\nC\\nout\\nj\\n\\u200b\\n\\u200b\\n)\\n+\\nk\\n=\\n0\\n∑\\nC\\nin\\n\\u200b\\n−\\n1\\n\\u200b\\nweight\\n(\\nC\\nout\\nj\\n\\u200b\\n\\u200b\\n,\\nk\\n)\\n⋆\\ninput\\n(\\nN\\ni\\n\\u200b\\n,\\nk\\n)\\nwhere\\n⋆\\n\\\\star'),\n",
       " Document(metadata={}, page_content='j\\n\\u200b\\n\\u200b\\n,\\nk\\n)\\n⋆\\ninput\\n(\\nN\\ni\\n\\u200b\\n,\\nk\\n)\\nwhere\\n⋆\\n\\\\star\\n⋆\\nis the valid\\ncross-correlation\\noperator,\\nN\\nN\\nN\\nis a batch size,\\nC\\nC\\nC\\ndenotes a number of channels,\\nL\\nL\\nL\\nis a length of signal sequence.\\nThis module supports\\nTensorFloat32\\n.\\nOn certain ROCm devices, when using float16 inputs this module will use\\ndifferent precision\\nfor backward.\\nstride\\ncontrols the stride for the cross-correlation, a single\\nnumber or a one-element tuple.\\npadding\\ncontrols the amount of padding applied to the input. It'),\n",
       " Document(metadata={}, page_content='can be either a string {‘valid’, ‘same’} or a tuple of ints giving the\\namount of implicit padding applied on both sides.\\ndilation\\ncontrols the spacing between the kernel points; also\\nknown as the à trous algorithm. It is harder to describe, but this\\nlink\\nhas a nice visualization of what\\ndilation\\ndoes.\\ngroups\\ncontrols the connections between inputs and outputs.\\nin_channels\\nand\\nout_channels\\nmust both be divisible by\\ngroups\\n. For example,\\nAt groups=1, all inputs are convolved to all outputs.'),\n",
       " Document(metadata={}, page_content='At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.\\nAt groups=\\nin_channels\\n, each input channel is convolved with\\nits own set of filters (of size\\nout_channels\\nin_channels\\n\\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}\\nin_channels\\nout_channels\\n\\u200b\\n).\\nNote\\nWhen\\ngroups == in_channels\\nand\\nout_channels == K * in_channels\\n,\\nwhere\\nK'),\n",
       " Document(metadata={}, page_content='and\\nout_channels == K * in_channels\\n,\\nwhere\\nK\\nis a positive integer, this operation is also known as a “depthwise convolution”.\\nIn other words, for an input of size\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nL\\ni\\nn\\n)\\n(N, C_{in}, L_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nL\\nin\\n\\u200b\\n)\\n,\\na depthwise convolution with a depthwise multiplier\\nK\\ncan be performed with the arguments\\n(\\nC\\nin\\n=\\nC\\nin\\n,\\nC\\nout\\n=\\nC\\nin\\n×\\nK\\n,\\n.\\n.\\n.\\n,\\ngroups\\n=\\nC\\nin\\n)\\n(C_\\\\text{in}=C_\\\\text{in}, C_\\\\text{out}=C_\\\\text{in} \\\\times \\\\text{K}, ..., \\\\text{groups}=C_\\\\text{in})\\n(\\nC\\nin\\n\\u200b\\n=\\nC\\nin\\n\\u200b'),\n",
       " Document(metadata={}, page_content=\"(\\nC\\nin\\n\\u200b\\n=\\nC\\nin\\n\\u200b\\n,\\nC\\nout\\n\\u200b\\n=\\nC\\nin\\n\\u200b\\n×\\nK\\n,\\n...\\n,\\ngroups\\n=\\nC\\nin\\n\\u200b\\n)\\n.\\nNote\\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting\\ntorch.backends.cudnn.deterministic\\n=\\nTrue\\n. See\\nReproducibility\\nfor more information.\\nNote\\npadding='valid'\\nis the same as no padding.\\npadding='same'\"),\n",
       " Document(metadata={}, page_content=\"is the same as no padding.\\npadding='same'\\npads\\nthe input so the output has the shape as the input. However, this mode\\ndoesn’t support any stride values other than 1.\\nNote\\nThis module supports complex data types i.e.\\ncomplex32,\\ncomplex64,\\ncomplex128\\n.\\nParameters\\nin_channels\\n(\\nint\\n) – Number of channels in the input image\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\"),\n",
       " Document(metadata={}, page_content=\"stride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\n,\\ntuple\\nor\\nstr\\n,\\noptional\\n) – Padding added to both sides of\\nthe input. Default: 0\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel\\nelements. Default: 1\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input\\nchannels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the\\noutput. Default:\\nTrue\\npadding_mode\\n(\\nstr\\n,\\noptional\\n) –\\n'zeros'\\n,\"),\n",
       " Document(metadata={}, page_content=\"True\\npadding_mode\\n(\\nstr\\n,\\noptional\\n) –\\n'zeros'\\n,\\n'reflect'\\n,\\n'replicate'\\nor\\n'circular'\\n. Default:\\n'zeros'\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nL\\ni\\nn\\n)\\n(N, C_{in}, L_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nL\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\ni\\nn\\n,\\nL\\ni\\nn\\n)\\n(C_{in}, L_{in})\\n(\\nC\\nin\\n\\u200b\\n,\\nL\\nin\\n\\u200b\\n)\\nOutput:\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n,\\nL\\no\\nu\\nt\\n)\\n(N, C_{out}, L_{out})\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\no\\nu\\nt\\n,\\nL\\no\\nu\\nt\\n)\\n(C_{out}, L_{out})\\n(\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nL\\no\\nu\\nt\\n=\\n⌊\\nL\\ni\\nn\\n+\\n2\\n×\\npadding\\n−\\ndilation\\n×\\n(\\nkernel_size\\n−\\n1\\n)\\n−\\n1\\nstride\\n+\\n1\\n⌋\"),\n",
       " Document(metadata={}, page_content='−\\ndilation\\n×\\n(\\nkernel_size\\n−\\n1\\n)\\n−\\n1\\nstride\\n+\\n1\\n⌋\\nL_{out} = \\\\left\\\\lfloor\\\\frac{L_{in} + 2 \\\\times \\\\text{padding} - \\\\text{dilation}\\n          \\\\times (\\\\text{kernel\\\\_size} - 1) - 1}{\\\\text{stride}} + 1\\\\right\\\\rfloor\\nL\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\nL\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n−\\ndilation\\n×\\n(\\nkernel_size\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nVariables\\nweight\\n(\\nTensor\\n) – the learnable weights of the module of shape\\n(\\nout_channels\\n,\\nin_channels\\ngroups\\n,\\nkernel_size\\n)\\n(\\\\text{out\\\\_channels},'),\n",
       " Document(metadata={}, page_content='groups\\n,\\nkernel_size\\n)\\n(\\\\text{out\\\\_channels},\\n\\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}}, \\\\text{kernel\\\\_size})\\n(\\nout_channels\\n,\\ngroups\\nin_channels\\n\\u200b\\n,\\nkernel_size\\n)\\n.\\nThe values of these weights are sampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nin\\n∗\\nkernel_size\\nk = \\\\frac{groups}{C_\\\\text{in} * \\\\text{kernel\\\\_size}}\\nk\\n=\\nC\\nin\\n\\u200b\\n∗\\nkernel_size\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nbias\\n(\\nTensor\\n) – the learnable bias of the module of shape\\n(out_channels). If\\nbias\\nis\\nTrue'),\n",
       " Document(metadata={}, page_content='(out_channels). If\\nbias\\nis\\nTrue\\n, then the values of these weights are\\nsampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nin\\n∗\\nkernel_size\\nk = \\\\frac{groups}{C_\\\\text{in} * \\\\text{kernel\\\\_size}}\\nk\\n=\\nC\\nin\\n\\u200b\\n∗\\nkernel_size\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nConv1d\\n(\\n16\\n,\\n33\\n,\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme'),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"Conv2d\\n¶\\nclass\\ntorch.nn.\\nConv2d\\n(\\nin_channels\\n,\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\ndilation\\n=\\n1\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 2D convolution over an input signal composed of several input\\nplanes.\\nIn the simplest case, the output value of the layer with input size\\n(\\nN\\n,\\nC\\nin\\n,\\nH\\n,\\nW\\n)\\n(N, C_{\\\\text{in}}, H, W)\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nH\\n,\\nW\\n)\\nand output\\n(\\nN\\n,\\nC\\nout\\n,\\nH\\nout\\n,\\nW\\nout\\n)\"),\n",
       " Document(metadata={}, page_content='H\\n,\\nW\\n)\\nand output\\n(\\nN\\n,\\nC\\nout\\n,\\nH\\nout\\n,\\nW\\nout\\n)\\n(N, C_{\\\\text{out}}, H_{\\\\text{out}}, W_{\\\\text{out}})\\n(\\nN\\n,\\nC\\nout\\n\\u200b\\n,\\nH\\nout\\n\\u200b\\n,\\nW\\nout\\n\\u200b\\n)\\ncan be precisely described as:\\nout\\n(\\nN\\ni\\n,\\nC\\nout\\nj\\n)\\n=\\nbias\\n(\\nC\\nout\\nj\\n)\\n+\\n∑\\nk\\n=\\n0\\nC\\nin\\n−\\n1\\nweight\\n(\\nC\\nout\\nj\\n,\\nk\\n)\\n⋆\\ninput\\n(\\nN\\ni\\n,\\nk\\n)\\n\\\\text{out}(N_i, C_{\\\\text{out}_j}) = \\\\text{bias}(C_{\\\\text{out}_j}) +\\n\\\\sum_{k = 0}^{C_{\\\\text{in}} - 1} \\\\text{weight}(C_{\\\\text{out}_j}, k) \\\\star \\\\text{input}(N_i, k)\\nout\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nout\\nj\\n\\u200b\\n\\u200b\\n)\\n=\\nbias\\n(\\nC\\nout\\nj\\n\\u200b\\n\\u200b\\n)\\n+\\nk\\n=\\n0\\n∑\\nC\\nin'),\n",
       " Document(metadata={}, page_content='out\\nj\\n\\u200b\\n\\u200b\\n)\\n=\\nbias\\n(\\nC\\nout\\nj\\n\\u200b\\n\\u200b\\n)\\n+\\nk\\n=\\n0\\n∑\\nC\\nin\\n\\u200b\\n−\\n1\\n\\u200b\\nweight\\n(\\nC\\nout\\nj\\n\\u200b\\n\\u200b\\n,\\nk\\n)\\n⋆\\ninput\\n(\\nN\\ni\\n\\u200b\\n,\\nk\\n)\\nwhere\\n⋆\\n\\\\star\\n⋆\\nis the valid 2D\\ncross-correlation\\noperator,\\nN\\nN\\nN\\nis a batch size,\\nC\\nC\\nC\\ndenotes a number of channels,\\nH\\nH\\nH\\nis a height of input planes in pixels, and\\nW\\nW\\nW\\nis\\nwidth in pixels.\\nThis module supports\\nTensorFloat32\\n.\\nOn certain ROCm devices, when using float16 inputs this module will use\\ndifferent precision\\nfor backward.\\nstride'),\n",
       " Document(metadata={}, page_content='different precision\\nfor backward.\\nstride\\ncontrols the stride for the cross-correlation, a single\\nnumber or a tuple.\\npadding\\ncontrols the amount of padding applied to the input. It\\ncan be either a string {‘valid’, ‘same’} or an int / a tuple of ints giving the\\namount of implicit padding applied on both sides.\\ndilation\\ncontrols the spacing between the kernel points; also\\nknown as the à trous algorithm. It is harder to describe, but this\\nlink\\nhas a nice visualization of what\\ndilation\\ndoes.\\ngroups'),\n",
       " Document(metadata={}, page_content='dilation\\ndoes.\\ngroups\\ncontrols the connections between inputs and outputs.\\nin_channels\\nand\\nout_channels\\nmust both be divisible by\\ngroups\\n. For example,\\nAt groups=1, all inputs are convolved to all outputs.\\nAt groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.\\nAt groups=\\nin_channels\\n, each input channel is convolved with\\nits own set of filters (of size'),\n",
       " Document(metadata={}, page_content='its own set of filters (of size\\nout_channels\\nin_channels\\n\\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}\\nin_channels\\nout_channels\\n\\u200b\\n).\\nThe parameters\\nkernel_size\\n,\\nstride\\n,\\npadding\\n,\\ndilation\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the height and width dimension\\na\\ntuple\\nof two ints – in which case, the first\\nint\\nis used for the height dimension,\\nand the second\\nint\\nfor the width dimension\\nNote\\nWhen\\ngroups == in_channels\\nand\\nout_channels == K * in_channels\\n,\\nwhere'),\n",
       " Document(metadata={}, page_content='and\\nout_channels == K * in_channels\\n,\\nwhere\\nK\\nis a positive integer, this operation is also known as a “depthwise convolution”.\\nIn other words, for an input of size\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nL\\ni\\nn\\n)\\n(N, C_{in}, L_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nL\\nin\\n\\u200b\\n)\\n,\\na depthwise convolution with a depthwise multiplier\\nK\\ncan be performed with the arguments\\n(\\nC\\nin\\n=\\nC\\nin\\n,\\nC\\nout\\n=\\nC\\nin\\n×\\nK\\n,\\n.\\n.\\n.\\n,\\ngroups\\n=\\nC\\nin\\n)\\n(C_\\\\text{in}=C_\\\\text{in}, C_\\\\text{out}=C_\\\\text{in} \\\\times \\\\text{K}, ..., \\\\text{groups}=C_\\\\text{in})\\n(\\nC\\nin\\n\\u200b\\n=\\nC\\nin\\n\\u200b'),\n",
       " Document(metadata={}, page_content=\"(\\nC\\nin\\n\\u200b\\n=\\nC\\nin\\n\\u200b\\n,\\nC\\nout\\n\\u200b\\n=\\nC\\nin\\n\\u200b\\n×\\nK\\n,\\n...\\n,\\ngroups\\n=\\nC\\nin\\n\\u200b\\n)\\n.\\nNote\\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting\\ntorch.backends.cudnn.deterministic\\n=\\nTrue\\n. See\\nReproducibility\\nfor more information.\\nNote\\npadding='valid'\\nis the same as no padding.\\npadding='same'\"),\n",
       " Document(metadata={}, page_content=\"is the same as no padding.\\npadding='same'\\npads\\nthe input so the output has the shape as the input. However, this mode\\ndoesn’t support any stride values other than 1.\\nNote\\nThis module supports complex data types i.e.\\ncomplex32,\\ncomplex64,\\ncomplex128\\n.\\nParameters\\nin_channels\\n(\\nint\\n) – Number of channels in the input image\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\"),\n",
       " Document(metadata={}, page_content=\"stride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\n,\\ntuple\\nor\\nstr\\n,\\noptional\\n) – Padding added to all four sides of\\nthe input. Default: 0\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel elements. Default: 1\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input\\nchannels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the\\noutput. Default:\\nTrue\\npadding_mode\\n(\\nstr\\n,\\noptional\\n) –\\n'zeros'\\n,\"),\n",
       " Document(metadata={}, page_content=\"True\\npadding_mode\\n(\\nstr\\n,\\noptional\\n) –\\n'zeros'\\n,\\n'reflect'\\n,\\n'replicate'\\nor\\n'circular'\\n. Default:\\n'zeros'\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C_{in}, H_{in}, W_{in})\\n(\\nC\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nOutput:\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C_{out}, H_{out}, W_{out})\\n(\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\"),\n",
       " Document(metadata={}, page_content='(\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\n⌊\\nH\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\n0\\n]\\n+\\n1\\n⌋\\nH_{out} = \\\\left\\\\lfloor\\\\frac{H_{in}  + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0]\\n          \\\\times (\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n0\\n]\\nH\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nW\\no\\nu\\nt\\n=\\n⌊\\nW\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\ndilation\\n[\\n1\\n]\\n×\\n('),\n",
       " Document(metadata={}, page_content='⌊\\nW\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\n1\\n]\\n+\\n1\\n⌋\\nW_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}  + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1]\\n          \\\\times (\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n1\\n]\\nW\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nVariables\\nweight\\n(\\nTensor\\n) – the learnable weights of the module of shape\\n(\\nout_channels\\n,\\nin_channels\\ngroups\\n,'),\n",
       " Document(metadata={}, page_content='(\\nout_channels\\n,\\nin_channels\\ngroups\\n,\\n(\\\\text{out\\\\_channels}, \\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}},\\n(\\nout_channels\\n,\\ngroups\\nin_channels\\n\\u200b\\n,\\nkernel_size[0]\\n,\\nkernel_size[1]\\n)\\n\\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]})\\nkernel_size[0]\\n,\\nkernel_size[1]\\n)\\n.\\nThe values of these weights are sampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nin\\n∗\\n∏\\ni\\n=\\n0\\n1\\nkernel_size\\n[\\ni\\n]\\nk = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}'),\n",
       " Document(metadata={}, page_content='k\\n=\\nC\\nin\\n\\u200b\\n∗\\n∏\\ni\\n=\\n0\\n1\\n\\u200b\\nkernel_size\\n[\\ni\\n]\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nbias\\n(\\nTensor\\n) – the learnable bias of the module of shape\\n(out_channels). If\\nbias\\nis\\nTrue\\n,\\nthen the values of these weights are\\nsampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nin\\n∗\\n∏\\ni\\n=\\n0\\n1\\nkernel_size\\n[\\ni\\n]\\nk = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}\\nk\\n=\\nC\\nin\\n\\u200b\\n∗\\n∏\\ni\\n=\\n0\\n1\\n\\u200b\\nkernel_size\\n[\\ni\\n]\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nExamples\\n>>>'),\n",
       " Document(metadata={}, page_content='0\\n1\\n\\u200b\\nkernel_size\\n[\\ni\\n]\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nExamples\\n>>>\\n# With square kernels and equal stride\\n>>>\\nm\\n=\\nnn\\n.\\nConv2d\\n(\\n16\\n,\\n33\\n,\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# non-square kernels and unequal stride and with padding\\n>>>\\nm\\n=\\nnn\\n.\\nConv2d\\n(\\n16\\n,\\n33\\n,\\n(\\n3\\n,\\n5\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n),\\npadding\\n=\\n(\\n4\\n,\\n2\\n))\\n>>>\\n# non-square kernels and unequal stride and with padding and dilation\\n>>>\\nm\\n=\\nnn\\n.\\nConv2d\\n(\\n16\\n,\\n33\\n,\\n(\\n3\\n,\\n5\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n),\\npadding\\n=\\n(\\n4\\n,\\n2\\n),\\ndilation\\n=\\n(\\n3\\n,\\n1\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16'),\n",
       " Document(metadata={}, page_content='=\\n(\\n3\\n,\\n1\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n100\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"Conv3d\\n¶\\nclass\\ntorch.nn.\\nConv3d\\n(\\nin_channels\\n,\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\ndilation\\n=\\n1\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 3D convolution over an input signal composed of several input\\nplanes.\\nIn the simplest case, the output value of the layer with input size\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C_{in}, D, H, W)\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nand output\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\"),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\no\\nu\\nt\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C_{out}, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\ncan be precisely described as:\\no\\nu\\nt\\n(\\nN\\ni\\n,\\nC\\no\\nu\\nt\\nj\\n)\\n=\\nb\\ni\\na\\ns\\n(\\nC\\no\\nu\\nt\\nj\\n)\\n+\\n∑\\nk\\n=\\n0\\nC\\ni\\nn\\n−\\n1\\nw\\ne\\ni\\ng\\nh\\nt\\n(\\nC\\no\\nu\\nt\\nj\\n,\\nk\\n)\\n⋆\\ni\\nn\\np\\nu\\nt\\n(\\nN\\ni\\n,\\nk\\n)\\nout(N_i, C_{out_j}) = bias(C_{out_j}) +\\n                        \\\\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\\\star input(N_i, k)\\no\\nu\\nt\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\no\\nu\\nt\\nj\\n\\u200b\\n\\u200b\\n)\\n=\\nbia\\ns\\n(\\nC\\no\\nu\\nt\\nj\\n\\u200b\\n\\u200b\\n)\\n+\\nk\\n=\\n0\\n∑\\nC\\nin'),\n",
       " Document(metadata={}, page_content='j\\n\\u200b\\n\\u200b\\n)\\n=\\nbia\\ns\\n(\\nC\\no\\nu\\nt\\nj\\n\\u200b\\n\\u200b\\n)\\n+\\nk\\n=\\n0\\n∑\\nC\\nin\\n\\u200b\\n−\\n1\\n\\u200b\\nw\\ne\\ni\\ng\\nh\\nt\\n(\\nC\\no\\nu\\nt\\nj\\n\\u200b\\n\\u200b\\n,\\nk\\n)\\n⋆\\nin\\np\\nu\\nt\\n(\\nN\\ni\\n\\u200b\\n,\\nk\\n)\\nwhere\\n⋆\\n\\\\star\\n⋆\\nis the valid 3D\\ncross-correlation\\noperator\\nThis module supports\\nTensorFloat32\\n.\\nOn certain ROCm devices, when using float16 inputs this module will use\\ndifferent precision\\nfor backward.\\nstride\\ncontrols the stride for the cross-correlation.\\npadding\\ncontrols the amount of padding applied to the input. It'),\n",
       " Document(metadata={}, page_content='can be either a string {‘valid’, ‘same’} or a tuple of ints giving the\\namount of implicit padding applied on both sides.\\ndilation\\ncontrols the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but this\\nlink\\nhas a nice visualization of what\\ndilation\\ndoes.\\ngroups\\ncontrols the connections between inputs and outputs.\\nin_channels\\nand\\nout_channels\\nmust both be divisible by\\ngroups\\n. For example,\\nAt groups=1, all inputs are convolved to all outputs.'),\n",
       " Document(metadata={}, page_content='At groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.\\nAt groups=\\nin_channels\\n, each input channel is convolved with\\nits own set of filters (of size\\nout_channels\\nin_channels\\n\\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}\\nin_channels\\nout_channels\\n\\u200b\\n).\\nThe parameters\\nkernel_size\\n,\\nstride\\n,\\npadding\\n,\\ndilation\\ncan either be:\\na single\\nint'),\n",
       " Document(metadata={}, page_content=',\\npadding\\n,\\ndilation\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the depth, height and width dimension\\na\\ntuple\\nof three ints – in which case, the first\\nint\\nis used for the depth dimension,\\nthe second\\nint\\nfor the height dimension and the third\\nint\\nfor the width dimension\\nNote\\nWhen\\ngroups == in_channels\\nand\\nout_channels == K * in_channels\\n,\\nwhere\\nK\\nis a positive integer, this operation is also known as a “depthwise convolution”.\\nIn other words, for an input of size\\n(\\nN'),\n",
       " Document(metadata={}, page_content='In other words, for an input of size\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nL\\ni\\nn\\n)\\n(N, C_{in}, L_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nL\\nin\\n\\u200b\\n)\\n,\\na depthwise convolution with a depthwise multiplier\\nK\\ncan be performed with the arguments\\n(\\nC\\nin\\n=\\nC\\nin\\n,\\nC\\nout\\n=\\nC\\nin\\n×\\nK\\n,\\n.\\n.\\n.\\n,\\ngroups\\n=\\nC\\nin\\n)\\n(C_\\\\text{in}=C_\\\\text{in}, C_\\\\text{out}=C_\\\\text{in} \\\\times \\\\text{K}, ..., \\\\text{groups}=C_\\\\text{in})\\n(\\nC\\nin\\n\\u200b\\n=\\nC\\nin\\n\\u200b\\n,\\nC\\nout\\n\\u200b\\n=\\nC\\nin\\n\\u200b\\n×\\nK\\n,\\n...\\n,\\ngroups\\n=\\nC\\nin\\n\\u200b\\n)\\n.\\nNote'),\n",
       " Document(metadata={}, page_content=\"\\u200b\\n=\\nC\\nin\\n\\u200b\\n×\\nK\\n,\\n...\\n,\\ngroups\\n=\\nC\\nin\\n\\u200b\\n)\\n.\\nNote\\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting\\ntorch.backends.cudnn.deterministic\\n=\\nTrue\\n. See\\nReproducibility\\nfor more information.\\nNote\\npadding='valid'\\nis the same as no padding.\\npadding='same'\\npads\"),\n",
       " Document(metadata={}, page_content=\"is the same as no padding.\\npadding='same'\\npads\\nthe input so the output has the shape as the input. However, this mode\\ndoesn’t support any stride values other than 1.\\nNote\\nThis module supports complex data types i.e.\\ncomplex32,\\ncomplex64,\\ncomplex128\\n.\\nParameters\\nin_channels\\n(\\nint\\n) – Number of channels in the input image\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\"),\n",
       " Document(metadata={}, page_content=\"stride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\n,\\ntuple\\nor\\nstr\\n,\\noptional\\n) – Padding added to all six sides of\\nthe input. Default: 0\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel elements. Default: 1\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input channels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the output. Default:\\nTrue\\npadding_mode\\n(\\nstr\\n,\\noptional\\n) –\\n'zeros'\\n,\"),\n",
       " Document(metadata={}, page_content=\"True\\npadding_mode\\n(\\nstr\\n,\\noptional\\n) –\\n'zeros'\\n,\\n'reflect'\\n,\\n'replicate'\\nor\\n'circular'\\n. Default:\\n'zeros'\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C_{in}, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\ni\\nn\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C_{in}, D_{in}, H_{in}, W_{in})\\n(\\nC\\nin\\n\\u200b\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nOutput:\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C_{out}, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\"),\n",
       " Document(metadata={}, page_content='t\\n\\u200b\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\no\\nu\\nt\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C_{out}, D_{out}, H_{out}, W_{out})\\n(\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n,\\nwhere\\nD\\no\\nu\\nt\\n=\\n⌊\\nD\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\n0\\n]\\n+\\n1\\n⌋\\nD_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0]\\n      \\\\times (\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\nD\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n0\\n]\\nD\\nin\\n\\u200b\\n+\\n2\\n×'),\n",
       " Document(metadata={}, page_content='D\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n0\\n]\\nD\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nH\\no\\nu\\nt\\n=\\n⌊\\nH\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\n1\\n]\\n+\\n1\\n⌋\\nH_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1]\\n      \\\\times (\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n1\\n]\\nH\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nW\\no'),\n",
       " Document(metadata={}, page_content='[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nW\\no\\nu\\nt\\n=\\n⌊\\nW\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n2\\n]\\n−\\ndilation\\n[\\n2\\n]\\n×\\n(\\nkernel_size\\n[\\n2\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\n2\\n]\\n+\\n1\\n⌋\\nW_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] - \\\\text{dilation}[2]\\n      \\\\times (\\\\text{kernel\\\\_size}[2] - 1) - 1}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloor\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n2\\n]\\nW\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n2\\n]\\n−\\ndilation\\n[\\n2\\n]\\n×\\n(\\nkernel_size\\n[\\n2\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nVariables\\nweight\\n(\\nTensor'),\n",
       " Document(metadata={}, page_content='[\\n2\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nVariables\\nweight\\n(\\nTensor\\n) – the learnable weights of the module of shape\\n(\\nout_channels\\n,\\nin_channels\\ngroups\\n,\\n(\\\\text{out\\\\_channels}, \\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}},\\n(\\nout_channels\\n,\\ngroups\\nin_channels\\n\\u200b\\n,\\nkernel_size[0]\\n,\\nkernel_size[1]\\n,\\nkernel_size[2]\\n)\\n\\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]}, \\\\text{kernel\\\\_size[2]})\\nkernel_size[0]\\n,\\nkernel_size[1]\\n,\\nkernel_size[2]\\n)\\n.\\nThe values of these weights are sampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)'),\n",
       " Document(metadata={}, page_content='U\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nin\\n∗\\n∏\\ni\\n=\\n0\\n2\\nkernel_size\\n[\\ni\\n]\\nk = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}\\nk\\n=\\nC\\nin\\n\\u200b\\n∗\\n∏\\ni\\n=\\n0\\n2\\n\\u200b\\nkernel_size\\n[\\ni\\n]\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nbias\\n(\\nTensor\\n) – the learnable bias of the module of shape (out_channels). If\\nbias\\nis\\nTrue\\n,\\nthen the values of these weights are\\nsampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nin\\n∗\\n∏\\ni\\n=\\n0\\n2'),\n",
       " Document(metadata={}, page_content='\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nin\\n∗\\n∏\\ni\\n=\\n0\\n2\\nkernel_size\\n[\\ni\\n]\\nk = \\\\frac{groups}{C_\\\\text{in} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}\\nk\\n=\\nC\\nin\\n\\u200b\\n∗\\n∏\\ni\\n=\\n0\\n2\\n\\u200b\\nkernel_size\\n[\\ni\\n]\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nExamples:\\n>>>\\n# With square kernels and equal stride\\n>>>\\nm\\n=\\nnn\\n.\\nConv3d\\n(\\n16\\n,\\n33\\n,\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# non-square kernels and unequal stride and with padding\\n>>>\\nm\\n=\\nnn\\n.\\nConv3d\\n(\\n16\\n,\\n33\\n,\\n(\\n3\\n,\\n5\\n,\\n2\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n,\\n1\\n),\\npadding\\n=\\n(\\n4\\n,\\n2\\n,\\n0\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n10\\n,\\n50'),\n",
       " Document(metadata={}, page_content='))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n10\\n,\\n50\\n,\\n100\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"ConvTranspose1d\\n¶\\nclass\\ntorch.nn.\\nConvTranspose1d\\n(\\nin_channels\\n,\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\noutput_padding\\n=\\n0\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\ndilation\\n=\\n1\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 1D transposed convolution operator over an input image\\ncomposed of several input planes.\\nThis module can be seen as the gradient of Conv1d with respect to its input.\\nIt is also known as a fractionally-strided convolution or\"),\n",
       " Document(metadata={}, page_content='a deconvolution (although it is not an actual deconvolution operation as it does\\nnot compute a true inverse of convolution). For more information, see the visualizations\\nhere\\nand the\\nDeconvolutional Networks\\npaper.\\nThis module supports\\nTensorFloat32\\n.\\nOn certain ROCm devices, when using float16 inputs this module will use\\ndifferent precision\\nfor backward.\\nstride\\ncontrols the stride for the cross-correlation.\\npadding\\ncontrols the amount of implicit zero padding on both\\nsides for\\ndilation\\n*'),\n",
       " Document(metadata={}, page_content='sides for\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nnumber of points. See note\\nbelow for details.\\noutput_padding\\ncontrols the additional size added to one side\\nof the output shape. See note below for details.\\ndilation\\ncontrols the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but the link\\nhere\\nhas a nice visualization of what\\ndilation\\ndoes.\\ngroups\\ncontrols the connections between inputs and outputs.\\nin_channels\\nand\\nout_channels'),\n",
       " Document(metadata={}, page_content='in_channels\\nand\\nout_channels\\nmust both be divisible by\\ngroups\\n. For example,\\nAt groups=1, all inputs are convolved to all outputs.\\nAt groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.\\nAt groups=\\nin_channels\\n, each input channel is convolved with\\nits own set of filters (of size\\nout_channels\\nin_channels\\n\\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}'),\n",
       " Document(metadata={}, page_content='\\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}\\nin_channels\\nout_channels\\n\\u200b\\n).\\nNote\\nThe\\npadding\\nargument effectively adds\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\namount of zero padding to both sizes of the input. This is set so that\\nwhen a\\nConv1d\\nand a\\nConvTranspose1d\\nare initialized with same parameters, they are inverses of each other in\\nregard to the input and output shapes. However, when\\nstride\\n>\\n1\\n,\\nConv1d\\nmaps multiple input shapes to the same output\\nshape.\\noutput_padding'),\n",
       " Document(metadata={}, page_content='shape.\\noutput_padding\\nis provided to resolve this ambiguity by\\neffectively increasing the calculated output shape on one side. Note\\nthat\\noutput_padding\\nis only used to find output shape, but does\\nnot actually add zero-padding to output.\\nNote\\nIn some circumstances when using the CUDA backend with CuDNN, this operator\\nmay select a nondeterministic algorithm to increase performance. If this is\\nundesirable, you can try to make the operation deterministic (potentially at'),\n",
       " Document(metadata={}, page_content='a performance cost) by setting\\ntorch.backends.cudnn.deterministic\\n=\\nTrue\\n.\\nPlease see the notes on\\nReproducibility\\nfor background.\\nParameters\\nin_channels\\n(\\nint\\n) – Number of channels in the input image\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) –\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nzero-padding'),\n",
       " Document(metadata={}, page_content='*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nzero-padding\\nwill be added to both sides of the input. Default: 0\\noutput_padding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Additional size added to one side\\nof the output shape. Default: 0\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input channels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the output. Default:\\nTrue\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel elements. Default: 1\\nShape:'),\n",
       " Document(metadata={}, page_content='Shape:\\nInput:\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nL\\ni\\nn\\n)\\n(N, C_{in}, L_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nL\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\ni\\nn\\n,\\nL\\ni\\nn\\n)\\n(C_{in}, L_{in})\\n(\\nC\\nin\\n\\u200b\\n,\\nL\\nin\\n\\u200b\\n)\\nOutput:\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n,\\nL\\no\\nu\\nt\\n)\\n(N, C_{out}, L_{out})\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\no\\nu\\nt\\n,\\nL\\no\\nu\\nt\\n)\\n(C_{out}, L_{out})\\n(\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nL\\no\\nu\\nt\\n=\\n(\\nL\\ni\\nn\\n−\\n1\\n)\\n×\\nstride\\n−\\n2\\n×\\npadding\\n+\\ndilation\\n×\\n(\\nkernel_size\\n−\\n1\\n)\\n+\\noutput_padding\\n+\\n1\\nL_{out} = (L_{in} - 1) \\\\times \\\\text{stride} - 2 \\\\times \\\\text{padding} + \\\\text{dilation}'),\n",
       " Document(metadata={}, page_content='\\\\times (\\\\text{kernel\\\\_size} - 1) + \\\\text{output\\\\_padding} + 1\\nL\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nL\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n−\\n2\\n×\\npadding\\n+\\ndilation\\n×\\n(\\nkernel_size\\n−\\n1\\n)\\n+\\noutput_padding\\n+\\n1\\nVariables\\nweight\\n(\\nTensor\\n) – the learnable weights of the module of shape\\n(\\nin_channels\\n,\\nout_channels\\ngroups\\n,\\n(\\\\text{in\\\\_channels}, \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{groups}},\\n(\\nin_channels\\n,\\ngroups\\nout_channels\\n\\u200b\\n,\\nkernel_size\\n)\\n\\\\text{kernel\\\\_size})\\nkernel_size\\n)\\n.\\nThe values of these weights are sampled from\\nU\\n(\\n−\\nk'),\n",
       " Document(metadata={}, page_content='U\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\nkernel_size\\nk = \\\\frac{groups}{C_\\\\text{out} * \\\\text{kernel\\\\_size}}\\nk\\n=\\nC\\nout\\n\\u200b\\n∗\\nkernel_size\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nbias\\n(\\nTensor\\n) – the learnable bias of the module of shape (out_channels).\\nIf\\nbias\\nis\\nTrue\\n, then the values of these weights are\\nsampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\nkernel_size'),\n",
       " Document(metadata={}, page_content=',\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\nkernel_size\\nk = \\\\frac{groups}{C_\\\\text{out} * \\\\text{kernel\\\\_size}}\\nk\\n=\\nC\\nout\\n\\u200b\\n∗\\nkernel_size\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"ConvTranspose2d\\n¶\\nclass\\ntorch.nn.\\nConvTranspose2d\\n(\\nin_channels\\n,\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\noutput_padding\\n=\\n0\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\ndilation\\n=\\n1\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 2D transposed convolution operator over an input image\\ncomposed of several input planes.\\nThis module can be seen as the gradient of Conv2d with respect to its input.\\nIt is also known as a fractionally-strided convolution or\"),\n",
       " Document(metadata={}, page_content='a deconvolution (although it is not an actual deconvolution operation as it does\\nnot compute a true inverse of convolution). For more information, see the visualizations\\nhere\\nand the\\nDeconvolutional Networks\\npaper.\\nThis module supports\\nTensorFloat32\\n.\\nOn certain ROCm devices, when using float16 inputs this module will use\\ndifferent precision\\nfor backward.\\nstride\\ncontrols the stride for the cross-correlation.\\npadding\\ncontrols the amount of implicit zero padding on both\\nsides for\\ndilation\\n*'),\n",
       " Document(metadata={}, page_content='sides for\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nnumber of points. See note\\nbelow for details.\\noutput_padding\\ncontrols the additional size added to one side\\nof the output shape. See note below for details.\\ndilation\\ncontrols the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but the link\\nhere\\nhas a nice visualization of what\\ndilation\\ndoes.\\ngroups\\ncontrols the connections between inputs and outputs.\\nin_channels\\nand\\nout_channels'),\n",
       " Document(metadata={}, page_content='in_channels\\nand\\nout_channels\\nmust both be divisible by\\ngroups\\n. For example,\\nAt groups=1, all inputs are convolved to all outputs.\\nAt groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels\\nand producing half the output channels, and both subsequently\\nconcatenated.\\nAt groups=\\nin_channels\\n, each input channel is convolved with\\nits own set of filters (of size\\nout_channels\\nin_channels\\n\\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}'),\n",
       " Document(metadata={}, page_content='\\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}\\nin_channels\\nout_channels\\n\\u200b\\n).\\nThe parameters\\nkernel_size\\n,\\nstride\\n,\\npadding\\n,\\noutput_padding\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the height and width dimensions\\na\\ntuple\\nof two ints – in which case, the first\\nint\\nis used for the height dimension,\\nand the second\\nint\\nfor the width dimension\\nNote\\nThe\\npadding\\nargument effectively adds\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding'),\n",
       " Document(metadata={}, page_content='dilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\namount of zero padding to both sizes of the input. This is set so that\\nwhen a\\nConv2d\\nand a\\nConvTranspose2d\\nare initialized with same parameters, they are inverses of each other in\\nregard to the input and output shapes. However, when\\nstride\\n>\\n1\\n,\\nConv2d\\nmaps multiple input shapes to the same output\\nshape.\\noutput_padding\\nis provided to resolve this ambiguity by\\neffectively increasing the calculated output shape on one side. Note\\nthat\\noutput_padding'),\n",
       " Document(metadata={}, page_content='that\\noutput_padding\\nis only used to find output shape, but does\\nnot actually add zero-padding to output.\\nNote\\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting\\ntorch.backends.cudnn.deterministic\\n=\\nTrue\\n. See\\nReproducibility\\nfor more information.\\nParameters\\nin_channels\\n(\\nint'),\n",
       " Document(metadata={}, page_content='Parameters\\nin_channels\\n(\\nint\\n) – Number of channels in the input image\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) –\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nzero-padding\\nwill be added to both sides of each dimension in the input. Default: 0\\noutput_padding\\n(\\nint\\nor\\ntuple\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='output_padding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Additional size added to one side\\nof each dimension in the output shape. Default: 0\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input channels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the output. Default:\\nTrue\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel elements. Default: 1\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nH'),\n",
       " Document(metadata={}, page_content='n\\n)\\n(N, C_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C_{in}, H_{in}, W_{in})\\n(\\nC\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nOutput:\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C_{out}, H_{out}, W_{out})\\n(\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\n(\\nH\\ni\\nn\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n−\\n2\\n×\\npadding\\n[\\n0\\n]\\n+\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n0'),\n",
       " Document(metadata={}, page_content='×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n0\\n]\\n+\\n1\\nH_{out} = (H_{in} - 1) \\\\times \\\\text{stride}[0] - 2 \\\\times \\\\text{padding}[0] + \\\\text{dilation}[0]\\n          \\\\times (\\\\text{kernel\\\\_size}[0] - 1) + \\\\text{output\\\\_padding}[0] + 1\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nH\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n−\\n2\\n×\\npadding\\n[\\n0\\n]\\n+\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n0\\n]\\n+\\n1\\nW\\no\\nu\\nt\\n=\\n(\\nW\\ni\\nn\\n−\\n1\\n)\\n×\\nstride\\n[\\n1\\n]\\n−\\n2\\n×\\npadding\\n[\\n1\\n]\\n+\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n1\\n]\\n+\\n1'),\n",
       " Document(metadata={}, page_content='[\\n1\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n1\\n]\\n+\\n1\\nW_{out} = (W_{in} - 1) \\\\times \\\\text{stride}[1] - 2 \\\\times \\\\text{padding}[1] + \\\\text{dilation}[1]\\n          \\\\times (\\\\text{kernel\\\\_size}[1] - 1) + \\\\text{output\\\\_padding}[1] + 1\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nW\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n[\\n1\\n]\\n−\\n2\\n×\\npadding\\n[\\n1\\n]\\n+\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n1\\n]\\n+\\n1\\nVariables\\nweight\\n(\\nTensor\\n) – the learnable weights of the module of shape\\n(\\nin_channels\\n,\\nout_channels\\ngroups\\n,'),\n",
       " Document(metadata={}, page_content='(\\nin_channels\\n,\\nout_channels\\ngroups\\n,\\n(\\\\text{in\\\\_channels}, \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{groups}},\\n(\\nin_channels\\n,\\ngroups\\nout_channels\\n\\u200b\\n,\\nkernel_size[0]\\n,\\nkernel_size[1]\\n)\\n\\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]})\\nkernel_size[0]\\n,\\nkernel_size[1]\\n)\\n.\\nThe values of these weights are sampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\n∏\\ni\\n=\\n0\\n1\\nkernel_size\\n[\\ni\\n]'),\n",
       " Document(metadata={}, page_content='=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\n∏\\ni\\n=\\n0\\n1\\nkernel_size\\n[\\ni\\n]\\nk = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}\\nk\\n=\\nC\\nout\\n\\u200b\\n∗\\n∏\\ni\\n=\\n0\\n1\\n\\u200b\\nkernel_size\\n[\\ni\\n]\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nbias\\n(\\nTensor\\n) – the learnable bias of the module of shape (out_channels)\\nIf\\nbias\\nis\\nTrue\\n, then the values of these weights are\\nsampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\n∏\\ni\\n=\\n0\\n1\\nkernel_size\\n[\\ni\\n]'),\n",
       " Document(metadata={}, page_content='=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\n∏\\ni\\n=\\n0\\n1\\nkernel_size\\n[\\ni\\n]\\nk = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}\\nk\\n=\\nC\\nout\\n\\u200b\\n∗\\n∏\\ni\\n=\\n0\\n1\\n\\u200b\\nkernel_size\\n[\\ni\\n]\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nExamples:\\n>>>\\n# With square kernels and equal stride\\n>>>\\nm\\n=\\nnn\\n.\\nConvTranspose2d\\n(\\n16\\n,\\n33\\n,\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# non-square kernels and unequal stride and with padding\\n>>>\\nm\\n=\\nnn\\n.\\nConvTranspose2d\\n(\\n16\\n,\\n33\\n,\\n(\\n3\\n,\\n5\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n),\\npadding\\n=\\n(\\n4\\n,\\n2\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n100\\n)\\n>>>'),\n",
       " Document(metadata={}, page_content='input\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n100\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# exact output size can be also specified as an argument\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n16\\n,\\n12\\n,\\n12\\n)\\n>>>\\ndownsample\\n=\\nnn\\n.\\nConv2d\\n(\\n16\\n,\\n16\\n,\\n3\\n,\\nstride\\n=\\n2\\n,\\npadding\\n=\\n1\\n)\\n>>>\\nupsample\\n=\\nnn\\n.\\nConvTranspose2d\\n(\\n16\\n,\\n16\\n,\\n3\\n,\\nstride\\n=\\n2\\n,\\npadding\\n=\\n1\\n)\\n>>>\\nh\\n=\\ndownsample\\n(\\ninput\\n)\\n>>>\\nh\\n.\\nsize\\n()\\ntorch.Size([1, 16, 6, 6])\\n>>>\\noutput\\n=\\nupsample\\n(\\nh\\n,\\noutput_size\\n=\\ninput\\n.\\nsize\\n())\\n>>>\\noutput\\n.\\nsize\\n()'),\n",
       " Document(metadata={}, page_content='=\\ninput\\n.\\nsize\\n())\\n>>>\\noutput\\n.\\nsize\\n()\\ntorch.Size([1, 16, 12, 12])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"ConvTranspose3d\\n¶\\nclass\\ntorch.nn.\\nConvTranspose3d\\n(\\nin_channels\\n,\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\noutput_padding\\n=\\n0\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\ndilation\\n=\\n1\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 3D transposed convolution operator over an input image composed of several input\\nplanes.\\nThe transposed convolution operator multiplies each input value element-wise by a learnable kernel,\"),\n",
       " Document(metadata={}, page_content='and sums over the outputs from all input feature planes.\\nThis module can be seen as the gradient of Conv3d with respect to its input.\\nIt is also known as a fractionally-strided convolution or\\na deconvolution (although it is not an actual deconvolution operation as it does\\nnot compute a true inverse of convolution). For more information, see the visualizations\\nhere\\nand the\\nDeconvolutional Networks\\npaper.\\nThis module supports\\nTensorFloat32\\n.'),\n",
       " Document(metadata={}, page_content='paper.\\nThis module supports\\nTensorFloat32\\n.\\nOn certain ROCm devices, when using float16 inputs this module will use\\ndifferent precision\\nfor backward.\\nstride\\ncontrols the stride for the cross-correlation.\\npadding\\ncontrols the amount of implicit zero padding on both\\nsides for\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nnumber of points. See note\\nbelow for details.\\noutput_padding\\ncontrols the additional size added to one side\\nof the output shape. See note below for details.\\ndilation'),\n",
       " Document(metadata={}, page_content='dilation\\ncontrols the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but the link\\nhere\\nhas a nice visualization of what\\ndilation\\ndoes.\\ngroups\\ncontrols the connections between inputs and outputs.\\nin_channels\\nand\\nout_channels\\nmust both be divisible by\\ngroups\\n. For example,\\nAt groups=1, all inputs are convolved to all outputs.\\nAt groups=2, the operation becomes equivalent to having two conv\\nlayers side by side, each seeing half the input channels'),\n",
       " Document(metadata={}, page_content='and producing half the output channels, and both subsequently\\nconcatenated.\\nAt groups=\\nin_channels\\n, each input channel is convolved with\\nits own set of filters (of size\\nout_channels\\nin_channels\\n\\\\frac{\\\\text{out\\\\_channels}}{\\\\text{in\\\\_channels}}\\nin_channels\\nout_channels\\n\\u200b\\n).\\nThe parameters\\nkernel_size\\n,\\nstride\\n,\\npadding\\n,\\noutput_padding\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the depth, height and width dimensions\\na\\ntuple\\nof three ints – in which case, the first\\nint'),\n",
       " Document(metadata={}, page_content='of three ints – in which case, the first\\nint\\nis used for the depth dimension,\\nthe second\\nint\\nfor the height dimension and the third\\nint\\nfor the width dimension\\nNote\\nThe\\npadding\\nargument effectively adds\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\namount of zero padding to both sizes of the input. This is set so that\\nwhen a\\nConv3d\\nand a\\nConvTranspose3d\\nare initialized with same parameters, they are inverses of each other in\\nregard to the input and output shapes. However, when\\nstride\\n>\\n1\\n,\\nConv3d'),\n",
       " Document(metadata={}, page_content='stride\\n>\\n1\\n,\\nConv3d\\nmaps multiple input shapes to the same output\\nshape.\\noutput_padding\\nis provided to resolve this ambiguity by\\neffectively increasing the calculated output shape on one side. Note\\nthat\\noutput_padding\\nis only used to find output shape, but does\\nnot actually add zero-padding to output.\\nNote'),\n",
       " Document(metadata={}, page_content='not actually add zero-padding to output.\\nNote\\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting\\ntorch.backends.cudnn.deterministic\\n=\\nTrue\\n. See\\nReproducibility\\nfor more information.\\nParameters\\nin_channels\\n(\\nint\\n) – Number of channels in the input image\\nout_channels\\n(\\nint'),\n",
       " Document(metadata={}, page_content='out_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) –\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nzero-padding\\nwill be added to both sides of each dimension in the input. Default: 0\\noutput_padding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Additional size added to one side\\nof each dimension in the output shape. Default: 0'),\n",
       " Document(metadata={}, page_content='of each dimension in the output shape. Default: 0\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input channels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the output. Default:\\nTrue\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel elements. Default: 1\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\ni\\nn\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C_{in}, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\nin\\n\\u200b\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\ni\\nn\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)'),\n",
       " Document(metadata={}, page_content='\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\ni\\nn\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C_{in}, D_{in}, H_{in}, W_{in})\\n(\\nC\\nin\\n\\u200b\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nOutput:\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C_{out}, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\no\\nu\\nt\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C_{out}, D_{out}, H_{out}, W_{out})\\n(\\nC\\no\\nu\\nt\\n\\u200b\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nD\\no\\nu\\nt\\n=\\n(\\nD\\ni\\nn\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n−\\n2\\n×\\npadding\\n[\\n0\\n]\\n+\\ndilation\\n[\\n0\\n]\\n×\\n('),\n",
       " Document(metadata={}, page_content='[\\n0\\n]\\n−\\n2\\n×\\npadding\\n[\\n0\\n]\\n+\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n0\\n]\\n+\\n1\\nD_{out} = (D_{in} - 1) \\\\times \\\\text{stride}[0] - 2 \\\\times \\\\text{padding}[0] + \\\\text{dilation}[0]\\n          \\\\times (\\\\text{kernel\\\\_size}[0] - 1) + \\\\text{output\\\\_padding}[0] + 1\\nD\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nD\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n−\\n2\\n×\\npadding\\n[\\n0\\n]\\n+\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n0\\n]\\n+\\n1\\nH\\no\\nu\\nt\\n=\\n(\\nH\\ni\\nn\\n−\\n1\\n)\\n×\\nstride\\n[\\n1\\n]\\n−\\n2\\n×\\npadding\\n[\\n1\\n]\\n+\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size'),\n",
       " Document(metadata={}, page_content='×\\npadding\\n[\\n1\\n]\\n+\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n1\\n]\\n+\\n1\\nH_{out} = (H_{in} - 1) \\\\times \\\\text{stride}[1] - 2 \\\\times \\\\text{padding}[1] + \\\\text{dilation}[1]\\n          \\\\times (\\\\text{kernel\\\\_size}[1] - 1) + \\\\text{output\\\\_padding}[1] + 1\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nH\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n[\\n1\\n]\\n−\\n2\\n×\\npadding\\n[\\n1\\n]\\n+\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n1\\n]\\n+\\n1\\nW\\no\\nu\\nt\\n=\\n(\\nW\\ni\\nn\\n−\\n1\\n)\\n×\\nstride\\n[\\n2\\n]\\n−\\n2\\n×\\npadding\\n[\\n2\\n]\\n+\\ndilation\\n[\\n2\\n]\\n×\\n(\\nkernel_size\\n[\\n2\\n]\\n−\\n1'),\n",
       " Document(metadata={}, page_content='[\\n2\\n]\\n+\\ndilation\\n[\\n2\\n]\\n×\\n(\\nkernel_size\\n[\\n2\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n2\\n]\\n+\\n1\\nW_{out} = (W_{in} - 1) \\\\times \\\\text{stride}[2] - 2 \\\\times \\\\text{padding}[2] + \\\\text{dilation}[2]\\n          \\\\times (\\\\text{kernel\\\\_size}[2] - 1) + \\\\text{output\\\\_padding}[2] + 1\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nW\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n[\\n2\\n]\\n−\\n2\\n×\\npadding\\n[\\n2\\n]\\n+\\ndilation\\n[\\n2\\n]\\n×\\n(\\nkernel_size\\n[\\n2\\n]\\n−\\n1\\n)\\n+\\noutput_padding\\n[\\n2\\n]\\n+\\n1\\nVariables\\nweight\\n(\\nTensor\\n) – the learnable weights of the module of shape\\n(\\nin_channels\\n,\\nout_channels\\ngroups\\n,'),\n",
       " Document(metadata={}, page_content='(\\nin_channels\\n,\\nout_channels\\ngroups\\n,\\n(\\\\text{in\\\\_channels}, \\\\frac{\\\\text{out\\\\_channels}}{\\\\text{groups}},\\n(\\nin_channels\\n,\\ngroups\\nout_channels\\n\\u200b\\n,\\nkernel_size[0]\\n,\\nkernel_size[1]\\n,\\nkernel_size[2]\\n)\\n\\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]}, \\\\text{kernel\\\\_size[2]})\\nkernel_size[0]\\n,\\nkernel_size[1]\\n,\\nkernel_size[2]\\n)\\n.\\nThe values of these weights are sampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\n∏\\ni\\n=\\n0\\n2\\nkernel_size\\n[\\ni\\n]'),\n",
       " Document(metadata={}, page_content='=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\n∏\\ni\\n=\\n0\\n2\\nkernel_size\\n[\\ni\\n]\\nk = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}\\nk\\n=\\nC\\nout\\n\\u200b\\n∗\\n∏\\ni\\n=\\n0\\n2\\n\\u200b\\nkernel_size\\n[\\ni\\n]\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nbias\\n(\\nTensor\\n) – the learnable bias of the module of shape (out_channels)\\nIf\\nbias\\nis\\nTrue\\n, then the values of these weights are\\nsampled from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\n∏\\ni\\n=\\n0\\n2\\nkernel_size\\n[\\ni\\n]'),\n",
       " Document(metadata={}, page_content='=\\ng\\nr\\no\\nu\\np\\ns\\nC\\nout\\n∗\\n∏\\ni\\n=\\n0\\n2\\nkernel_size\\n[\\ni\\n]\\nk = \\\\frac{groups}{C_\\\\text{out} * \\\\prod_{i=0}^{2}\\\\text{kernel\\\\_size}[i]}\\nk\\n=\\nC\\nout\\n\\u200b\\n∗\\n∏\\ni\\n=\\n0\\n2\\n\\u200b\\nkernel_size\\n[\\ni\\n]\\ng\\nro\\nu\\np\\ns\\n\\u200b\\nExamples:\\n>>>\\n# With square kernels and equal stride\\n>>>\\nm\\n=\\nnn\\n.\\nConvTranspose3d\\n(\\n16\\n,\\n33\\n,\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# non-square kernels and unequal stride and with padding\\n>>>\\nm\\n=\\nnn\\n.\\nConvTranspose3d\\n(\\n16\\n,\\n33\\n,\\n(\\n3\\n,\\n5\\n,\\n2\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n,\\n1\\n),\\npadding\\n=\\n(\\n0\\n,\\n4\\n,\\n2\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n10\\n,'),\n",
       " Document(metadata={}, page_content=',\\n2\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n10\\n,\\n50\\n,\\n100\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"LazyConv1d\\n¶\\nclass\\ntorch.nn.\\nLazyConv1d\\n(\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\ndilation\\n=\\n1\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.Conv1d\\nmodule with lazy initialization of the\\nin_channels\\nargument.\\nThe\\nin_channels\\nargument of the\\nConv1d\\nis inferred from the\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\nand\\nbias\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\"),\n",
       " Document(metadata={}, page_content='.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Zero-padding added to both sides of\\nthe input. Default: 0\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel'),\n",
       " Document(metadata={}, page_content=\"or\\ntuple\\n,\\noptional\\n) – Spacing between kernel\\nelements. Default: 1\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input\\nchannels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the\\noutput. Default:\\nTrue\\npadding_mode\\n(\\nstr\\n,\\noptional\\n) –\\n'zeros'\\n,\\n'reflect'\\n,\\n'replicate'\\nor\\n'circular'\\n. Default:\\n'zeros'\\nSee also\\ntorch.nn.Conv1d\\nand\\ntorch.nn.modules.lazy.LazyModuleMixin\\ncls_to_become\\n[source]\\n¶\\nalias of\\nConv1d\\nNext\\nPrevious\"),\n",
       " Document(metadata={}, page_content='[source]\\n¶\\nalias of\\nConv1d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"LazyConv2d\\n¶\\nclass\\ntorch.nn.\\nLazyConv2d\\n(\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\ndilation\\n=\\n1\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.Conv2d\\nmodule with lazy initialization of the\\nin_channels\\nargument.\\nThe\\nin_channels\\nargument of the\\nConv2d\\nthat is inferred from the\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\nand\\nbias\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\"),\n",
       " Document(metadata={}, page_content='.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Zero-padding added to both sides of\\nthe input. Default: 0\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel'),\n",
       " Document(metadata={}, page_content=\"or\\ntuple\\n,\\noptional\\n) – Spacing between kernel\\nelements. Default: 1\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input\\nchannels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the\\noutput. Default:\\nTrue\\npadding_mode\\n(\\nstr\\n,\\noptional\\n) –\\n'zeros'\\n,\\n'reflect'\\n,\\n'replicate'\\nor\\n'circular'\\n. Default:\\n'zeros'\\nSee also\\ntorch.nn.Conv2d\\nand\\ntorch.nn.modules.lazy.LazyModuleMixin\\ncls_to_become\\n[source]\\n¶\\nalias of\\nConv2d\\nNext\\nPrevious\"),\n",
       " Document(metadata={}, page_content='[source]\\n¶\\nalias of\\nConv2d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"LazyConv3d\\n¶\\nclass\\ntorch.nn.\\nLazyConv3d\\n(\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\ndilation\\n=\\n1\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.Conv3d\\nmodule with lazy initialization of the\\nin_channels\\nargument.\\nThe\\nin_channels\\nargument of the\\nConv3d\\nthat is inferred from\\nthe\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\nand\\nbias\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\"),\n",
       " Document(metadata={}, page_content='.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Zero-padding added to both sides of\\nthe input. Default: 0\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel'),\n",
       " Document(metadata={}, page_content=\"or\\ntuple\\n,\\noptional\\n) – Spacing between kernel\\nelements. Default: 1\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input\\nchannels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the\\noutput. Default:\\nTrue\\npadding_mode\\n(\\nstr\\n,\\noptional\\n) –\\n'zeros'\\n,\\n'reflect'\\n,\\n'replicate'\\nor\\n'circular'\\n. Default:\\n'zeros'\\nSee also\\ntorch.nn.Conv3d\\nand\\ntorch.nn.modules.lazy.LazyModuleMixin\\ncls_to_become\\n[source]\\n¶\\nalias of\\nConv3d\\nNext\\nPrevious\"),\n",
       " Document(metadata={}, page_content='[source]\\n¶\\nalias of\\nConv3d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"LazyConvTranspose1d\\n¶\\nclass\\ntorch.nn.\\nLazyConvTranspose1d\\n(\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\noutput_padding\\n=\\n0\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\ndilation\\n=\\n1\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.ConvTranspose1d\\nmodule with lazy initialization of the\\nin_channels\\nargument.\\nThe\\nin_channels\\nargument of the\\nConvTranspose1d\\nthat is inferred from\\nthe\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\nand\\nbias\"),\n",
       " Document(metadata={}, page_content='weight\\nand\\nbias\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) –\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nzero-padding\\nwill be added to both sides of the input. Default: 0'),\n",
       " Document(metadata={}, page_content='output_padding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Additional size added to one side\\nof the output shape. Default: 0\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input channels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the output. Default:\\nTrue\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel elements. Default: 1\\nSee also\\ntorch.nn.ConvTranspose1d\\nand\\ntorch.nn.modules.lazy.LazyModuleMixin\\ncls_to_become\\n[source]\\n¶'),\n",
       " Document(metadata={}, page_content='cls_to_become\\n[source]\\n¶\\nalias of\\nConvTranspose1d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"LazyConvTranspose2d\\n¶\\nclass\\ntorch.nn.\\nLazyConvTranspose2d\\n(\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\noutput_padding\\n=\\n0\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\ndilation\\n=\\n1\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.ConvTranspose2d\\nmodule with lazy initialization of the\\nin_channels\\nargument.\\nThe\\nin_channels\\nargument of the\\nConvTranspose2d\\nis inferred from\\nthe\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\nand\\nbias\\n.\"),\n",
       " Document(metadata={}, page_content='weight\\nand\\nbias\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) –\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nzero-padding'),\n",
       " Document(metadata={}, page_content='*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nzero-padding\\nwill be added to both sides of each dimension in the input. Default: 0\\noutput_padding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Additional size added to one side\\nof each dimension in the output shape. Default: 0\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input channels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the output. Default:\\nTrue\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='True\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel elements. Default: 1\\nSee also\\ntorch.nn.ConvTranspose2d\\nand\\ntorch.nn.modules.lazy.LazyModuleMixin\\ncls_to_become\\n[source]\\n¶\\nalias of\\nConvTranspose2d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"LazyConvTranspose3d\\n¶\\nclass\\ntorch.nn.\\nLazyConvTranspose3d\\n(\\nout_channels\\n,\\nkernel_size\\n,\\nstride\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\noutput_padding\\n=\\n0\\n,\\ngroups\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\ndilation\\n=\\n1\\n,\\npadding_mode\\n=\\n'zeros'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.ConvTranspose3d\\nmodule with lazy initialization of the\\nin_channels\\nargument.\\nThe\\nin_channels\\nargument of the\\nConvTranspose3d\\nis inferred from\\nthe\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\nand\\nbias\\n.\"),\n",
       " Document(metadata={}, page_content='weight\\nand\\nbias\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nout_channels\\n(\\nint\\n) – Number of channels produced by the convolution\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the convolving kernel\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Stride of the convolution. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) –\\ndilation\\n*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nzero-padding'),\n",
       " Document(metadata={}, page_content='*\\n(kernel_size\\n-\\n1)\\n-\\npadding\\nzero-padding\\nwill be added to both sides of each dimension in the input. Default: 0\\noutput_padding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Additional size added to one side\\nof each dimension in the output shape. Default: 0\\ngroups\\n(\\nint\\n,\\noptional\\n) – Number of blocked connections from input channels to output channels. Default: 1\\nbias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a learnable bias to the output. Default:\\nTrue\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='True\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – Spacing between kernel elements. Default: 1\\nSee also\\ntorch.nn.ConvTranspose3d\\nand\\ntorch.nn.modules.lazy.LazyModuleMixin\\ncls_to_become\\n[source]\\n¶\\nalias of\\nConvTranspose3d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Unfold\\n¶\\nclass\\ntorch.nn.\\nUnfold\\n(\\nkernel_size\\n,\\ndilation\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\nstride\\n=\\n1\\n)\\n[source]\\n[source]\\n¶\\nExtracts sliding local blocks from a batched input tensor.\\nConsider a batched\\ninput\\ntensor of shape\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\n(N, C, *)\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\n,\\nwhere\\nN\\nN\\nN\\nis the batch dimension,\\nC\\nC\\nC\\nis the channel dimension,\\nand\\n∗\\n*\\n∗\\nrepresent arbitrary spatial dimensions. This operation flattens\\neach sliding\\nkernel_size\\n-sized block within the spatial dimensions\\nof\\ninput'),\n",
       " Document(metadata={}, page_content='of\\ninput\\ninto a column (i.e., last dimension) of a 3-D\\noutput\\ntensor of shape\\n(\\nN\\n,\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\n(N, C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)\\n(\\nN\\n,\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\n, where\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\nC \\\\times \\\\prod(\\\\text{kernel\\\\_size})\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\nis the total number of values\\nwithin each block (a block has\\n∏\\n(\\nkernel_size\\n)\\n\\\\prod(\\\\text{kernel\\\\_size})\\n∏\\n(\\nkernel_size\\n)\\nspatial\\nlocations each containing a\\nC\\nC\\nC\\n-channeled vector), and\\nL\\nL\\nL\\nis'),\n",
       " Document(metadata={}, page_content='C\\nC\\nC\\n-channeled vector), and\\nL\\nL\\nL\\nis\\nthe total number of such blocks:\\nL\\n=\\n∏\\nd\\n⌊\\nspatial_size\\n[\\nd\\n]\\n+\\n2\\n×\\npadding\\n[\\nd\\n]\\n−\\ndilation\\n[\\nd\\n]\\n×\\n(\\nkernel_size\\n[\\nd\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\nd\\n]\\n+\\n1\\n⌋\\n,\\nL = \\\\prod_d \\\\left\\\\lfloor\\\\frac{\\\\text{spatial\\\\_size}[d] + 2 \\\\times \\\\text{padding}[d] %\\n    - \\\\text{dilation}[d] \\\\times (\\\\text{kernel\\\\_size}[d] - 1) - 1}{\\\\text{stride}[d]} + 1\\\\right\\\\rfloor,\\nL\\n=\\nd\\n∏\\n\\u200b\\n⌊\\nstride\\n[\\nd\\n]\\nspatial_size\\n[\\nd\\n]\\n+\\n2\\n×\\npadding\\n[\\nd\\n]\\n−\\ndilation\\n[\\nd\\n]\\n×\\n(\\nkernel_size\\n[\\nd\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋'),\n",
       " Document(metadata={}, page_content='[\\nd\\n]\\n×\\n(\\nkernel_size\\n[\\nd\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\n,\\nwhere\\nspatial_size\\n\\\\text{spatial\\\\_size}\\nspatial_size\\nis formed by the spatial dimensions\\nof\\ninput\\n(\\n∗\\n*\\n∗\\nabove), and\\nd\\nd\\nd\\nis over all spatial\\ndimensions.\\nTherefore, indexing\\noutput\\nat the last dimension (column dimension)\\ngives all values within a certain block.\\nThe\\npadding\\n,\\nstride\\nand\\ndilation\\narguments specify\\nhow the sliding blocks are retrieved.\\nstride\\ncontrols the stride for the sliding blocks.\\npadding'),\n",
       " Document(metadata={}, page_content='padding\\ncontrols the amount of implicit zero-paddings on both\\nsides for\\npadding\\nnumber of points for each dimension before\\nreshaping.\\ndilation\\ncontrols the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but this\\nlink\\nhas a nice visualization of what\\ndilation\\ndoes.\\nParameters\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – the size of the sliding blocks\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – a parameter that controls the\\nstride of elements within the'),\n",
       " Document(metadata={}, page_content='stride of elements within the\\nneighborhood. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – implicit zero padding to be added on\\nboth sides of input. Default: 0\\nstride\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – the stride of the sliding blocks in the input\\nspatial dimensions. Default: 1\\nIf\\nkernel_size\\n,\\ndilation\\n,\\npadding\\nor\\nstride\\nis an int or a tuple of length 1, their values will be\\nreplicated across all spatial dimensions.\\nFor the case of two input spatial dimensions this operation is sometimes\\ncalled'),\n",
       " Document(metadata={}, page_content='called\\nim2col\\n.\\nNote\\nFold\\ncalculates each combined value in the resulting\\nlarge tensor by summing all values from all containing blocks.\\nUnfold\\nextracts the values in the local blocks by\\ncopying from the large tensor. So, if the blocks overlap, they are not\\ninverses of each other.\\nIn general, folding and unfolding operations are related as\\nfollows. Consider\\nFold\\nand\\nUnfold\\ninstances created with the same\\nparameters:\\n>>>\\nfold_params\\n=\\ndict\\n(\\nkernel_size\\n=...\\n,\\ndilation\\n=...\\n,\\npadding\\n=...\\n,'),\n",
       " Document(metadata={}, page_content='kernel_size\\n=...\\n,\\ndilation\\n=...\\n,\\npadding\\n=...\\n,\\nstride\\n=...\\n)\\n>>>\\nfold\\n=\\nnn\\n.\\nFold\\n(\\noutput_size\\n=...\\n,\\n**\\nfold_params\\n)\\n>>>\\nunfold\\n=\\nnn\\n.\\nUnfold\\n(\\n**\\nfold_params\\n)\\nThen for any (supported)\\ninput\\ntensor the following\\nequality holds:\\nfold\\n(\\nunfold\\n(\\ninput\\n))\\n==\\ndivisor\\n*\\ninput\\nwhere\\ndivisor\\nis a tensor that depends only on the shape\\nand dtype of the\\ninput\\n:\\n>>>\\ninput_ones\\n=\\ntorch\\n.\\nones\\n(\\ninput\\n.\\nshape\\n,\\ndtype\\n=\\ninput\\n.\\ndtype\\n)\\n>>>\\ndivisor\\n=\\nfold\\n(\\nunfold\\n(\\ninput_ones\\n))\\nWhen the\\ndivisor'),\n",
       " Document(metadata={}, page_content='=\\nfold\\n(\\nunfold\\n(\\ninput_ones\\n))\\nWhen the\\ndivisor\\ntensor contains no zero elements, then\\nfold\\nand\\nunfold\\noperations are inverses of each\\nother (up to constant divisor).\\nWarning\\nCurrently, only 4-D input tensors (batched image-like tensors) are\\nsupported.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\n(N, C, *)\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\n(N, C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)\\n(\\nN\\n,\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\nas described above\\nExamples:\\n>>>\\nunfold\\n=\\nnn\\n.\\nUnfold\\n(\\nkernel_size\\n=\\n(\\n2'),\n",
       " Document(metadata={}, page_content='>>>\\nunfold\\n=\\nnn\\n.\\nUnfold\\n(\\nkernel_size\\n=\\n(\\n2\\n,\\n3\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n5\\n,\\n3\\n,\\n4\\n)\\n>>>\\noutput\\n=\\nunfold\\n(\\ninput\\n)\\n>>>\\n# each patch contains 30 values (2x3=6 vectors, each of 5 channels)\\n>>>\\n# 4 blocks (2x3 kernels) in total in the 3x4 input\\n>>>\\noutput\\n.\\nsize\\n()\\ntorch.Size([2, 30, 4])\\n>>>\\n# Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)\\n>>>\\ninp\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n3\\n,\\n10\\n,\\n12\\n)\\n>>>\\nw\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n,\\n4\\n,\\n5\\n)\\n>>>\\ninp_unf\\n='),\n",
       " Document(metadata={}, page_content='w\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n,\\n4\\n,\\n5\\n)\\n>>>\\ninp_unf\\n=\\ntorch\\n.\\nnn\\n.\\nfunctional\\n.\\nunfold\\n(\\ninp\\n,\\n(\\n4\\n,\\n5\\n))\\n>>>\\nout_unf\\n=\\ninp_unf\\n.\\ntranspose\\n(\\n1\\n,\\n2\\n)\\n.\\nmatmul\\n(\\nw\\n.\\nview\\n(\\nw\\n.\\nsize\\n(\\n0\\n),\\n-\\n1\\n)\\n.\\nt\\n())\\n.\\ntranspose\\n(\\n1\\n,\\n2\\n)\\n>>>\\nout\\n=\\ntorch\\n.\\nnn\\n.\\nfunctional\\n.\\nfold\\n(\\nout_unf\\n,\\n(\\n7\\n,\\n8\\n),\\n(\\n1\\n,\\n1\\n))\\n>>>\\n# or equivalently (and avoiding a copy),\\n>>>\\n# out = out_unf.view(1, 2, 7, 8)\\n>>>\\n(\\ntorch\\n.\\nnn\\n.\\nfunctional\\n.\\nconv2d\\n(\\ninp\\n,\\nw\\n)\\n-\\nout\\n)\\n.\\nabs\\n()\\n.\\nmax\\n()\\ntensor(1.9073e-06)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='abs\\n()\\n.\\nmax\\n()\\ntensor(1.9073e-06)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Fold\\n¶\\nclass\\ntorch.nn.\\nFold\\n(\\noutput_size\\n,\\nkernel_size\\n,\\ndilation\\n=\\n1\\n,\\npadding\\n=\\n0\\n,\\nstride\\n=\\n1\\n)\\n[source]\\n[source]\\n¶\\nCombines an array of sliding local blocks into a large containing tensor.\\nConsider a batched\\ninput\\ntensor containing sliding local blocks,\\ne.g., patches of images, of shape\\n(\\nN\\n,\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\n(N, C \\\\times  \\\\prod(\\\\text{kernel\\\\_size}), L)\\n(\\nN\\n,\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\n,\\nwhere\\nN\\nN\\nN\\nis batch dimension,\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\nC \\\\times \\\\prod(\\\\text{kernel\\\\_size})\\nC'),\n",
       " Document(metadata={}, page_content=')\\nC \\\\times \\\\prod(\\\\text{kernel\\\\_size})\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\nis the number of values within a block (a block has\\n∏\\n(\\nkernel_size\\n)\\n\\\\prod(\\\\text{kernel\\\\_size})\\n∏\\n(\\nkernel_size\\n)\\nspatial locations each containing a\\nC\\nC\\nC\\n-channeled vector), and\\nL\\nL\\nL\\nis the total number of blocks. (This is exactly the\\nsame specification as the output shape of\\nUnfold\\n.) This\\noperation combines these local blocks into the large\\noutput\\ntensor\\nof shape\\n(\\nN\\n,\\nC\\n,\\noutput_size\\n[\\n0\\n]\\n,\\noutput_size\\n[\\n1\\n]\\n,\\n…\\n)'),\n",
       " Document(metadata={}, page_content=',\\nC\\n,\\noutput_size\\n[\\n0\\n]\\n,\\noutput_size\\n[\\n1\\n]\\n,\\n…\\n)\\n(N, C, \\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1], \\\\dots)\\n(\\nN\\n,\\nC\\n,\\noutput_size\\n[\\n0\\n]\\n,\\noutput_size\\n[\\n1\\n]\\n,\\n…\\n)\\nby summing the overlapping values. Similar to\\nUnfold\\n, the\\narguments must satisfy\\nL\\n=\\n∏\\nd\\n⌊\\noutput_size\\n[\\nd\\n]\\n+\\n2\\n×\\npadding\\n[\\nd\\n]\\n−\\ndilation\\n[\\nd\\n]\\n×\\n(\\nkernel_size\\n[\\nd\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\nd\\n]\\n+\\n1\\n⌋\\n,\\nL = \\\\prod_d \\\\left\\\\lfloor\\\\frac{\\\\text{output\\\\_size}[d] + 2 \\\\times \\\\text{padding}[d] %'),\n",
       " Document(metadata={}, page_content='- \\\\text{dilation}[d] \\\\times (\\\\text{kernel\\\\_size}[d] - 1) - 1}{\\\\text{stride}[d]} + 1\\\\right\\\\rfloor,\\nL\\n=\\nd\\n∏\\n\\u200b\\n⌊\\nstride\\n[\\nd\\n]\\noutput_size\\n[\\nd\\n]\\n+\\n2\\n×\\npadding\\n[\\nd\\n]\\n−\\ndilation\\n[\\nd\\n]\\n×\\n(\\nkernel_size\\n[\\nd\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\n,\\nwhere\\nd\\nd\\nd\\nis over all spatial dimensions.\\noutput_size\\ndescribes the spatial shape of the large containing\\ntensor of the sliding local blocks. It is useful to resolve the ambiguity\\nwhen multiple input shapes map to same number of sliding blocks, e.g.,\\nwith\\nstride\\n>\\n0\\n.\\nThe'),\n",
       " Document(metadata={}, page_content='with\\nstride\\n>\\n0\\n.\\nThe\\npadding\\n,\\nstride\\nand\\ndilation\\narguments specify\\nhow the sliding blocks are retrieved.\\nstride\\ncontrols the stride for the sliding blocks.\\npadding\\ncontrols the amount of implicit zero-paddings on both\\nsides for\\npadding\\nnumber of points for each dimension before\\nreshaping.\\ndilation\\ncontrols the spacing between the kernel points; also known as the à trous algorithm.\\nIt is harder to describe, but this\\nlink\\nhas a nice visualization of what\\ndilation\\ndoes.\\nParameters\\noutput_size\\n('),\n",
       " Document(metadata={}, page_content='dilation\\ndoes.\\nParameters\\noutput_size\\n(\\nint\\nor\\ntuple\\n) – the shape of the spatial dimensions of the\\noutput (i.e.,\\noutput.sizes()[2:]\\n)\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – the size of the sliding blocks\\ndilation\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – a parameter that controls the\\nstride of elements within the\\nneighborhood. Default: 1\\npadding\\n(\\nint\\nor\\ntuple\\n,\\noptional\\n) – implicit zero padding to be added on\\nboth sides of input. Default: 0\\nstride\\n(\\nint\\nor\\ntuple\\n) – the stride of the sliding blocks in the input'),\n",
       " Document(metadata={}, page_content=') – the stride of the sliding blocks in the input\\nspatial dimensions. Default: 1\\nIf\\noutput_size\\n,\\nkernel_size\\n,\\ndilation\\n,\\npadding\\nor\\nstride\\nis an int or a tuple of length 1 then\\ntheir values will be replicated across all spatial dimensions.\\nFor the case of two output spatial dimensions this operation is sometimes\\ncalled\\ncol2im\\n.\\nNote\\nFold\\ncalculates each combined value in the resulting\\nlarge tensor by summing all values from all containing blocks.\\nUnfold'),\n",
       " Document(metadata={}, page_content='Unfold\\nextracts the values in the local blocks by\\ncopying from the large tensor. So, if the blocks overlap, they are not\\ninverses of each other.\\nIn general, folding and unfolding operations are related as\\nfollows. Consider\\nFold\\nand\\nUnfold\\ninstances created with the same\\nparameters:\\n>>>\\nfold_params\\n=\\ndict\\n(\\nkernel_size\\n=...\\n,\\ndilation\\n=...\\n,\\npadding\\n=...\\n,\\nstride\\n=...\\n)\\n>>>\\nfold\\n=\\nnn\\n.\\nFold\\n(\\noutput_size\\n=...\\n,\\n**\\nfold_params\\n)\\n>>>\\nunfold\\n=\\nnn\\n.\\nUnfold\\n(\\n**\\nfold_params\\n)\\nThen for any (supported)'),\n",
       " Document(metadata={}, page_content='(\\n**\\nfold_params\\n)\\nThen for any (supported)\\ninput\\ntensor the following\\nequality holds:\\nfold\\n(\\nunfold\\n(\\ninput\\n))\\n==\\ndivisor\\n*\\ninput\\nwhere\\ndivisor\\nis a tensor that depends only on the shape\\nand dtype of the\\ninput\\n:\\n>>>\\ninput_ones\\n=\\ntorch\\n.\\nones\\n(\\ninput\\n.\\nshape\\n,\\ndtype\\n=\\ninput\\n.\\ndtype\\n)\\n>>>\\ndivisor\\n=\\nfold\\n(\\nunfold\\n(\\ninput_ones\\n))\\nWhen the\\ndivisor\\ntensor contains no zero elements, then\\nfold\\nand\\nunfold\\noperations are inverses of each\\nother (up to constant divisor).\\nWarning'),\n",
       " Document(metadata={}, page_content='other (up to constant divisor).\\nWarning\\nCurrently, only unbatched (3D) or batched (4D) image-like output tensors are supported.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\n(N, C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)\\n(\\nN\\n,\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\nor\\n(\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\n(C \\\\times \\\\prod(\\\\text{kernel\\\\_size}), L)\\n(\\nC\\n×\\n∏\\n(\\nkernel_size\\n)\\n,\\nL\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\noutput_size\\n[\\n0\\n]\\n,\\noutput_size\\n[\\n1\\n]\\n,\\n…\\n)\\n(N, C, \\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1], \\\\dots)\\n(\\nN\\n,\\nC\\n,'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\noutput_size\\n[\\n0\\n]\\n,\\noutput_size\\n[\\n1\\n]\\n,\\n…\\n)\\nor\\n(\\nC\\n,\\noutput_size\\n[\\n0\\n]\\n,\\noutput_size\\n[\\n1\\n]\\n,\\n…\\n)\\n(C, \\\\text{output\\\\_size}[0], \\\\text{output\\\\_size}[1], \\\\dots)\\n(\\nC\\n,\\noutput_size\\n[\\n0\\n]\\n,\\noutput_size\\n[\\n1\\n]\\n,\\n…\\n)\\nas described above\\nExamples:\\n>>>\\nfold\\n=\\nnn\\n.\\nFold\\n(\\noutput_size\\n=\\n(\\n4\\n,\\n5\\n),\\nkernel_size\\n=\\n(\\n2\\n,\\n2\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n3\\n*\\n2\\n*\\n2\\n,\\n12\\n)\\n>>>\\noutput\\n=\\nfold\\n(\\ninput\\n)\\n>>>\\noutput\\n.\\nsize\\n()\\ntorch.Size([1, 3, 4, 5])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.'),\n",
       " Document(metadata={}, page_content='Previous\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='MaxPool1d\\n¶\\nclass\\ntorch.nn.\\nMaxPool1d\\n(\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\npadding\\n=\\n0\\n,\\ndilation\\n=\\n1\\n,\\nreturn_indices\\n=\\nFalse\\n,\\nceil_mode\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies a 1D max pooling over an input signal composed of several input planes.\\nIn the simplest case, the output value of the layer with input size\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\nand output\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(N, C, L_{out})\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\ncan be precisely described as:\\no\\nu\\nt\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nk\\n)\\n=\\nmax\\n\\u2061\\nm\\n=\\n0\\n,\\n…\\n,'),\n",
       " Document(metadata={}, page_content='o\\nu\\nt\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nk\\n)\\n=\\nmax\\n\\u2061\\nm\\n=\\n0\\n,\\n…\\n,\\nkernel_size\\n−\\n1\\ni\\nn\\np\\nu\\nt\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\ns\\nt\\nr\\ni\\nd\\ne\\n×\\nk\\n+\\nm\\n)\\nout(N_i, C_j, k) = \\\\max_{m=0, \\\\ldots, \\\\text{kernel\\\\_size} - 1}\\n        input(N_i, C_j, stride \\\\times k + m)\\no\\nu\\nt\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nk\\n)\\n=\\nm\\n=\\n0\\n,\\n…\\n,\\nkernel_size\\n−\\n1\\nmax\\n\\u200b\\nin\\np\\nu\\nt\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\ns\\nt\\nr\\ni\\nd\\ne\\n×\\nk\\n+\\nm\\n)\\nIf\\npadding\\nis non-zero, then the input is implicitly padded with negative infinity on both sides\\nfor\\npadding\\nnumber of points.\\ndilation'),\n",
       " Document(metadata={}, page_content='for\\npadding\\nnumber of points.\\ndilation\\nis the stride between the elements within the\\nsliding window. This\\nlink\\nhas a nice visualization of the pooling parameters.\\nNote\\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – The size of the sliding window, must be > 0.\\nstride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]'),\n",
       " Document(metadata={}, page_content='stride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – The stride of the sliding window, must be > 0. Default value is\\nkernel_size\\n.\\npadding\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\\ndilation\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – The stride between elements within a sliding window, must be > 0.\\nreturn_indices\\n(\\nbool\\n) – If\\nTrue\\n, will return the argmax along with the max values.\\nUseful for\\ntorch.nn.MaxUnpool1d\\nlater'),\n",
       " Document(metadata={}, page_content='Useful for\\ntorch.nn.MaxUnpool1d\\nlater\\nceil_mode\\n(\\nbool\\n) – If\\nTrue\\n, will use\\nceil\\ninstead of\\nfloor\\nto compute the output shape. This\\nensures that every element in the input tensor is covered by a sliding window.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nL\\ni\\nn\\n)\\n(N, C, L_{in})\\n(\\nN\\n,\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\ni\\nn\\n)\\n(C, L_{in})\\n(\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(N, C, L_{out})\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(C, L_{out})\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nL\\no\\nu\\nt\\n=\\n⌊\\nL\\ni\\nn\\n+\\n2\\n×\\npadding\\n−\\ndilation\\n×\\n('),\n",
       " Document(metadata={}, page_content='L\\no\\nu\\nt\\n=\\n⌊\\nL\\ni\\nn\\n+\\n2\\n×\\npadding\\n−\\ndilation\\n×\\n(\\nkernel_size\\n−\\n1\\n)\\n−\\n1\\nstride\\n+\\n1\\n⌋\\nL_{out} = \\\\left\\\\lfloor \\\\frac{L_{in} + 2 \\\\times \\\\text{padding} - \\\\text{dilation}\\n      \\\\times (\\\\text{kernel\\\\_size} - 1) - 1}{\\\\text{stride}} + 1\\\\right\\\\rfloor\\nL\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\nL\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n−\\ndilation\\n×\\n(\\nkernel_size\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nExamples:\\n>>>\\n# pool of size=3, stride=2\\n>>>\\nm\\n=\\nnn\\n.\\nMaxPool1d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='16\\n,\\n50\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='MaxPool2d\\n¶\\nclass\\ntorch.nn.\\nMaxPool2d\\n(\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\npadding\\n=\\n0\\n,\\ndilation\\n=\\n1\\n,\\nreturn_indices\\n=\\nFalse\\n,\\nceil_mode\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies a 2D max pooling over an input signal composed of several input planes.\\nIn the simplest case, the output value of the layer with input size\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n,\\noutput\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nand\\nkernel_size\\n(\\nk\\nH\\n,\\nk\\nW\\n)\\n(kH, kW)\\n('),\n",
       " Document(metadata={}, page_content='u\\nt\\n\\u200b\\n)\\nand\\nkernel_size\\n(\\nk\\nH\\n,\\nk\\nW\\n)\\n(kH, kW)\\n(\\nk\\nH\\n,\\nkW\\n)\\ncan be precisely described as:\\no\\nu\\nt\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nh\\n,\\nw\\n)\\n=\\nmax\\n\\u2061\\nm\\n=\\n0\\n,\\n…\\n,\\nk\\nH\\n−\\n1\\nmax\\n\\u2061\\nn\\n=\\n0\\n,\\n…\\n,\\nk\\nW\\n−\\n1\\ninput\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nstride[0]\\n×\\nh\\n+\\nm\\n,\\nstride[1]\\n×\\nw\\n+\\nn\\n)\\n\\\\begin{aligned}\\n    out(N_i, C_j, h, w) ={} & \\\\max_{m=0, \\\\ldots, kH-1} \\\\max_{n=0, \\\\ldots, kW-1} \\\\\\\\\\n                            & \\\\text{input}(N_i, C_j, \\\\text{stride[0]} \\\\times h + m,\\n                                           \\\\text{stride[1]} \\\\times w + n)'),\n",
       " Document(metadata={}, page_content='\\\\end{aligned}\\no\\nu\\nt\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nh\\n,\\nw\\n)\\n=\\n\\u200b\\nm\\n=\\n0\\n,\\n…\\n,\\nk\\nH\\n−\\n1\\nmax\\n\\u200b\\nn\\n=\\n0\\n,\\n…\\n,\\nkW\\n−\\n1\\nmax\\n\\u200b\\ninput\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nstride[0]\\n×\\nh\\n+\\nm\\n,\\nstride[1]\\n×\\nw\\n+\\nn\\n)\\n\\u200b\\nIf\\npadding\\nis non-zero, then the input is implicitly padded with negative infinity on both sides\\nfor\\npadding\\nnumber of points.\\ndilation\\ncontrols the spacing between the kernel points.\\nIt is harder to describe, but this\\nlink\\nhas a nice visualization of what\\ndilation\\ndoes.\\nNote'),\n",
       " Document(metadata={}, page_content='dilation\\ndoes.\\nNote\\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.\\nThe parameters\\nkernel_size\\n,\\nstride\\n,\\npadding\\n,\\ndilation\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the height and width dimension\\na\\ntuple\\nof two ints – in which case, the first\\nint\\nis used for the height dimension,\\nand the second\\nint\\nfor the width dimension'),\n",
       " Document(metadata={}, page_content='and the second\\nint\\nfor the width dimension\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – the size of the window to take a max over\\nstride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – the stride of the window. Default value is\\nkernel_size\\npadding\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – Implicit negative infinity padding to be added on both sides\\ndilation\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – a parameter that controls the stride of elements in the window\\nreturn_indices\\n('),\n",
       " Document(metadata={}, page_content='return_indices\\n(\\nbool\\n) – if\\nTrue\\n, will return the max indices along with the outputs.\\nUseful for\\ntorch.nn.MaxUnpool2d\\nlater\\nceil_mode\\n(\\nbool\\n) – when True, will use\\nceil\\ninstead of\\nfloor\\nto compute the output shape\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\n⌊\\nH\\ni\\nn\\n+\\n2\\n∗\\npadding[0]\\n−\\ndilation[0]\\n×\\n(\\nkernel_size[0]\\n−\\n1\\n)\\n−\\n1\\nstride[0]\\n+\\n1\\n⌋\\nH_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 * \\\\text{padding[0]} - \\\\text{dilation[0]}\\n      \\\\times (\\\\text{kernel\\\\_size[0]} - 1) - 1}{\\\\text{stride[0]}} + 1\\\\right\\\\rfloor\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride[0]\\nH\\nin\\n\\u200b\\n+\\n2\\n∗\\npadding[0]\\n−\\ndilation[0]\\n×\\n(\\nkernel_size[0]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nW\\no\\nu\\nt\\n=\\n⌊\\nW\\ni\\nn\\n+\\n2\\n∗'),\n",
       " Document(metadata={}, page_content='−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nW\\no\\nu\\nt\\n=\\n⌊\\nW\\ni\\nn\\n+\\n2\\n∗\\npadding[1]\\n−\\ndilation[1]\\n×\\n(\\nkernel_size[1]\\n−\\n1\\n)\\n−\\n1\\nstride[1]\\n+\\n1\\n⌋\\nW_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 * \\\\text{padding[1]} - \\\\text{dilation[1]}\\n      \\\\times (\\\\text{kernel\\\\_size[1]} - 1) - 1}{\\\\text{stride[1]}} + 1\\\\right\\\\rfloor\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride[1]\\nW\\nin\\n\\u200b\\n+\\n2\\n∗\\npadding[1]\\n−\\ndilation[1]\\n×\\n(\\nkernel_size[1]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nExamples:\\n>>>\\n# pool of square window of size=3, stride=2\\n>>>\\nm\\n=\\nnn\\n.\\nMaxPool2d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>'),\n",
       " Document(metadata={}, page_content='>>>\\nm\\n=\\nnn\\n.\\nMaxPool2d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# pool of non-square window\\n>>>\\nm\\n=\\nnn\\n.\\nMaxPool2d\\n((\\n3\\n,\\n2\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='MaxPool3d\\n¶\\nclass\\ntorch.nn.\\nMaxPool3d\\n(\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\npadding\\n=\\n0\\n,\\ndilation\\n=\\n1\\n,\\nreturn_indices\\n=\\nFalse\\n,\\nceil_mode\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies a 3D max pooling over an input signal composed of several input planes.\\nIn the simplest case, the output value of the layer with input size\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n,\\noutput\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nand\\nkernel_size\\n(\\nk\\nD\\n,\\nk\\nH\\n,\\nk\\nW\\n)\\n(kD, kH, kW)\\n(\\nk\\nD\\n,\\nk\\nH\\n,\\nkW\\n)\\ncan be precisely described as:\\nout\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nd\\n,\\nh\\n,\\nw\\n)\\n=\\nmax\\n\\u2061\\nk\\n=\\n0\\n,\\n…\\n,\\nk\\nD\\n−\\n1\\nmax\\n\\u2061\\nm\\n=\\n0\\n,\\n…\\n,\\nk\\nH\\n−\\n1\\nmax\\n\\u2061\\nn\\n=\\n0\\n,\\n…\\n,\\nk\\nW\\n−\\n1\\ninput\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nstride[0]\\n×\\nd\\n+\\nk\\n,\\nstride[1]\\n×\\nh\\n+\\nm\\n,\\nstride[2]\\n×\\nw\\n+\\nn\\n)\\n\\\\begin{aligned}\\n    \\\\text{out}(N_i, C_j, d, h, w) ={} & \\\\max_{k=0, \\\\ldots, kD-1} \\\\max_{m=0, \\\\ldots, kH-1} \\\\max_{n=0, \\\\ldots, kW-1} \\\\\\\\'),\n",
       " Document(metadata={}, page_content='& \\\\text{input}(N_i, C_j, \\\\text{stride[0]} \\\\times d + k,\\n                                                     \\\\text{stride[1]} \\\\times h + m, \\\\text{stride[2]} \\\\times w + n)\\n\\\\end{aligned}\\nout\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nd\\n,\\nh\\n,\\nw\\n)\\n=\\n\\u200b\\nk\\n=\\n0\\n,\\n…\\n,\\nk\\nD\\n−\\n1\\nmax\\n\\u200b\\nm\\n=\\n0\\n,\\n…\\n,\\nk\\nH\\n−\\n1\\nmax\\n\\u200b\\nn\\n=\\n0\\n,\\n…\\n,\\nkW\\n−\\n1\\nmax\\n\\u200b\\ninput\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nstride[0]\\n×\\nd\\n+\\nk\\n,\\nstride[1]\\n×\\nh\\n+\\nm\\n,\\nstride[2]\\n×\\nw\\n+\\nn\\n)\\n\\u200b\\nIf\\npadding'),\n",
       " Document(metadata={}, page_content='×\\nh\\n+\\nm\\n,\\nstride[2]\\n×\\nw\\n+\\nn\\n)\\n\\u200b\\nIf\\npadding\\nis non-zero, then the input is implicitly padded with negative infinity on both sides\\nfor\\npadding\\nnumber of points.\\ndilation\\ncontrols the spacing between the kernel points.\\nIt is harder to describe, but this\\nlink\\nhas a nice visualization of what\\ndilation\\ndoes.\\nNote\\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.'),\n",
       " Document(metadata={}, page_content='The parameters\\nkernel_size\\n,\\nstride\\n,\\npadding\\n,\\ndilation\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the depth, height and width dimension\\na\\ntuple\\nof three ints – in which case, the first\\nint\\nis used for the depth dimension,\\nthe second\\nint\\nfor the height dimension and the third\\nint\\nfor the width dimension\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – the size of the window to take a max over\\nstride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]'),\n",
       " Document(metadata={}, page_content='(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – the stride of the window. Default value is\\nkernel_size\\npadding\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – Implicit negative infinity padding to be added on all three sides\\ndilation\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – a parameter that controls the stride of elements in the window\\nreturn_indices\\n(\\nbool\\n) – if\\nTrue\\n, will return the max indices along with the outputs.\\nUseful for\\ntorch.nn.MaxUnpool3d\\nlater\\nceil_mode\\n(\\nbool'),\n",
       " Document(metadata={}, page_content='torch.nn.MaxUnpool3d\\nlater\\nceil_mode\\n(\\nbool\\n) – when True, will use\\nceil\\ninstead of\\nfloor\\nto compute the output shape\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)'),\n",
       " Document(metadata={}, page_content='o\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nD\\no\\nu\\nt\\n=\\n⌊\\nD\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\n0\\n]\\n+\\n1\\n⌋\\nD_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0] \\\\times\\n  (\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\nD\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n0\\n]\\nD\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n−'),\n",
       " Document(metadata={}, page_content=']\\n−\\ndilation\\n[\\n0\\n]\\n×\\n(\\nkernel_size\\n[\\n0\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nH\\no\\nu\\nt\\n=\\n⌊\\nH\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\n1\\n]\\n+\\n1\\n⌋\\nH_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1] \\\\times\\n  (\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n1\\n]\\nH\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\ndilation\\n[\\n1\\n]\\n×\\n(\\nkernel_size\\n[\\n1\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nW\\no\\nu\\nt\\n=\\n⌊\\nW\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n2\\n]\\n−\\ndilation\\n[\\n2\\n]\\n×\\n('),\n",
       " Document(metadata={}, page_content='⌊\\nW\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n2\\n]\\n−\\ndilation\\n[\\n2\\n]\\n×\\n(\\nkernel_size\\n[\\n2\\n]\\n−\\n1\\n)\\n−\\n1\\nstride\\n[\\n2\\n]\\n+\\n1\\n⌋\\nW_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] - \\\\text{dilation}[2] \\\\times\\n  (\\\\text{kernel\\\\_size}[2] - 1) - 1}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloor\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n2\\n]\\nW\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n2\\n]\\n−\\ndilation\\n[\\n2\\n]\\n×\\n(\\nkernel_size\\n[\\n2\\n]\\n−\\n1\\n)\\n−\\n1\\n\\u200b\\n+\\n1\\n⌋\\nExamples:\\n>>>\\n# pool of square window of size=3, stride=2\\n>>>\\nm\\n=\\nnn\\n.\\nMaxPool3d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>'),\n",
       " Document(metadata={}, page_content='>>>\\nm\\n=\\nnn\\n.\\nMaxPool3d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# pool of non-square window\\n>>>\\nm\\n=\\nnn\\n.\\nMaxPool3d\\n((\\n3\\n,\\n2\\n,\\n2\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n,\\n2\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n44\\n,\\n31\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='MaxUnpool1d\\n¶\\nclass\\ntorch.nn.\\nMaxUnpool1d\\n(\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\npadding\\n=\\n0\\n)\\n[source]\\n[source]\\n¶\\nComputes a partial inverse of\\nMaxPool1d\\n.\\nMaxPool1d\\nis not fully invertible, since the non-maximal values are lost.\\nMaxUnpool1d\\ntakes in as input the output of\\nMaxPool1d\\nincluding the indices of the maximal values and computes a partial inverse\\nin which all non-maximal values are set to zero.\\nNote\\nThis operation may behave nondeterministically when the input indices has repeat values.\\nSee'),\n",
       " Document(metadata={}, page_content='See\\nhttps://github.com/pytorch/pytorch/issues/80827\\nand\\nReproducibility\\nfor more information.\\nNote\\nMaxPool1d\\ncan map several input sizes to the same output\\nsizes. Hence, the inversion process can get ambiguous.\\nTo accommodate this, you can provide the needed output size\\nas an additional argument\\noutput_size\\nin the forward call.\\nSee the Inputs and Example below.\\nParameters\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the max pooling window.\\nstride\\n(\\nint\\nor\\ntuple\\n) – Stride of the max pooling window.'),\n",
       " Document(metadata={}, page_content='or\\ntuple\\n) – Stride of the max pooling window.\\nIt is set to\\nkernel_size\\nby default.\\npadding\\n(\\nint\\nor\\ntuple\\n) – Padding that was added to the input\\nInputs:\\ninput\\n: the input Tensor to invert\\nindices\\n: the indices given out by\\nMaxPool1d\\noutput_size\\n(optional): the targeted output size\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n)\\n(N, C, H_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n)\\n(C, H_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n)\\n(N, C, H_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n)\\n(C, H_{out})\\n(\\nC\\n,'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n)\\n(C, H_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\n(\\nH\\ni\\nn\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n−\\n2\\n×\\npadding\\n[\\n0\\n]\\n+\\nkernel_size\\n[\\n0\\n]\\nH_{out} = (H_{in} - 1) \\\\times \\\\text{stride}[0] - 2 \\\\times \\\\text{padding}[0] + \\\\text{kernel\\\\_size}[0]\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nH\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n−\\n2\\n×\\npadding\\n[\\n0\\n]\\n+\\nkernel_size\\n[\\n0\\n]\\nor as given by\\noutput_size\\nin the call operator\\nExample:\\n>>>\\npool\\n=\\nnn\\n.\\nMaxPool1d\\n(\\n2\\n,\\nstride\\n=\\n2\\n,\\nreturn_indices\\n=\\nTrue\\n)\\n>>>\\nunpool\\n=\\nnn\\n.\\nMaxUnpool1d\\n(\\n2\\n,'),\n",
       " Document(metadata={}, page_content='=\\nTrue\\n)\\n>>>\\nunpool\\n=\\nnn\\n.\\nMaxUnpool1d\\n(\\n2\\n,\\nstride\\n=\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\ntensor\\n([[[\\n1.\\n,\\n2\\n,\\n3\\n,\\n4\\n,\\n5\\n,\\n6\\n,\\n7\\n,\\n8\\n]]])\\n>>>\\noutput\\n,\\nindices\\n=\\npool\\n(\\ninput\\n)\\n>>>\\nunpool\\n(\\noutput\\n,\\nindices\\n)\\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\\n>>>\\n# Example showcasing the use of output_size\\n>>>\\ninput\\n=\\ntorch\\n.\\ntensor\\n([[[\\n1.\\n,\\n2\\n,\\n3\\n,\\n4\\n,\\n5\\n,\\n6\\n,\\n7\\n,\\n8\\n,\\n9\\n]]])\\n>>>\\noutput\\n,\\nindices\\n=\\npool\\n(\\ninput\\n)\\n>>>\\nunpool\\n(\\noutput\\n,\\nindices\\n,\\noutput_size\\n=\\ninput\\n.\\nsize\\n())'),\n",
       " Document(metadata={}, page_content='output\\n,\\nindices\\n,\\noutput_size\\n=\\ninput\\n.\\nsize\\n())\\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])\\n>>>\\nunpool\\n(\\noutput\\n,\\nindices\\n)\\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='MaxUnpool2d\\n¶\\nclass\\ntorch.nn.\\nMaxUnpool2d\\n(\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\npadding\\n=\\n0\\n)\\n[source]\\n[source]\\n¶\\nComputes a partial inverse of\\nMaxPool2d\\n.\\nMaxPool2d\\nis not fully invertible, since the non-maximal values are lost.\\nMaxUnpool2d\\ntakes in as input the output of\\nMaxPool2d\\nincluding the indices of the maximal values and computes a partial inverse\\nin which all non-maximal values are set to zero.\\nNote\\nThis operation may behave nondeterministically when the input indices has repeat values.\\nSee'),\n",
       " Document(metadata={}, page_content='See\\nhttps://github.com/pytorch/pytorch/issues/80827\\nand\\nReproducibility\\nfor more information.\\nNote\\nMaxPool2d\\ncan map several input sizes to the same output\\nsizes. Hence, the inversion process can get ambiguous.\\nTo accommodate this, you can provide the needed output size\\nas an additional argument\\noutput_size\\nin the forward call.\\nSee the Inputs and Example below.\\nParameters\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the max pooling window.\\nstride\\n(\\nint\\nor\\ntuple\\n) – Stride of the max pooling window.'),\n",
       " Document(metadata={}, page_content='or\\ntuple\\n) – Stride of the max pooling window.\\nIt is set to\\nkernel_size\\nby default.\\npadding\\n(\\nint\\nor\\ntuple\\n) – Padding that was added to the input\\nInputs:\\ninput\\n: the input Tensor to invert\\nindices\\n: the indices given out by\\nMaxPool2d\\noutput_size\\n(optional): the targeted output size\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)'),\n",
       " Document(metadata={}, page_content='W\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\n(\\nH\\ni\\nn\\n−\\n1\\n)\\n×\\nstride[0]\\n−\\n2\\n×\\npadding[0]\\n+\\nkernel_size[0]\\nH_{out} = (H_{in} - 1) \\\\times \\\\text{stride[0]} - 2 \\\\times \\\\text{padding[0]} + \\\\text{kernel\\\\_size[0]}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nH\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride[0]\\n−\\n2\\n×\\npadding[0]\\n+\\nkernel_size[0]\\nW\\no\\nu\\nt\\n=\\n(\\nW\\ni\\nn\\n−\\n1\\n)\\n×\\nstride[1]\\n−\\n2\\n×\\npadding[1]\\n+'),\n",
       " Document(metadata={}, page_content='t\\n=\\n(\\nW\\ni\\nn\\n−\\n1\\n)\\n×\\nstride[1]\\n−\\n2\\n×\\npadding[1]\\n+\\nkernel_size[1]\\nW_{out} = (W_{in} - 1) \\\\times \\\\text{stride[1]} - 2 \\\\times \\\\text{padding[1]} + \\\\text{kernel\\\\_size[1]}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nW\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride[1]\\n−\\n2\\n×\\npadding[1]\\n+\\nkernel_size[1]\\nor as given by\\noutput_size\\nin the call operator\\nExample:\\n>>>\\npool\\n=\\nnn\\n.\\nMaxPool2d\\n(\\n2\\n,\\nstride\\n=\\n2\\n,\\nreturn_indices\\n=\\nTrue\\n)\\n>>>\\nunpool\\n=\\nnn\\n.\\nMaxUnpool2d\\n(\\n2\\n,\\nstride\\n=\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\ntensor\\n([[[[\\n1.\\n,\\n2.\\n,\\n3.\\n,\\n4.\\n],\\n[ 5.,  6.,  7.,  8.],'),\n",
       " Document(metadata={}, page_content='([[[[\\n1.\\n,\\n2.\\n,\\n3.\\n,\\n4.\\n],\\n[ 5.,  6.,  7.,  8.],\\n[ 9., 10., 11., 12.],\\n[13., 14., 15., 16.]]]])\\n>>>\\noutput\\n,\\nindices\\n=\\npool\\n(\\ninput\\n)\\n>>>\\nunpool\\n(\\noutput\\n,\\nindices\\n)\\ntensor([[[[  0.,   0.,   0.,   0.],\\n[  0.,   6.,   0.,   8.],\\n[  0.,   0.,   0.,   0.],\\n[  0.,  14.,   0.,  16.]]]])\\n>>>\\n# Now using output_size to resolve an ambiguous size for the inverse\\n>>>\\ninput\\n=\\ntorch\\n.\\ntensor\\n([[[[\\n1.\\n,\\n2.\\n,\\n3.\\n,\\n4.\\n,\\n5.\\n],\\n[ 6.,  7.,  8.,  9., 10.],\\n[11., 12., 13., 14., 15.],\\n[16., 17., 18., 19., 20.]]]])'),\n",
       " Document(metadata={}, page_content='[16., 17., 18., 19., 20.]]]])\\n>>>\\noutput\\n,\\nindices\\n=\\npool\\n(\\ninput\\n)\\n>>>\\n# This call will not work without specifying output_size\\n>>>\\nunpool\\n(\\noutput\\n,\\nindices\\n,\\noutput_size\\n=\\ninput\\n.\\nsize\\n())\\ntensor([[[[ 0.,  0.,  0.,  0.,  0.],\\n[ 0.,  7.,  0.,  9.,  0.],\\n[ 0.,  0.,  0.,  0.,  0.],\\n[ 0., 17.,  0., 19.,  0.]]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='MaxUnpool3d\\n¶\\nclass\\ntorch.nn.\\nMaxUnpool3d\\n(\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\npadding\\n=\\n0\\n)\\n[source]\\n[source]\\n¶\\nComputes a partial inverse of\\nMaxPool3d\\n.\\nMaxPool3d\\nis not fully invertible, since the non-maximal values are lost.\\nMaxUnpool3d\\ntakes in as input the output of\\nMaxPool3d\\nincluding the indices of the maximal values and computes a partial inverse\\nin which all non-maximal values are set to zero.\\nNote\\nThis operation may behave nondeterministically when the input indices has repeat values.\\nSee'),\n",
       " Document(metadata={}, page_content='See\\nhttps://github.com/pytorch/pytorch/issues/80827\\nand\\nReproducibility\\nfor more information.\\nNote\\nMaxPool3d\\ncan map several input sizes to the same output\\nsizes. Hence, the inversion process can get ambiguous.\\nTo accommodate this, you can provide the needed output size\\nas an additional argument\\noutput_size\\nin the forward call.\\nSee the Inputs section below.\\nParameters\\nkernel_size\\n(\\nint\\nor\\ntuple\\n) – Size of the max pooling window.\\nstride\\n(\\nint\\nor\\ntuple\\n) – Stride of the max pooling window.'),\n",
       " Document(metadata={}, page_content='or\\ntuple\\n) – Stride of the max pooling window.\\nIt is set to\\nkernel_size\\nby default.\\npadding\\n(\\nint\\nor\\ntuple\\n) – Padding that was added to the input\\nInputs:\\ninput\\n: the input Tensor to invert\\nindices\\n: the indices given out by\\nMaxPool3d\\noutput_size\\n(optional): the targeted output size\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.'),\n",
       " Document(metadata={}, page_content='(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nD\\no\\nu\\nt\\n=\\n(\\nD\\ni\\nn\\n−\\n1\\n)\\n×\\nstride[0]\\n−\\n2\\n×\\npadding[0]\\n+\\nkernel_size[0]\\nD_{out} = (D_{in} - 1) \\\\times \\\\text{stride[0]} - 2 \\\\times \\\\text{padding[0]} + \\\\text{kernel\\\\_size[0]}\\nD\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nD\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride[0]\\n−\\n2\\n×'),\n",
       " Document(metadata={}, page_content='D\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nD\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride[0]\\n−\\n2\\n×\\npadding[0]\\n+\\nkernel_size[0]\\nH\\no\\nu\\nt\\n=\\n(\\nH\\ni\\nn\\n−\\n1\\n)\\n×\\nstride[1]\\n−\\n2\\n×\\npadding[1]\\n+\\nkernel_size[1]\\nH_{out} = (H_{in} - 1) \\\\times \\\\text{stride[1]} - 2 \\\\times \\\\text{padding[1]} + \\\\text{kernel\\\\_size[1]}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nH\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride[1]\\n−\\n2\\n×\\npadding[1]\\n+\\nkernel_size[1]\\nW\\no\\nu\\nt\\n=\\n(\\nW\\ni\\nn\\n−\\n1\\n)\\n×\\nstride[2]\\n−\\n2\\n×\\npadding[2]\\n+\\nkernel_size[2]\\nW_{out} = (W_{in} - 1) \\\\times \\\\text{stride[2]} - 2 \\\\times \\\\text{padding[2]} + \\\\text{kernel\\\\_size[2]}\\nW\\no\\nu\\nt\\n\\u200b\\n='),\n",
       " Document(metadata={}, page_content='W\\no\\nu\\nt\\n\\u200b\\n=\\n(\\nW\\nin\\n\\u200b\\n−\\n1\\n)\\n×\\nstride[2]\\n−\\n2\\n×\\npadding[2]\\n+\\nkernel_size[2]\\nor as given by\\noutput_size\\nin the call operator\\nExample:\\n>>>\\n# pool of square window of size=3, stride=2\\n>>>\\npool\\n=\\nnn\\n.\\nMaxPool3d\\n(\\n3\\n,\\nstride\\n=\\n2\\n,\\nreturn_indices\\n=\\nTrue\\n)\\n>>>\\nunpool\\n=\\nnn\\n.\\nMaxUnpool3d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\noutput\\n,\\nindices\\n=\\npool\\n(\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n51\\n,\\n33\\n,\\n15\\n))\\n>>>\\nunpooled_output\\n=\\nunpool\\n(\\noutput\\n,\\nindices\\n)\\n>>>\\nunpooled_output\\n.\\nsize\\n()\\ntorch.Size([20, 16, 51, 33, 15])\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='()\\ntorch.Size([20, 16, 51, 33, 15])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AvgPool1d\\n¶\\nclass\\ntorch.nn.\\nAvgPool1d\\n(\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\npadding\\n=\\n0\\n,\\nceil_mode\\n=\\nFalse\\n,\\ncount_include_pad\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nApplies a 1D average pooling over an input signal composed of several input planes.\\nIn the simplest case, the output value of the layer with input size\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n,\\noutput\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(N, C, L_{out})\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\nand\\nkernel_size\\nk\\nk\\nk\\ncan be precisely described as:\\nout\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nl\\n)\\n=\\n1\\nk\\n∑\\nm\\n=\\n0\\nk\\n−'),\n",
       " Document(metadata={}, page_content='out\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nl\\n)\\n=\\n1\\nk\\n∑\\nm\\n=\\n0\\nk\\n−\\n1\\ninput\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nstride\\n×\\nl\\n+\\nm\\n)\\n\\\\text{out}(N_i, C_j, l) = \\\\frac{1}{k} \\\\sum_{m=0}^{k-1}\\n                       \\\\text{input}(N_i, C_j, \\\\text{stride} \\\\times l + m)\\nout\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nl\\n)\\n=\\nk\\n1\\n\\u200b\\nm\\n=\\n0\\n∑\\nk\\n−\\n1\\n\\u200b\\ninput\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nstride\\n×\\nl\\n+\\nm\\n)\\nIf\\npadding\\nis non-zero, then the input is implicitly zero-padded on both sides\\nfor\\npadding\\nnumber of points.\\nNote'),\n",
       " Document(metadata={}, page_content='for\\npadding\\nnumber of points.\\nNote\\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.\\nThe parameters\\nkernel_size\\n,\\nstride\\n,\\npadding\\ncan each be\\nan\\nint\\nor a one-element tuple.\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – the size of the window\\nstride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – the stride of the window. Default value is\\nkernel_size'),\n",
       " Document(metadata={}, page_content='kernel_size\\npadding\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – implicit zero padding to be added on both sides\\nceil_mode\\n(\\nbool\\n) – when True, will use\\nceil\\ninstead of\\nfloor\\nto compute the output shape\\ncount_include_pad\\n(\\nbool\\n) – when True, will include the zero-padding in the averaging calculation\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nL\\ni\\nn\\n)\\n(N, C, L_{in})\\n(\\nN\\n,\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\ni\\nn\\n)\\n(C, L_{in})\\n(\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(N, C, L_{out})\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n)'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(C, L_{out})\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nL\\no\\nu\\nt\\n=\\n⌊\\nL\\ni\\nn\\n+\\n2\\n×\\npadding\\n−\\nkernel_size\\nstride\\n+\\n1\\n⌋\\nL_{out} = \\\\left\\\\lfloor \\\\frac{L_{in} +\\n2 \\\\times \\\\text{padding} - \\\\text{kernel\\\\_size}}{\\\\text{stride}} + 1\\\\right\\\\rfloor\\nL\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\nL\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n−\\nkernel_size\\n\\u200b\\n+\\n1\\n⌋\\nPer the note above, if\\nceil_mode\\nis True and\\n(\\nL\\no\\nu\\nt\\n−\\n1\\n)\\n×\\nstride\\n≥\\nL\\ni\\nn\\n+\\npadding\\n(L_{out} - 1) \\\\times \\\\text{stride} \\\\geq L_{in}\\n+ \\\\text{padding}\\n(\\nL\\no\\nu\\nt\\n\\u200b\\n−\\n1\\n)\\n×\\nstride'),\n",
       " Document(metadata={}, page_content='+ \\\\text{padding}\\n(\\nL\\no\\nu\\nt\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n≥\\nL\\nin\\n\\u200b\\n+\\npadding\\n, we skip the last window as it would start in the right padded region, resulting in\\nL\\no\\nu\\nt\\nL_{out}\\nL\\no\\nu\\nt\\n\\u200b\\nbeing reduced by one.\\nExamples:\\n>>>\\n# pool with window of size=3, stride=2\\n>>>\\nm\\n=\\nnn\\n.\\nAvgPool1d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\nm\\n(\\ntorch\\n.\\ntensor\\n([[[\\n1.\\n,\\n2\\n,\\n3\\n,\\n4\\n,\\n5\\n,\\n6\\n,\\n7\\n]]]))\\ntensor([[[2., 4., 6.]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AvgPool2d\\n¶\\nclass\\ntorch.nn.\\nAvgPool2d\\n(\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\npadding\\n=\\n0\\n,\\nceil_mode\\n=\\nFalse\\n,\\ncount_include_pad\\n=\\nTrue\\n,\\ndivisor_override\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 2D average pooling over an input signal composed of several input planes.\\nIn the simplest case, the output value of the layer with input size\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n,\\noutput\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nand\\nkernel_size\\n(\\nk\\nH\\n,'),\n",
       " Document(metadata={}, page_content=',\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nand\\nkernel_size\\n(\\nk\\nH\\n,\\nk\\nW\\n)\\n(kH, kW)\\n(\\nk\\nH\\n,\\nkW\\n)\\ncan be precisely described as:\\no\\nu\\nt\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nh\\n,\\nw\\n)\\n=\\n1\\nk\\nH\\n∗\\nk\\nW\\n∑\\nm\\n=\\n0\\nk\\nH\\n−\\n1\\n∑\\nn\\n=\\n0\\nk\\nW\\n−\\n1\\ni\\nn\\np\\nu\\nt\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\ns\\nt\\nr\\ni\\nd\\ne\\n[\\n0\\n]\\n×\\nh\\n+\\nm\\n,\\ns\\nt\\nr\\ni\\nd\\ne\\n[\\n1\\n]\\n×\\nw\\n+\\nn\\n)\\nout(N_i, C_j, h, w)  = \\\\frac{1}{kH * kW} \\\\sum_{m=0}^{kH-1} \\\\sum_{n=0}^{kW-1}\\n                       input(N_i, C_j, stride[0] \\\\times h + m, stride[1] \\\\times w + n)\\no\\nu\\nt\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nh\\n,\\nw\\n)\\n=\\nk\\nH\\n∗\\nkW\\n1\\n\\u200b\\nm\\n=\\n0\\n∑\\nk\\nH\\n−\\n1\\n\\u200b\\nn\\n=\\n0\\n∑'),\n",
       " Document(metadata={}, page_content='h\\n,\\nw\\n)\\n=\\nk\\nH\\n∗\\nkW\\n1\\n\\u200b\\nm\\n=\\n0\\n∑\\nk\\nH\\n−\\n1\\n\\u200b\\nn\\n=\\n0\\n∑\\nkW\\n−\\n1\\n\\u200b\\nin\\np\\nu\\nt\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\ns\\nt\\nr\\ni\\nd\\ne\\n[\\n0\\n]\\n×\\nh\\n+\\nm\\n,\\ns\\nt\\nr\\ni\\nd\\ne\\n[\\n1\\n]\\n×\\nw\\n+\\nn\\n)\\nIf\\npadding\\nis non-zero, then the input is implicitly zero-padded on both sides\\nfor\\npadding\\nnumber of points.\\nNote\\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.\\nThe parameters\\nkernel_size\\n,\\nstride\\n,\\npadding'),\n",
       " Document(metadata={}, page_content='The parameters\\nkernel_size\\n,\\nstride\\n,\\npadding\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the height and width dimension\\na\\ntuple\\nof two ints – in which case, the first\\nint\\nis used for the height dimension,\\nand the second\\nint\\nfor the width dimension\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – the size of the window\\nstride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – the stride of the window. Default value is\\nkernel_size\\npadding\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n['),\n",
       " Document(metadata={}, page_content='kernel_size\\npadding\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – implicit zero padding to be added on both sides\\nceil_mode\\n(\\nbool\\n) – when True, will use\\nceil\\ninstead of\\nfloor\\nto compute the output shape\\ncount_include_pad\\n(\\nbool\\n) – when True, will include the zero-padding in the averaging calculation\\ndivisor_override\\n(\\nOptional\\n[\\nint\\n]\\n) – if specified, it will be used as divisor, otherwise size of the pooling region will be used.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN'),\n",
       " Document(metadata={}, page_content=',\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\n⌊\\nH\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\nkernel_size\\n[\\n0\\n]\\nstride\\n[\\n0\\n]\\n+\\n1\\n⌋\\nH_{out} = \\\\left\\\\lfloor\\\\frac{H_{in}  + 2 \\\\times \\\\text{padding}[0] -'),\n",
       " Document(metadata={}, page_content='\\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n0\\n]\\nH\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\nkernel_size\\n[\\n0\\n]\\n\\u200b\\n+\\n1\\n⌋\\nW\\no\\nu\\nt\\n=\\n⌊\\nW\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\nkernel_size\\n[\\n1\\n]\\nstride\\n[\\n1\\n]\\n+\\n1\\n⌋\\nW_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}  + 2 \\\\times \\\\text{padding}[1] -\\n  \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n1\\n]\\nW\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\nkernel_size\\n[\\n1\\n]\\n\\u200b\\n+\\n1\\n⌋\\nPer the note above, if\\nceil_mode\\nis True and\\n(\\nH\\no\\nu\\nt\\n−\\n1\\n)\\n×\\nstride'),\n",
       " Document(metadata={}, page_content='ceil_mode\\nis True and\\n(\\nH\\no\\nu\\nt\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n≥\\nH\\ni\\nn\\n+\\npadding\\n[\\n0\\n]\\n(H_{out} - 1)\\\\times \\\\text{stride}[0]\\\\geq H_{in}\\n+ \\\\text{padding}[0]\\n(\\nH\\no\\nu\\nt\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n≥\\nH\\nin\\n\\u200b\\n+\\npadding\\n[\\n0\\n]\\n, we skip the last window as it would start in the bottom padded region,\\nresulting in\\nH\\no\\nu\\nt\\nH_{out}\\nH\\no\\nu\\nt\\n\\u200b\\nbeing reduced by one.\\nThe same applies for\\nW\\no\\nu\\nt\\nW_{out}\\nW\\no\\nu\\nt\\n\\u200b\\n.\\nExamples:\\n>>>\\n# pool of square window of size=3, stride=2\\n>>>\\nm\\n=\\nnn\\n.\\nAvgPool2d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>'),\n",
       " Document(metadata={}, page_content='>>>\\nm\\n=\\nnn\\n.\\nAvgPool2d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# pool of non-square window\\n>>>\\nm\\n=\\nnn\\n.\\nAvgPool2d\\n((\\n3\\n,\\n2\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AvgPool3d\\n¶\\nclass\\ntorch.nn.\\nAvgPool3d\\n(\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\npadding\\n=\\n0\\n,\\nceil_mode\\n=\\nFalse\\n,\\ncount_include_pad\\n=\\nTrue\\n,\\ndivisor_override\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 3D average pooling over an input signal composed of several input planes.\\nIn the simplest case, the output value of the layer with input size\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n,\\noutput\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nand\\nkernel_size\\n(\\nk\\nD\\n,\\nk\\nH\\n,\\nk\\nW\\n)\\n(kD, kH, kW)\\n(\\nk\\nD\\n,\\nk\\nH\\n,\\nkW\\n)\\ncan be precisely described as:\\nout\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nd\\n,\\nh\\n,\\nw\\n)\\n=\\n∑\\nk\\n=\\n0\\nk\\nD\\n−\\n1\\n∑\\nm\\n=\\n0\\nk\\nH\\n−\\n1\\n∑\\nn\\n=\\n0\\nk\\nW\\n−\\n1\\ninput\\n(\\nN\\ni\\n,\\nC\\nj\\n,\\nstride\\n[\\n0\\n]\\n×\\nd\\n+\\nk\\n,\\nstride\\n[\\n1\\n]\\n×\\nh\\n+\\nm\\n,\\nstride\\n[\\n2\\n]\\n×\\nw\\n+\\nn\\n)\\nk\\nD\\n×\\nk\\nH\\n×\\nk\\nW\\n\\\\begin{aligned}\\n    \\\\text{out}(N_i, C_j, d, h, w) ={} & \\\\sum_{k=0}^{kD-1} \\\\sum_{m=0}^{kH-1} \\\\sum_{n=0}^{kW-1} \\\\\\\\'),\n",
       " Document(metadata={}, page_content='& \\\\frac{\\\\text{input}(N_i, C_j, \\\\text{stride}[0] \\\\times d + k,\\n                                              \\\\text{stride}[1] \\\\times h + m, \\\\text{stride}[2] \\\\times w + n)}\\n                                             {kD \\\\times kH \\\\times kW}\\n\\\\end{aligned}\\nout\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nd\\n,\\nh\\n,\\nw\\n)\\n=\\n\\u200b\\nk\\n=\\n0\\n∑\\nk\\nD\\n−\\n1\\n\\u200b\\nm\\n=\\n0\\n∑\\nk\\nH\\n−\\n1\\n\\u200b\\nn\\n=\\n0\\n∑\\nkW\\n−\\n1\\n\\u200b\\nk\\nD\\n×\\nk\\nH\\n×\\nkW\\ninput\\n(\\nN\\ni\\n\\u200b\\n,\\nC\\nj\\n\\u200b\\n,\\nstride\\n[\\n0\\n]\\n×\\nd\\n+\\nk\\n,\\nstride\\n[\\n1\\n]\\n×\\nh\\n+\\nm\\n,\\nstride\\n[\\n2\\n]\\n×\\nw\\n+\\nn\\n)\\n\\u200b\\n\\u200b\\nIf'),\n",
       " Document(metadata={}, page_content='[\\n1\\n]\\n×\\nh\\n+\\nm\\n,\\nstride\\n[\\n2\\n]\\n×\\nw\\n+\\nn\\n)\\n\\u200b\\n\\u200b\\nIf\\npadding\\nis non-zero, then the input is implicitly zero-padded on all three sides\\nfor\\npadding\\nnumber of points.\\nNote\\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\\nor the input. Sliding windows that would start in the right padded region are ignored.\\nThe parameters\\nkernel_size\\n,\\nstride\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the depth, height and width dimension\\na'),\n",
       " Document(metadata={}, page_content='a\\ntuple\\nof three ints – in which case, the first\\nint\\nis used for the depth dimension,\\nthe second\\nint\\nfor the height dimension and the third\\nint\\nfor the width dimension\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – the size of the window\\nstride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – the stride of the window. Default value is\\nkernel_size\\npadding\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – implicit zero padding to be added on all three sides\\nceil_mode\\n(\\nbool'),\n",
       " Document(metadata={}, page_content='ceil_mode\\n(\\nbool\\n) – when True, will use\\nceil\\ninstead of\\nfloor\\nto compute the output shape\\ncount_include_pad\\n(\\nbool\\n) – when True, will include the zero-padding in the averaging calculation\\ndivisor_override\\n(\\nOptional\\n[\\nint\\n]\\n) – if specified, it will be used as divisor, otherwise\\nkernel_size\\nwill be used\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,'),\n",
       " Document(metadata={}, page_content='i\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nD\\no\\nu\\nt\\n=\\n⌊\\nD\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\nkernel_size\\n[\\n0\\n]\\nstride\\n[\\n0\\n]\\n+\\n1\\n⌋\\nD_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} + 2 \\\\times \\\\text{padding}[0] -'),\n",
       " Document(metadata={}, page_content='\\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\nD\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n0\\n]\\nD\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n0\\n]\\n−\\nkernel_size\\n[\\n0\\n]\\n\\u200b\\n+\\n1\\n⌋\\nH\\no\\nu\\nt\\n=\\n⌊\\nH\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\nkernel_size\\n[\\n1\\n]\\nstride\\n[\\n1\\n]\\n+\\n1\\n⌋\\nH_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} + 2 \\\\times \\\\text{padding}[1] -\\n      \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n1\\n]\\nH\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n1\\n]\\n−\\nkernel_size\\n[\\n1\\n]\\n\\u200b\\n+\\n1\\n⌋\\nW\\no\\nu\\nt\\n=\\n⌊\\nW\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n2\\n]\\n−\\nkernel_size\\n[\\n2\\n]'),\n",
       " Document(metadata={}, page_content='=\\n⌊\\nW\\ni\\nn\\n+\\n2\\n×\\npadding\\n[\\n2\\n]\\n−\\nkernel_size\\n[\\n2\\n]\\nstride\\n[\\n2\\n]\\n+\\n1\\n⌋\\nW_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} + 2 \\\\times \\\\text{padding}[2] -\\n      \\\\text{kernel\\\\_size}[2]}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloor\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n2\\n]\\nW\\nin\\n\\u200b\\n+\\n2\\n×\\npadding\\n[\\n2\\n]\\n−\\nkernel_size\\n[\\n2\\n]\\n\\u200b\\n+\\n1\\n⌋\\nPer the note above, if\\nceil_mode\\nis True and\\n(\\nD\\no\\nu\\nt\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n≥\\nD\\ni\\nn\\n+\\npadding\\n[\\n0\\n]\\n(D_{out} - 1)\\\\times \\\\text{stride}[0]\\\\geq D_{in}\\n+ \\\\text{padding}[0]\\n(\\nD\\no\\nu\\nt\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n≥\\nD\\nin\\n\\u200b\\n+'),\n",
       " Document(metadata={}, page_content='(\\nD\\no\\nu\\nt\\n\\u200b\\n−\\n1\\n)\\n×\\nstride\\n[\\n0\\n]\\n≥\\nD\\nin\\n\\u200b\\n+\\npadding\\n[\\n0\\n]\\n, we skip the last window as it would start in the padded region,\\nresulting in\\nD\\no\\nu\\nt\\nD_{out}\\nD\\no\\nu\\nt\\n\\u200b\\nbeing reduced by one.\\nThe same applies for\\nW\\no\\nu\\nt\\nW_{out}\\nW\\no\\nu\\nt\\n\\u200b\\nand\\nH\\no\\nu\\nt\\nH_{out}\\nH\\no\\nu\\nt\\n\\u200b\\n.\\nExamples:\\n>>>\\n# pool of square window of size=3, stride=2\\n>>>\\nm\\n=\\nnn\\n.\\nAvgPool3d\\n(\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# pool of non-square window\\n>>>\\nm\\n=\\nnn\\n.\\nAvgPool3d\\n((\\n3\\n,\\n2\\n,\\n2\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n,\\n2\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50'),\n",
       " Document(metadata={}, page_content='1\\n,\\n2\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n44\\n,\\n31\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='FractionalMaxPool2d\\n¶\\nclass\\ntorch.nn.\\nFractionalMaxPool2d\\n(\\nkernel_size\\n,\\noutput_size\\n=\\nNone\\n,\\noutput_ratio\\n=\\nNone\\n,\\nreturn_indices\\n=\\nFalse\\n,\\n_random_samples\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 2D fractional max pooling over an input signal composed of several input planes.\\nFractional MaxPooling is described in detail in the paper\\nFractional MaxPooling\\nby Ben Graham\\nThe max-pooling operation is applied in\\nk\\nH\\n×\\nk\\nW\\nkH \\\\times kW\\nk\\nH\\n×\\nkW\\nregions by a stochastic'),\n",
       " Document(metadata={}, page_content='k\\nW\\nkH \\\\times kW\\nk\\nH\\n×\\nkW\\nregions by a stochastic\\nstep size determined by the target output size.\\nThe number of output features is equal to the number of input planes.\\nNote\\nExactly one of\\noutput_size\\nor\\noutput_ratio\\nmust be defined.\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – the size of the window to take a max over.\\nCan be a single number k (for a square kernel of k x k) or a tuple\\n(kh, kw)\\noutput_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]'),\n",
       " Document(metadata={}, page_content='output_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – the target output size of the image of the form\\noH x oW\\n.\\nCan be a tuple\\n(oH, oW)\\nor a single number oH for a square image\\noH x oH\\n.\\nNote that we must have\\nk\\nH\\n+\\no\\nH\\n−\\n1\\n<\\n=\\nH\\ni\\nn\\nkH + oH - 1 <= H_{in}\\nk\\nH\\n+\\noH\\n−\\n1\\n<=\\nH\\nin\\n\\u200b\\nand\\nk\\nW\\n+\\no\\nW\\n−\\n1\\n<\\n=\\nW\\ni\\nn\\nkW + oW - 1 <= W_{in}\\nkW\\n+\\no\\nW\\n−\\n1\\n<=\\nW\\nin\\n\\u200b\\noutput_ratio\\n(\\nUnion\\n[\\nfloat\\n,\\nTuple\\n[\\nfloat\\n,\\nfloat\\n]\\n]'),\n",
       " Document(metadata={}, page_content='(\\nUnion\\n[\\nfloat\\n,\\nTuple\\n[\\nfloat\\n,\\nfloat\\n]\\n]\\n) – If one wants to have an output size as a ratio of the input size, this option can be given.\\nThis has to be a number or tuple in the range (0, 1).\\nNote that we must have\\nk\\nH\\n+\\n(\\no\\nu\\nt\\np\\nu\\nt\\n_\\nr\\na\\nt\\ni\\no\\n_\\nH\\n∗\\nH\\ni\\nn\\n)\\n−\\n1\\n<\\n=\\nH\\ni\\nn\\nkH + (output\\\\_ratio\\\\_H * H_{in}) - 1 <= H_{in}\\nk\\nH\\n+\\n(\\no\\nu\\ntp\\nu\\nt\\n_\\nr\\na\\nt\\ni\\no\\n_\\nH\\n∗\\nH\\nin\\n\\u200b\\n)\\n−\\n1\\n<=\\nH\\nin\\n\\u200b\\nand\\nk\\nW\\n+\\n(\\no\\nu\\nt\\np\\nu\\nt\\n_\\nr\\na\\nt\\ni\\no\\n_\\nW\\n∗\\nW\\ni\\nn\\n)\\n−\\n1\\n<\\n=\\nW\\ni\\nn\\nkW + (output\\\\_ratio\\\\_W * W_{in}) - 1 <= W_{in}\\nkW\\n+'),\n",
       " Document(metadata={}, page_content='kW\\n+\\n(\\no\\nu\\ntp\\nu\\nt\\n_\\nr\\na\\nt\\ni\\no\\n_\\nW\\n∗\\nW\\nin\\n\\u200b\\n)\\n−\\n1\\n<=\\nW\\nin\\n\\u200b\\nreturn_indices\\n(\\nbool\\n) – if\\nTrue\\n, will return the indices along with the outputs.\\nUseful to pass to\\nnn.MaxUnpool2d()\\n. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)'),\n",
       " Document(metadata={}, page_content='u\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\n(\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n=\\noutput_size\\n(H_{out}, W_{out})=\\\\text{output\\\\_size}\\n(\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n=\\noutput_size\\nor\\n(\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n=\\noutput_ratio\\n×\\n(\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(H_{out}, W_{out})=\\\\text{output\\\\_ratio} \\\\times (H_{in}, W_{in})\\n(\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n=\\noutput_ratio\\n×\\n(\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nExamples\\n>>>\\n# pool of square window of size=3, and target output size 13x12\\n>>>\\nm\\n='),\n",
       " Document(metadata={}, page_content='>>>\\nm\\n=\\nnn\\n.\\nFractionalMaxPool2d\\n(\\n3\\n,\\noutput_size\\n=\\n(\\n13\\n,\\n12\\n))\\n>>>\\n# pool of square window and target output size being half of input image size\\n>>>\\nm\\n=\\nnn\\n.\\nFractionalMaxPool2d\\n(\\n3\\n,\\noutput_ratio\\n=\\n(\\n0.5\\n,\\n0.5\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='FractionalMaxPool3d\\n¶\\nclass\\ntorch.nn.\\nFractionalMaxPool3d\\n(\\nkernel_size\\n,\\noutput_size\\n=\\nNone\\n,\\noutput_ratio\\n=\\nNone\\n,\\nreturn_indices\\n=\\nFalse\\n,\\n_random_samples\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a 3D fractional max pooling over an input signal composed of several input planes.\\nFractional MaxPooling is described in detail in the paper\\nFractional MaxPooling\\nby Ben Graham\\nThe max-pooling operation is applied in\\nk\\nT\\n×\\nk\\nH\\n×\\nk\\nW\\nkT \\\\times kH \\\\times kW\\nk\\nT\\n×\\nk\\nH\\n×\\nkW\\nregions by a stochastic'),\n",
       " Document(metadata={}, page_content='k\\nT\\n×\\nk\\nH\\n×\\nkW\\nregions by a stochastic\\nstep size determined by the target output size.\\nThe number of output features is equal to the number of input planes.\\nNote\\nExactly one of\\noutput_size\\nor\\noutput_ratio\\nmust be defined.\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – the size of the window to take a max over.\\nCan be a single number k (for a square kernel of k x k x k) or a tuple\\n(kt x kh x kw)\\noutput_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]'),\n",
       " Document(metadata={}, page_content='(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – the target output size of the image of the form\\noT x oH x oW\\n.\\nCan be a tuple\\n(oT, oH, oW)\\nor a single number oH for a square image\\noH x oH x oH\\noutput_ratio\\n(\\nUnion\\n[\\nfloat\\n,\\nTuple\\n[\\nfloat\\n,\\nfloat\\n,\\nfloat\\n]\\n]\\n) – If one wants to have an output size as a ratio of the input size, this option can be given.\\nThis has to be a number or tuple in the range (0, 1)\\nreturn_indices\\n(\\nbool\\n) – if\\nTrue\\n, will return the indices along with the outputs.'),\n",
       " Document(metadata={}, page_content=', will return the indices along with the outputs.\\nUseful to pass to\\nnn.MaxUnpool3d()\\n. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nT\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, T_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nT\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nT\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, T_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nT\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nT\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, T_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nT\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nT\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)'),\n",
       " Document(metadata={}, page_content='o\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nT\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, T_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nT\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\n(\\nT\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n=\\noutput_size\\n(T_{out}, H_{out}, W_{out})=\\\\text{output\\\\_size}\\n(\\nT\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n=\\noutput_size\\nor\\n(\\nT\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n=\\noutput_ratio\\n×\\n(\\nT\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(T_{out}, H_{out}, W_{out})=\\\\text{output\\\\_ratio} \\\\times (T_{in}, H_{in}, W_{in})\\n(\\nT\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n=\\noutput_ratio\\n×\\n(\\nT\\nin\\n\\u200b'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n=\\noutput_ratio\\n×\\n(\\nT\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nExamples\\n>>>\\n# pool of cubic window of size=3, and target output size 13x12x11\\n>>>\\nm\\n=\\nnn\\n.\\nFractionalMaxPool3d\\n(\\n3\\n,\\noutput_size\\n=\\n(\\n13\\n,\\n12\\n,\\n11\\n))\\n>>>\\n# pool of cubic window and target output size being half of input size\\n>>>\\nm\\n=\\nnn\\n.\\nFractionalMaxPool3d\\n(\\n3\\n,\\noutput_ratio\\n=\\n(\\n0.5\\n,\\n0.5\\n,\\n0.5\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n32\\n,\\n16\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='32\\n,\\n16\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LPPool1d\\n¶\\nclass\\ntorch.nn.\\nLPPool1d\\n(\\nnorm_type\\n,\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\nceil_mode\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies a 1D power-average pooling over an input signal composed of several input planes.\\nOn each window, the function computed is:\\nf\\n(\\nX\\n)\\n=\\n∑\\nx\\n∈\\nX\\nx\\np\\np\\nf(X) = \\\\sqrt[p]{\\\\sum_{x \\\\in X} x^{p}}\\nf\\n(\\nX\\n)\\n=\\np\\n\\u200b\\nx\\n∈\\nX\\n∑\\n\\u200b\\nx\\np\\n\\u200b\\nAt p =\\n∞\\n\\\\infty\\n∞\\n, one gets Max Pooling\\nAt p = 1, one gets Sum Pooling (which is proportional to Average Pooling)\\nNote\\nIf the sum to the power of\\np'),\n",
       " Document(metadata={}, page_content='Note\\nIf the sum to the power of\\np\\nis zero, the gradient of this function is\\nnot defined. This implementation will set the gradient to zero in this case.\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – a single int, the size of the window\\nstride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – a single int, the stride of the window. Default value is\\nkernel_size\\nceil_mode\\n(\\nbool\\n) – when True, will use\\nceil\\ninstead of\\nfloor\\nto compute the output shape\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nL\\ni\\nn\\n)\\n(N, C, L_{in})'),\n",
       " Document(metadata={}, page_content='Shape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nL\\ni\\nn\\n)\\n(N, C, L_{in})\\n(\\nN\\n,\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\ni\\nn\\n)\\n(C, L_{in})\\n(\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(N, C, L_{out})\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(C, L_{out})\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nL\\no\\nu\\nt\\n=\\n⌊\\nL\\ni\\nn\\n−\\nkernel_size\\nstride\\n+\\n1\\n⌋\\nL_{out} = \\\\left\\\\lfloor\\\\frac{L_{in} - \\\\text{kernel\\\\_size}}{\\\\text{stride}} + 1\\\\right\\\\rfloor\\nL\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\nL\\nin\\n\\u200b\\n−\\nkernel_size\\n\\u200b\\n+\\n1\\n⌋\\nExamples::\\n>>>\\n# power-2 pool of window of length 3, with stride 2.\\n>>>\\nm\\n=\\nnn'),\n",
       " Document(metadata={}, page_content='>>>\\nm\\n=\\nnn\\n.\\nLPPool1d\\n(\\n2\\n,\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LPPool2d\\n¶\\nclass\\ntorch.nn.\\nLPPool2d\\n(\\nnorm_type\\n,\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\nceil_mode\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies a 2D power-average pooling over an input signal composed of several input planes.\\nOn each window, the function computed is:\\nf\\n(\\nX\\n)\\n=\\n∑\\nx\\n∈\\nX\\nx\\np\\np\\nf(X) = \\\\sqrt[p]{\\\\sum_{x \\\\in X} x^{p}}\\nf\\n(\\nX\\n)\\n=\\np\\n\\u200b\\nx\\n∈\\nX\\n∑\\n\\u200b\\nx\\np\\n\\u200b\\nAt p =\\n∞\\n\\\\infty\\n∞\\n, one gets Max Pooling\\nAt p = 1, one gets Sum Pooling (which is proportional to average pooling)\\nThe parameters\\nkernel_size\\n,\\nstride'),\n",
       " Document(metadata={}, page_content='The parameters\\nkernel_size\\n,\\nstride\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the height and width dimension\\na\\ntuple\\nof two ints – in which case, the first\\nint\\nis used for the height dimension,\\nand the second\\nint\\nfor the width dimension\\nNote\\nIf the sum to the power of\\np\\nis zero, the gradient of this function is\\nnot defined. This implementation will set the gradient to zero in this case.\\nParameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]'),\n",
       " Document(metadata={}, page_content='kernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – the size of the window\\nstride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n]\\n]\\n) – the stride of the window. Default value is\\nkernel_size\\nceil_mode\\n(\\nbool\\n) – when True, will use\\nceil\\ninstead of\\nfloor\\nto compute the output shape\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n('),\n",
       " Document(metadata={}, page_content=',\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\n⌊\\nH\\ni\\nn\\n−\\nkernel_size\\n[\\n0\\n]\\nstride\\n[\\n0\\n]\\n+\\n1\\n⌋\\nH_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} - \\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n0\\n]\\nH\\nin\\n\\u200b\\n−\\nkernel_size\\n[\\n0\\n]\\n\\u200b\\n+\\n1\\n⌋\\nW\\no\\nu\\nt\\n=\\n⌊\\nW\\ni\\nn\\n−\\nkernel_size\\n[\\n1\\n]\\nstride\\n[\\n1\\n]\\n+\\n1\\n⌋'),\n",
       " Document(metadata={}, page_content='=\\n⌊\\nW\\ni\\nn\\n−\\nkernel_size\\n[\\n1\\n]\\nstride\\n[\\n1\\n]\\n+\\n1\\n⌋\\nW_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} - \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n1\\n]\\nW\\nin\\n\\u200b\\n−\\nkernel_size\\n[\\n1\\n]\\n\\u200b\\n+\\n1\\n⌋\\nExamples:\\n>>>\\n# power-2 pool of square window of size=3, stride=2\\n>>>\\nm\\n=\\nnn\\n.\\nLPPool2d\\n(\\n2\\n,\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# pool of non-square window of power 1.2\\n>>>\\nm\\n=\\nnn\\n.\\nLPPool2d\\n(\\n1.2\\n,\\n(\\n3\\n,\\n2\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)'),\n",
       " Document(metadata={}, page_content='(\\n20\\n,\\n16\\n,\\n50\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LPPool3d\\n¶\\nclass\\ntorch.nn.\\nLPPool3d\\n(\\nnorm_type\\n,\\nkernel_size\\n,\\nstride\\n=\\nNone\\n,\\nceil_mode\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies a 3D power-average pooling over an input signal composed of several input planes.\\nOn each window, the function computed is:\\nf\\n(\\nX\\n)\\n=\\n∑\\nx\\n∈\\nX\\nx\\np\\np\\nf(X) = \\\\sqrt[p]{\\\\sum_{x \\\\in X} x^{p}}\\nf\\n(\\nX\\n)\\n=\\np\\n\\u200b\\nx\\n∈\\nX\\n∑\\n\\u200b\\nx\\np\\n\\u200b\\nAt p =\\n∞\\n\\\\infty\\n∞\\n, one gets Max Pooling\\nAt p = 1, one gets Sum Pooling (which is proportional to average pooling)\\nThe parameters\\nkernel_size\\n,\\nstride'),\n",
       " Document(metadata={}, page_content='The parameters\\nkernel_size\\n,\\nstride\\ncan either be:\\na single\\nint\\n– in which case the same value is used for the height, width and depth dimension\\na\\ntuple\\nof three ints – in which case, the first\\nint\\nis used for the depth dimension,\\nthe second\\nint\\nfor the height dimension and the third\\nint\\nfor the width dimension\\nNote\\nIf the sum to the power of\\np\\nis zero, the gradient of this function is\\nnot defined. This implementation will set the gradient to zero in this case.\\nParameters\\nkernel_size\\n(\\nUnion\\n['),\n",
       " Document(metadata={}, page_content='Parameters\\nkernel_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – the size of the window\\nstride\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n,\\nint\\n,\\nint\\n]\\n]\\n) – the stride of the window. Default value is\\nkernel_size\\nceil_mode\\n(\\nbool\\n) – when True, will use\\nceil\\ninstead of\\nfloor\\nto compute the output shape\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW'),\n",
       " Document(metadata={}, page_content='(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nD\\no\\nu\\nt\\n=\\n⌊\\nD\\ni\\nn\\n−\\nkernel_size\\n[\\n0\\n]\\nstride\\n[\\n0\\n]\\n+\\n1\\n⌋\\nD_{out} = \\\\left\\\\lfloor\\\\frac{D_{in} - \\\\text{kernel\\\\_size}[0]}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\nD\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n0\\n]\\nD\\nin\\n\\u200b\\n−\\nkernel_size\\n[\\n0\\n]\\n\\u200b\\n+\\n1\\n⌋\\nH'),\n",
       " Document(metadata={}, page_content='stride\\n[\\n0\\n]\\nD\\nin\\n\\u200b\\n−\\nkernel_size\\n[\\n0\\n]\\n\\u200b\\n+\\n1\\n⌋\\nH\\no\\nu\\nt\\n=\\n⌊\\nH\\ni\\nn\\n−\\nkernel_size\\n[\\n1\\n]\\nstride\\n[\\n1\\n]\\n+\\n1\\n⌋\\nH_{out} = \\\\left\\\\lfloor\\\\frac{H_{in} - \\\\text{kernel\\\\_size}[1]}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n1\\n]\\nH\\nin\\n\\u200b\\n−\\nkernel_size\\n[\\n1\\n]\\n\\u200b\\n+\\n1\\n⌋\\nW\\no\\nu\\nt\\n=\\n⌊\\nW\\ni\\nn\\n−\\nkernel_size\\n[\\n2\\n]\\nstride\\n[\\n2\\n]\\n+\\n1\\n⌋\\nW_{out} = \\\\left\\\\lfloor\\\\frac{W_{in} - \\\\text{kernel\\\\_size}[2]}{\\\\text{stride}[2]} + 1\\\\right\\\\rfloor\\nW\\no\\nu\\nt\\n\\u200b\\n=\\n⌊\\nstride\\n[\\n2\\n]\\nW\\nin\\n\\u200b\\n−\\nkernel_size\\n[\\n2\\n]\\n\\u200b\\n+\\n1\\n⌋\\nExamples:\\n>>>'),\n",
       " Document(metadata={}, page_content='W\\nin\\n\\u200b\\n−\\nkernel_size\\n[\\n2\\n]\\n\\u200b\\n+\\n1\\n⌋\\nExamples:\\n>>>\\n# power-2 pool of square window of size=3, stride=2\\n>>>\\nm\\n=\\nnn\\n.\\nLPPool3d\\n(\\n2\\n,\\n3\\n,\\nstride\\n=\\n2\\n)\\n>>>\\n# pool of non-square window of power 1.2\\n>>>\\nm\\n=\\nnn\\n.\\nLPPool3d\\n(\\n1.2\\n,\\n(\\n3\\n,\\n2\\n,\\n2\\n),\\nstride\\n=\\n(\\n2\\n,\\n1\\n,\\n2\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n50\\n,\\n44\\n,\\n31\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AdaptiveMaxPool1d\\n¶\\nclass\\ntorch.nn.\\nAdaptiveMaxPool1d\\n(\\noutput_size\\n,\\nreturn_indices\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies a 1D adaptive max pooling over an input signal composed of several input planes.\\nThe output size is\\nL\\no\\nu\\nt\\nL_{out}\\nL\\no\\nu\\nt\\n\\u200b\\n, for any input size.\\nThe number of output features is equal to the number of input planes.\\nParameters\\noutput_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – the target output size\\nL\\no\\nu\\nt\\nL_{out}\\nL\\no\\nu\\nt\\n\\u200b\\n.\\nreturn_indices\\n(\\nbool\\n) – if\\nTrue'),\n",
       " Document(metadata={}, page_content='L\\no\\nu\\nt\\n\\u200b\\n.\\nreturn_indices\\n(\\nbool\\n) – if\\nTrue\\n, will return the indices along with the outputs.\\nUseful to pass to nn.MaxUnpool1d. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nL\\ni\\nn\\n)\\n(N, C, L_{in})\\n(\\nN\\n,\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\ni\\nn\\n)\\n(C, L_{in})\\n(\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(N, C, L_{out})\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(C, L_{out})\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nL\\no\\nu\\nt\\n=\\noutput_size\\nL_{out}=\\\\text{output\\\\_size}\\nL\\no\\nu\\nt\\n\\u200b\\n=\\noutput_size\\n.\\nExamples\\n>>>\\n# target output size of 5\\n>>>'),\n",
       " Document(metadata={}, page_content='.\\nExamples\\n>>>\\n# target output size of 5\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveMaxPool1d\\n(\\n5\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n8\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AdaptiveMaxPool2d\\n¶\\nclass\\ntorch.nn.\\nAdaptiveMaxPool2d\\n(\\noutput_size\\n,\\nreturn_indices\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies a 2D adaptive max pooling over an input signal composed of several input planes.\\nThe output is of size\\nH\\no\\nu\\nt\\n×\\nW\\no\\nu\\nt\\nH_{out} \\\\times W_{out}\\nH\\no\\nu\\nt\\n\\u200b\\n×\\nW\\no\\nu\\nt\\n\\u200b\\n, for any input size.\\nThe number of output features is equal to the number of input planes.\\nParameters\\noutput_size\\n(\\nUnion\\n[\\nint\\n,\\nNone\\n,\\nTuple\\n[\\nOptional\\n[\\nint\\n]\\n,\\nOptional\\n[\\nint\\n]\\n]\\n]'),\n",
       " Document(metadata={}, page_content=',\\nTuple\\n[\\nOptional\\n[\\nint\\n]\\n,\\nOptional\\n[\\nint\\n]\\n]\\n]\\n) – the target output size of the image of the form\\nH\\no\\nu\\nt\\n×\\nW\\no\\nu\\nt\\nH_{out} \\\\times W_{out}\\nH\\no\\nu\\nt\\n\\u200b\\n×\\nW\\no\\nu\\nt\\n\\u200b\\n.\\nCan be a tuple\\n(\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(H_{out}, W_{out})\\n(\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor a single\\nH\\no\\nu\\nt\\nH_{out}\\nH\\no\\nu\\nt\\n\\u200b\\nfor a\\nsquare image\\nH\\no\\nu\\nt\\n×\\nH\\no\\nu\\nt\\nH_{out} \\\\times H_{out}\\nH\\no\\nu\\nt\\n\\u200b\\n×\\nH\\no\\nu\\nt\\n\\u200b\\n.\\nH\\no\\nu\\nt\\nH_{out}\\nH\\no\\nu\\nt\\n\\u200b\\nand\\nW\\no\\nu\\nt\\nW_{out}\\nW\\no\\nu\\nt\\n\\u200b\\ncan be either a\\nint\\n, or\\nNone'),\n",
       " Document(metadata={}, page_content='t\\nW_{out}\\nW\\no\\nu\\nt\\n\\u200b\\ncan be either a\\nint\\n, or\\nNone\\nwhich means the size will be the same as that\\nof the input.\\nreturn_indices\\n(\\nbool\\n) – if\\nTrue\\n, will return the indices along with the outputs.\\nUseful to pass to nn.MaxUnpool2d. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\n(\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n=\\noutput_size\\n(H_{out}, W_{out})=\\\\text{output\\\\_size}\\n(\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n=\\noutput_size\\n.\\nExamples\\n>>>\\n# target output size of 5x7\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveMaxPool2d\\n((\\n5\\n,\\n7\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n8\\n,\\n9\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# target output size of 7x7 (square)\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveMaxPool2d\\n(\\n7\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n('),\n",
       " Document(metadata={}, page_content='(\\n7\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n10\\n,\\n9\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# target output size of 10x7\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveMaxPool2d\\n((\\nNone\\n,\\n7\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n10\\n,\\n9\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AdaptiveMaxPool3d\\n¶\\nclass\\ntorch.nn.\\nAdaptiveMaxPool3d\\n(\\noutput_size\\n,\\nreturn_indices\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies a 3D adaptive max pooling over an input signal composed of several input planes.\\nThe output is of size\\nD\\no\\nu\\nt\\n×\\nH\\no\\nu\\nt\\n×\\nW\\no\\nu\\nt\\nD_{out} \\\\times H_{out} \\\\times W_{out}\\nD\\no\\nu\\nt\\n\\u200b\\n×\\nH\\no\\nu\\nt\\n\\u200b\\n×\\nW\\no\\nu\\nt\\n\\u200b\\n, for any input size.\\nThe number of output features is equal to the number of input planes.\\nParameters\\noutput_size\\n(\\nUnion\\n[\\nint\\n,\\nNone\\n,\\nTuple\\n[\\nOptional\\n[\\nint\\n]\\n,\\nOptional'),\n",
       " Document(metadata={}, page_content='int\\n,\\nNone\\n,\\nTuple\\n[\\nOptional\\n[\\nint\\n]\\n,\\nOptional\\n[\\nint\\n]\\n,\\nOptional\\n[\\nint\\n]\\n]\\n]\\n) – the target output size of the image of the form\\nD\\no\\nu\\nt\\n×\\nH\\no\\nu\\nt\\n×\\nW\\no\\nu\\nt\\nD_{out} \\\\times H_{out} \\\\times W_{out}\\nD\\no\\nu\\nt\\n\\u200b\\n×\\nH\\no\\nu\\nt\\n\\u200b\\n×\\nW\\no\\nu\\nt\\n\\u200b\\n.\\nCan be a tuple\\n(\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(D_{out}, H_{out}, W_{out})\\n(\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor a single\\nD\\no\\nu\\nt\\nD_{out}\\nD\\no\\nu\\nt\\n\\u200b\\nfor a cube\\nD\\no\\nu\\nt\\n×\\nD\\no\\nu\\nt\\n×\\nD\\no\\nu\\nt\\nD_{out} \\\\times D_{out} \\\\times D_{out}\\nD\\no\\nu\\nt\\n\\u200b\\n×\\nD\\no\\nu\\nt\\n\\u200b\\n×\\nD\\no\\nu\\nt\\n\\u200b\\n.\\nD'),\n",
       " Document(metadata={}, page_content='D\\no\\nu\\nt\\n\\u200b\\n×\\nD\\no\\nu\\nt\\n\\u200b\\n×\\nD\\no\\nu\\nt\\n\\u200b\\n.\\nD\\no\\nu\\nt\\nD_{out}\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\nH_{out}\\nH\\no\\nu\\nt\\n\\u200b\\nand\\nW\\no\\nu\\nt\\nW_{out}\\nW\\no\\nu\\nt\\n\\u200b\\ncan be either a\\nint\\n, or\\nNone\\nwhich means the size will be the same as that of the input.\\nreturn_indices\\n(\\nbool\\n) – if\\nTrue\\n, will return the indices along with the outputs.\\nUseful to pass to nn.MaxUnpool3d. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)'),\n",
       " Document(metadata={}, page_content='in\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n,\\nwhere\\n(\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n=\\noutput_size\\n(D_{out}, H_{out}, W_{out})=\\\\text{output\\\\_size}\\n(\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n='),\n",
       " Document(metadata={}, page_content='(\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n=\\noutput_size\\n.\\nExamples\\n>>>\\n# target output size of 5x7x9\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveMaxPool3d\\n((\\n5\\n,\\n7\\n,\\n9\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n8\\n,\\n9\\n,\\n10\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# target output size of 7x7x7 (cube)\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveMaxPool3d\\n(\\n7\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n10\\n,\\n9\\n,\\n8\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# target output size of 7x9x8\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveMaxPool3d\\n((\\n7\\n,\\nNone\\n,\\nNone\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n10'),\n",
       " Document(metadata={}, page_content=',\\nNone\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n10\\n,\\n9\\n,\\n8\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AdaptiveAvgPool1d\\n¶\\nclass\\ntorch.nn.\\nAdaptiveAvgPool1d\\n(\\noutput_size\\n)\\n[source]\\n[source]\\n¶\\nApplies a 1D adaptive average pooling over an input signal composed of several input planes.\\nThe output size is\\nL\\no\\nu\\nt\\nL_{out}\\nL\\no\\nu\\nt\\n\\u200b\\n, for any input size.\\nThe number of output features is equal to the number of input planes.\\nParameters\\noutput_size\\n(\\nUnion\\n[\\nint\\n,\\nTuple\\n[\\nint\\n]\\n]\\n) – the target output size\\nL\\no\\nu\\nt\\nL_{out}\\nL\\no\\nu\\nt\\n\\u200b\\n.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nL\\ni\\nn\\n)\\n(N, C, L_{in})\\n(\\nN\\n,\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\nor\\n('),\n",
       " Document(metadata={}, page_content=',\\nL\\ni\\nn\\n)\\n(N, C, L_{in})\\n(\\nN\\n,\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\ni\\nn\\n)\\n(C, L_{in})\\n(\\nC\\n,\\nL\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(N, C, L_{out})\\n(\\nN\\n,\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n)\\n(C, L_{out})\\n(\\nC\\n,\\nL\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nL\\no\\nu\\nt\\n=\\noutput_size\\nL_{out}=\\\\text{output\\\\_size}\\nL\\no\\nu\\nt\\n\\u200b\\n=\\noutput_size\\n.\\nExamples\\n>>>\\n# target output size of 5\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveAvgPool1d\\n(\\n5\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n8\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with'),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AdaptiveAvgPool2d\\n¶\\nclass\\ntorch.nn.\\nAdaptiveAvgPool2d\\n(\\noutput_size\\n)\\n[source]\\n[source]\\n¶\\nApplies a 2D adaptive average pooling over an input signal composed of several input planes.\\nThe output is of size H x W, for any input size.\\nThe number of output features is equal to the number of input planes.\\nParameters\\noutput_size\\n(\\nUnion\\n[\\nint\\n,\\nNone\\n,\\nTuple\\n[\\nOptional\\n[\\nint\\n]\\n,\\nOptional\\n[\\nint\\n]\\n]\\n]\\n) – the target output size of the image of the form H x W.'),\n",
       " Document(metadata={}, page_content='Can be a tuple (H, W) or a single H for a square image H x H.\\nH and W can be either a\\nint\\n, or\\nNone\\nwhich means the size will\\nbe the same as that of the input.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nS\\n0\\n,\\nS\\n1\\n)\\n(N, C, S_{0}, S_{1})\\n(\\nN\\n,\\nC\\n,\\nS\\n0\\n\\u200b\\n,\\nS\\n1\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nS\\n0\\n,\\nS\\n1\\n)\\n(C, S_{0}, S_{1})\\n(\\nC\\n,\\nS\\n0\\n\\u200b\\n,\\nS\\n1\\n\\u200b\\n)\\n, where\\nS\\n=\\noutput_size'),\n",
       " Document(metadata={}, page_content='(\\nC\\n,\\nS\\n0\\n\\u200b\\n,\\nS\\n1\\n\\u200b\\n)\\n, where\\nS\\n=\\noutput_size\\nS=\\\\text{output\\\\_size}\\nS\\n=\\noutput_size\\n.\\nExamples\\n>>>\\n# target output size of 5x7\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveAvgPool2d\\n((\\n5\\n,\\n7\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n8\\n,\\n9\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# target output size of 7x7 (square)\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveAvgPool2d\\n(\\n7\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n10\\n,\\n9\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# target output size of 10x7\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveAvgPool2d\\n((\\nNone\\n,\\n7\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,'),\n",
       " Document(metadata={}, page_content='((\\nNone\\n,\\n7\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n10\\n,\\n9\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AdaptiveAvgPool3d\\n¶\\nclass\\ntorch.nn.\\nAdaptiveAvgPool3d\\n(\\noutput_size\\n)\\n[source]\\n[source]\\n¶\\nApplies a 3D adaptive average pooling over an input signal composed of several input planes.\\nThe output is of size D x H x W, for any input size.\\nThe number of output features is equal to the number of input planes.\\nParameters\\noutput_size\\n(\\nUnion\\n[\\nint\\n,\\nNone\\n,\\nTuple\\n[\\nOptional\\n[\\nint\\n]\\n,\\nOptional\\n[\\nint\\n]\\n,\\nOptional\\n[\\nint\\n]\\n]\\n]\\n) – the target output size of the form D x H x W.'),\n",
       " Document(metadata={}, page_content=') – the target output size of the form D x H x W.\\nCan be a tuple (D, H, W) or a single number D for a cube D x D x D.\\nD, H and W can be either a\\nint\\n, or\\nNone\\nwhich means the size will\\nbe the same as that of the input.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nS\\n0\\n,\\nS\\n1\\n,\\nS\\n2\\n)\\n(N, C, S_{0}, S_{1}, S_{2})\\n(\\nN'),\n",
       " Document(metadata={}, page_content='S\\n0\\n,\\nS\\n1\\n,\\nS\\n2\\n)\\n(N, C, S_{0}, S_{1}, S_{2})\\n(\\nN\\n,\\nC\\n,\\nS\\n0\\n\\u200b\\n,\\nS\\n1\\n\\u200b\\n,\\nS\\n2\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nS\\n0\\n,\\nS\\n1\\n,\\nS\\n2\\n)\\n(C, S_{0}, S_{1}, S_{2})\\n(\\nC\\n,\\nS\\n0\\n\\u200b\\n,\\nS\\n1\\n\\u200b\\n,\\nS\\n2\\n\\u200b\\n)\\n,\\nwhere\\nS\\n=\\noutput_size\\nS=\\\\text{output\\\\_size}\\nS\\n=\\noutput_size\\n.\\nExamples\\n>>>\\n# target output size of 5x7x9\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveAvgPool3d\\n((\\n5\\n,\\n7\\n,\\n9\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n8\\n,\\n9\\n,\\n10\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# target output size of 7x7x7 (cube)\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveAvgPool3d\\n(\\n7\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1'),\n",
       " Document(metadata={}, page_content='(\\n7\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n10\\n,\\n9\\n,\\n8\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# target output size of 7x9x8\\n>>>\\nm\\n=\\nnn\\n.\\nAdaptiveAvgPool3d\\n((\\n7\\n,\\nNone\\n,\\nNone\\n))\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n64\\n,\\n10\\n,\\n9\\n,\\n8\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ReflectionPad1d\\n¶\\nclass\\ntorch.nn.\\nReflectionPad1d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor using the reflection of the input boundary.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 2-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n)\\nShape:\\nInput:\\n(\\nC\\n,\\nW\\ni\\nn\\n)\\n(C, W_{in})\\n(\\nC\\n,\\nW\\nin\\n\\u200b'),\n",
       " Document(metadata={}, page_content='Input:\\n(\\nC\\n,\\nW\\ni\\nn\\n)\\n(C, W_{in})\\n(\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\ni\\nn\\n)\\n(N, C, W_{in})\\n(\\nN\\n,\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(C, W_{out})\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, W_{out})\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nReflectionPad1d\\n(\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\narange\\n(\\n8\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat\\n)\\n.'),\n",
       " Document(metadata={}, page_content='=\\ntorch\\n.\\narange\\n(\\n8\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat\\n)\\n.\\nreshape\\n(\\n1\\n,\\n2\\n,\\n4\\n)\\n>>>\\ninput\\ntensor([[[0., 1., 2., 3.],\\n[4., 5., 6., 7.]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[2., 1., 0., 1., 2., 3., 2., 1.],\\n[6., 5., 4., 5., 6., 7., 6., 5.]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nReflectionPad1d\\n((\\n3\\n,\\n1\\n))\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[3., 2., 1., 0., 1., 2., 3., 2.],\\n[7., 6., 5., 4., 5., 6., 7., 6.]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme'),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ReflectionPad2d\\n¶\\nclass\\ntorch.nn.\\nReflectionPad2d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor using the reflection of the input boundary.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 4-\\ntuple\\n, uses (\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,'),\n",
       " Document(metadata={}, page_content=',\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n)\\nNote that padding size should be less than the corresponding input dimension.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nwhere\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nReflectionPad2d\\n(\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\narange\\n(\\n9\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat'),\n",
       " Document(metadata={}, page_content='=\\ntorch\\n.\\narange\\n(\\n9\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat\\n)\\n.\\nreshape\\n(\\n1\\n,\\n1\\n,\\n3\\n,\\n3\\n)\\n>>>\\ninput\\ntensor([[[[0., 1., 2.],\\n[3., 4., 5.],\\n[6., 7., 8.]]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[[8., 7., 6., 7., 8., 7., 6.],\\n[5., 4., 3., 4., 5., 4., 3.],\\n[2., 1., 0., 1., 2., 1., 0.],\\n[5., 4., 3., 4., 5., 4., 3.],\\n[8., 7., 6., 7., 8., 7., 6.],\\n[5., 4., 3., 4., 5., 4., 3.],\\n[2., 1., 0., 1., 2., 1., 0.]]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nReflectionPad2d\\n((\\n1\\n,\\n1\\n,\\n2\\n,\\n0\\n))\\n>>>\\nm\\n(\\ninput\\n)'),\n",
       " Document(metadata={}, page_content='((\\n1\\n,\\n1\\n,\\n2\\n,\\n0\\n))\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[[7., 6., 7., 8., 7.],\\n[4., 3., 4., 5., 4.],\\n[1., 0., 1., 2., 1.],\\n[4., 3., 4., 5., 4.],\\n[7., 6., 7., 8., 7.]]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ReflectionPad3d\\n¶\\nclass\\ntorch.nn.\\nReflectionPad3d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor using the reflection of the input boundary.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 6-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,'),\n",
       " Document(metadata={}, page_content=',\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n,\\npadding_front\\n\\\\text{padding\\\\_front}\\npadding_front\\n,\\npadding_back\\n\\\\text{padding\\\\_back}\\npadding_back\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n('),\n",
       " Document(metadata={}, page_content='t\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n,\\nwhere\\nD\\no\\nu\\nt\\n=\\nD\\ni\\nn\\n+\\npadding_front\\n+\\npadding_back\\nD_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}\\nD\\no\\nu\\nt\\n\\u200b\\n=\\nD\\nin\\n\\u200b\\n+\\npadding_front\\n+\\npadding_back\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nReflectionPad3d\\n(\\n1\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\narange\\n(\\n8\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat\\n)\\n.\\nreshape\\n(\\n1\\n,\\n1\\n,\\n2\\n,\\n2\\n,\\n2\\n)\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[[[7., 6., 7., 6.],\\n[5., 4., 5., 4.],\\n[7., 6., 7., 6.],\\n[5., 4., 5., 4.]],\\n[[3., 2., 3., 2.],\\n[1., 0., 1., 0.],\\n[3., 2., 3., 2.],'),\n",
       " Document(metadata={}, page_content='[1., 0., 1., 0.],\\n[3., 2., 3., 2.],\\n[1., 0., 1., 0.]],\\n[[7., 6., 7., 6.],\\n[5., 4., 5., 4.],\\n[7., 6., 7., 6.],\\n[5., 4., 5., 4.]],\\n[[3., 2., 3., 2.],\\n[1., 0., 1., 0.],\\n[3., 2., 3., 2.],\\n[1., 0., 1., 0.]]]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ReplicationPad1d\\n¶\\nclass\\ntorch.nn.\\nReplicationPad1d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor using replication of the input boundary.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 2-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n)\\nShape:\\nInput:\\n(\\nC\\n,\\nW\\ni\\nn\\n)\\n(C, W_{in})\\n(\\nC\\n,\\nW\\nin\\n\\u200b\\n)'),\n",
       " Document(metadata={}, page_content='Input:\\n(\\nC\\n,\\nW\\ni\\nn\\n)\\n(C, W_{in})\\n(\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\ni\\nn\\n)\\n(N, C, W_{in})\\n(\\nN\\n,\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(C, W_{out})\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, W_{out})\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nReplicationPad1d\\n(\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\narange\\n(\\n8\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat\\n)\\n.'),\n",
       " Document(metadata={}, page_content='=\\ntorch\\n.\\narange\\n(\\n8\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat\\n)\\n.\\nreshape\\n(\\n1\\n,\\n2\\n,\\n4\\n)\\n>>>\\ninput\\ntensor([[[0., 1., 2., 3.],\\n[4., 5., 6., 7.]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[0., 0., 0., 1., 2., 3., 3., 3.],\\n[4., 4., 4., 5., 6., 7., 7., 7.]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nReplicationPad1d\\n((\\n3\\n,\\n1\\n))\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[0., 0., 0., 0., 1., 2., 3., 3.],\\n[4., 4., 4., 4., 5., 6., 7., 7.]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme'),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ReplicationPad2d\\n¶\\nclass\\ntorch.nn.\\nReplicationPad2d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor using replication of the input boundary.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 4-\\ntuple\\n, uses (\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,'),\n",
       " Document(metadata={}, page_content=',\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nReplicationPad2d\\n(\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\narange\\n(\\n9\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat\\n)\\n.\\nreshape\\n(\\n1\\n,\\n1\\n,\\n3\\n,\\n3\\n)\\n>>>\\ninput\\ntensor([[[[0., 1., 2.],'),\n",
       " Document(metadata={}, page_content='1\\n,\\n1\\n,\\n3\\n,\\n3\\n)\\n>>>\\ninput\\ntensor([[[[0., 1., 2.],\\n[3., 4., 5.],\\n[6., 7., 8.]]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[[0., 0., 0., 1., 2., 2., 2.],\\n[0., 0., 0., 1., 2., 2., 2.],\\n[0., 0., 0., 1., 2., 2., 2.],\\n[3., 3., 3., 4., 5., 5., 5.],\\n[6., 6., 6., 7., 8., 8., 8.],\\n[6., 6., 6., 7., 8., 8., 8.],\\n[6., 6., 6., 7., 8., 8., 8.]]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nReplicationPad2d\\n((\\n1\\n,\\n1\\n,\\n2\\n,\\n0\\n))\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[[0., 0., 1., 2., 2.],\\n[0., 0., 1., 2., 2.],'),\n",
       " Document(metadata={}, page_content='[0., 0., 1., 2., 2.],\\n[0., 0., 1., 2., 2.],\\n[3., 3., 4., 5., 5.],\\n[6., 6., 7., 8., 8.]]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ReplicationPad3d\\n¶\\nclass\\ntorch.nn.\\nReplicationPad3d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor using replication of the input boundary.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 6-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,'),\n",
       " Document(metadata={}, page_content=',\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n,\\npadding_front\\n\\\\text{padding\\\\_front}\\npadding_front\\n,\\npadding_back\\n\\\\text{padding\\\\_back}\\npadding_back\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n('),\n",
       " Document(metadata={}, page_content='t\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n,\\nwhere\\nD\\no\\nu\\nt\\n=\\nD\\ni\\nn\\n+\\npadding_front\\n+\\npadding_back\\nD_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}\\nD\\no\\nu\\nt\\n\\u200b\\n=\\nD\\nin\\n\\u200b\\n+\\npadding_front\\n+\\npadding_back\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nReplicationPad3d\\n(\\n3\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n16\\n,\\n3\\n,\\n8\\n,\\n320\\n,\\n480\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nReplicationPad3d\\n((\\n3\\n,\\n3\\n,\\n6\\n,\\n6\\n,\\n1\\n,\\n1\\n))\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content=',\\n1\\n,\\n1\\n))\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ZeroPad1d\\n¶\\nclass\\ntorch.nn.\\nZeroPad1d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor boundaries with zero.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in both boundaries. If a 2-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n)\\nShape:\\nInput:\\n(\\nC\\n,\\nW\\ni\\nn\\n)\\n(C, W_{in})\\n(\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\ni\\nn\\n)'),\n",
       " Document(metadata={}, page_content=')\\n(C, W_{in})\\n(\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\ni\\nn\\n)\\n(N, C, W_{in})\\n(\\nN\\n,\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(C, W_{out})\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, W_{out})\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nZeroPad1d\\n(\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n2\\n,\\n4\\n)\\n>>>\\ninput'),\n",
       " Document(metadata={}, page_content='>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n2\\n,\\n4\\n)\\n>>>\\ninput\\ntensor([[[-1.0491, -0.7152, -0.0749,  0.8530],\\n[-1.3287,  1.8966,  0.1466, -0.2771]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[ 0.0000,  0.0000, -1.0491, -0.7152, -0.0749,  0.8530,  0.0000,\\n0.0000],\\n[ 0.0000,  0.0000, -1.3287,  1.8966,  0.1466, -0.2771,  0.0000,\\n0.0000]]])\\n>>>\\nm\\n=\\nnn\\n.\\nZeroPad1d\\n(\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n2\\n,\\n3\\n)\\n>>>\\ninput\\ntensor([[[ 1.6616,  1.4523, -1.1255],\\n[-3.6372,  0.1182, -1.8652]]])\\n>>>\\nm\\n(\\ninput\\n)'),\n",
       " Document(metadata={}, page_content='[-3.6372,  0.1182, -1.8652]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[ 0.0000,  0.0000,  1.6616,  1.4523, -1.1255,  0.0000,  0.0000],\\n[ 0.0000,  0.0000, -3.6372,  0.1182, -1.8652,  0.0000,  0.0000]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nZeroPad1d\\n((\\n3\\n,\\n1\\n))\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[ 0.0000,  0.0000,  0.0000,  1.6616,  1.4523, -1.1255,  0.0000],\\n[ 0.0000,  0.0000,  0.0000, -3.6372,  0.1182, -1.8652,  0.0000]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with'),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ZeroPad2d\\n¶\\nclass\\ntorch.nn.\\nZeroPad2d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor boundaries with zero.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 4-\\ntuple\\n, uses (\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}'),\n",
       " Document(metadata={}, page_content=',\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nZeroPad2d\\n(\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n1\\n,\\n3\\n,\\n3\\n)\\n>>>\\ninput\\ntensor([[[[-0.1678, -0.4418,  1.9466],\\n[ 0.9604, -0.4219, -0.5241],'),\n",
       " Document(metadata={}, page_content='[ 0.9604, -0.4219, -0.5241],\\n[-0.9162, -0.5436, -0.6446]]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\\n[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\\n[ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],\\n[ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],\\n[ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],\\n[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],'),\n",
       " Document(metadata={}, page_content='[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nZeroPad2d\\n((\\n1\\n,\\n1\\n,\\n2\\n,\\n0\\n))\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\\n[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\\n[ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],\\n[ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],\\n[ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx'),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ZeroPad3d\\n¶\\nclass\\ntorch.nn.\\nZeroPad3d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor boundaries with zero.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 6-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}'),\n",
       " Document(metadata={}, page_content=',\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n,\\npadding_front\\n\\\\text{padding\\\\_front}\\npadding_front\\n,\\npadding_back\\n\\\\text{padding\\\\_back}\\npadding_back\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nD\\no\\nu\\nt\\n=\\nD\\ni\\nn\\n+\\npadding_front\\n+\\npadding_back\\nD_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}\\nD\\no\\nu\\nt\\n\\u200b\\n=\\nD\\nin\\n\\u200b\\n+\\npadding_front\\n+\\npadding_back\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW'),\n",
       " Document(metadata={}, page_content='H\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nZeroPad3d\\n(\\n3\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n16\\n,\\n3\\n,\\n10\\n,\\n20\\n,\\n30\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nZeroPad3d\\n((\\n3\\n,\\n3\\n,\\n6\\n,\\n6\\n,\\n0\\n,\\n1\\n))\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.'),\n",
       " Document(metadata={}, page_content='Previous\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ConstantPad1d\\n¶\\nclass\\ntorch.nn.\\nConstantPad1d\\n(\\npadding\\n,\\nvalue\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor boundaries with a constant value.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in both boundaries. If a 2-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n)\\nShape:\\nInput:\\n(\\nC\\n,\\nW\\ni\\nn\\n)\\n(C, W_{in})\\n(\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\nor'),\n",
       " Document(metadata={}, page_content='(\\nC\\n,\\nW\\ni\\nn\\n)\\n(C, W_{in})\\n(\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\ni\\nn\\n)\\n(N, C, W_{in})\\n(\\nN\\n,\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(C, W_{out})\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, W_{out})\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nConstantPad1d\\n(\\n2\\n,\\n3.5\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n2\\n,\\n4\\n)\\n>>>\\ninput'),\n",
       " Document(metadata={}, page_content='>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n2\\n,\\n4\\n)\\n>>>\\ninput\\ntensor([[[-1.0491, -0.7152, -0.0749,  0.8530],\\n[-1.3287,  1.8966,  0.1466, -0.2771]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,\\n3.5000],\\n[ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,\\n3.5000]]])\\n>>>\\nm\\n=\\nnn\\n.\\nConstantPad1d\\n(\\n2\\n,\\n3.5\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n2\\n,\\n3\\n)\\n>>>\\ninput\\ntensor([[[ 1.6616,  1.4523, -1.1255],\\n[-3.6372,  0.1182, -1.8652]]])\\n>>>\\nm\\n(\\ninput\\n)'),\n",
       " Document(metadata={}, page_content='[-3.6372,  0.1182, -1.8652]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],\\n[ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nConstantPad1d\\n((\\n3\\n,\\n1\\n),\\n3.5\\n)\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],\\n[ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.'),\n",
       " Document(metadata={}, page_content='Previous\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ConstantPad2d\\n¶\\nclass\\ntorch.nn.\\nConstantPad2d\\n(\\npadding\\n,\\nvalue\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor boundaries with a constant value.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 4-\\ntuple\\n, uses (\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,'),\n",
       " Document(metadata={}, page_content=',\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nConstantPad2d\\n(\\n2\\n,\\n3.5\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n1\\n,\\n2\\n,\\n2\\n)\\n>>>\\ninput\\ntensor([[[ 1.6585,  0.4320],\\n[-0.8701, -0.4649]]])\\n>>>\\nm\\n(\\ninput\\n)'),\n",
       " Document(metadata={}, page_content='[-0.8701, -0.4649]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\\n[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\\n[ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],\\n[ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],\\n[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\\n[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nConstantPad2d\\n((\\n3\\n,\\n0\\n,\\n2\\n,\\n1\\n),\\n3.5\\n)\\n>>>\\nm\\n('),\n",
       " Document(metadata={}, page_content='.\\nConstantPad2d\\n((\\n3\\n,\\n0\\n,\\n2\\n,\\n1\\n),\\n3.5\\n)\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\\n[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\\n[ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],\\n[ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],\\n[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ConstantPad3d\\n¶\\nclass\\ntorch.nn.\\nConstantPad3d\\n(\\npadding\\n,\\nvalue\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor boundaries with a constant value.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same\\npadding in all boundaries. If a 6-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,'),\n",
       " Document(metadata={}, page_content=',\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n,\\npadding_front\\n\\\\text{padding\\\\_front}\\npadding_front\\n,\\npadding_back\\n\\\\text{padding\\\\_back}\\npadding_back\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n('),\n",
       " Document(metadata={}, page_content='t\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nD\\no\\nu\\nt\\n=\\nD\\ni\\nn\\n+\\npadding_front\\n+\\npadding_back\\nD_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}\\nD\\no\\nu\\nt\\n\\u200b\\n=\\nD\\nin\\n\\u200b\\n+\\npadding_front\\n+\\npadding_back\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH'),\n",
       " Document(metadata={}, page_content='H\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nConstantPad3d\\n(\\n3\\n,\\n3.5\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n16\\n,\\n3\\n,\\n10\\n,\\n20\\n,\\n30\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nConstantPad3d\\n((\\n3\\n,\\n3\\n,\\n6\\n,\\n6\\n,\\n0\\n,\\n1\\n),\\n3.5\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='1\\n),\\n3.5\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='CircularPad1d\\n¶\\nclass\\ntorch.nn.\\nCircularPad1d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor using circular padding of the input boundary.\\nTensor values at the beginning of the dimension are used to pad the end,\\nand values at the end are used to pad the beginning. If negative padding is\\napplied then the ends of the tensor get removed.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same'),\n",
       " Document(metadata={}, page_content='int\\n, uses the same\\npadding in all boundaries. If a 2-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n)\\nShape:\\nInput:\\n(\\nC\\n,\\nW\\ni\\nn\\n)\\n(C, W_{in})\\n(\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\ni\\nn\\n)\\n(N, C, W_{in})\\n(\\nN\\n,\\nC\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(C, W_{out})\\n(\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, W_{out})\\n(\\nN\\n,\\nC\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right'),\n",
       " Document(metadata={}, page_content='W\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nCircularPad1d\\n(\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\narange\\n(\\n8\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat\\n)\\n.\\nreshape\\n(\\n1\\n,\\n2\\n,\\n4\\n)\\n>>>\\ninput\\ntensor([[[0., 1., 2., 3.],\\n[4., 5., 6., 7.]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[2., 3., 0., 1., 2., 3., 0., 1.],\\n[6., 7., 4., 5., 6., 7., 4., 5.]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn'),\n",
       " Document(metadata={}, page_content='>>>\\nm\\n=\\nnn\\n.\\nCircularPad1d\\n((\\n3\\n,\\n1\\n))\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[1., 2., 3., 0., 1., 2., 3., 0.],\\n[5., 6., 7., 4., 5., 6., 7., 4.]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='CircularPad2d\\n¶\\nclass\\ntorch.nn.\\nCircularPad2d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor using circular padding of the input boundary.\\nTensor values at the beginning of the dimension are used to pad the end,\\nand values at the end are used to pad the beginning. If negative padding is\\napplied then the ends of the tensor get removed.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same'),\n",
       " Document(metadata={}, page_content='int\\n, uses the same\\npadding in all boundaries. If a 4-\\ntuple\\n, uses (\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, H_{in}, W_{in})\\n(\\nC\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n('),\n",
       " Document(metadata={}, page_content=',\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, H_{out}, W_{out})\\n(\\nC\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n, where\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right'),\n",
       " Document(metadata={}, page_content='W\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nCircularPad2d\\n(\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\narange\\n(\\n9\\n,\\ndtype\\n=\\ntorch\\n.\\nfloat\\n)\\n.\\nreshape\\n(\\n1\\n,\\n1\\n,\\n3\\n,\\n3\\n)\\n>>>\\ninput\\ntensor([[[[0., 1., 2.],\\n[3., 4., 5.],\\n[6., 7., 8.]]]])\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[[4., 5., 3., 4., 5., 3., 4.],\\n[7., 8., 6., 7., 8., 6., 7.],\\n[1., 2., 0., 1., 2., 0., 1.],\\n[4., 5., 3., 4., 5., 3., 4.],\\n[7., 8., 6., 7., 8., 6., 7.],\\n[1., 2., 0., 1., 2., 0., 1.],\\n[4., 5., 3., 4., 5., 3., 4.]]]])\\n>>>'),\n",
       " Document(metadata={}, page_content='[4., 5., 3., 4., 5., 3., 4.]]]])\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nCircularPad2d\\n((\\n1\\n,\\n1\\n,\\n2\\n,\\n0\\n))\\n>>>\\nm\\n(\\ninput\\n)\\ntensor([[[[5., 3., 4., 5., 3.],\\n[8., 6., 7., 8., 6.],\\n[2., 0., 1., 2., 0.],\\n[5., 3., 4., 5., 3.],\\n[8., 6., 7., 8., 6.]]]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='CircularPad3d\\n¶\\nclass\\ntorch.nn.\\nCircularPad3d\\n(\\npadding\\n)\\n[source]\\n[source]\\n¶\\nPads the input tensor using circular padding of the input boundary.\\nTensor values at the beginning of the dimension are used to pad the end,\\nand values at the end are used to pad the beginning. If negative padding is\\napplied then the ends of the tensor get removed.\\nFor\\nN\\n-dimensional padding, use\\ntorch.nn.functional.pad()\\n.\\nParameters\\npadding\\n(\\nint\\n,\\ntuple\\n) – the size of the padding. If is\\nint\\n, uses the same'),\n",
       " Document(metadata={}, page_content='int\\n, uses the same\\npadding in all boundaries. If a 6-\\ntuple\\n, uses\\n(\\npadding_left\\n\\\\text{padding\\\\_left}\\npadding_left\\n,\\npadding_right\\n\\\\text{padding\\\\_right}\\npadding_right\\n,\\npadding_top\\n\\\\text{padding\\\\_top}\\npadding_top\\n,\\npadding_bottom\\n\\\\text{padding\\\\_bottom}\\npadding_bottom\\n,\\npadding_front\\n\\\\text{padding\\\\_front}\\npadding_front\\n,\\npadding_back\\n\\\\text{padding\\\\_back}\\npadding_back\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(N, C, D_{in}, H_{in}, W_{in})\\n(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\ni\\nn\\n,\\nH\\ni\\nn\\n,\\nW\\ni\\nn\\n)\\n(C, D_{in}, H_{in}, W_{in})\\n(\\nC\\n,\\nD\\nin\\n\\u200b\\n,\\nH\\nin\\n\\u200b\\n,\\nW\\nin\\n\\u200b\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(N, C, D_{out}, H_{out}, W_{out})\\n(\\nN\\n,\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n,\\nH\\no\\nu\\nt\\n,\\nW\\no\\nu\\nt\\n)\\n(C, D_{out}, H_{out}, W_{out})\\n(\\nC\\n,\\nD\\no\\nu\\nt\\n\\u200b\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n,\\nW\\no\\nu\\nt\\n\\u200b\\n)\\n,\\nwhere\\nD\\no\\nu\\nt\\n=\\nD\\ni\\nn\\n+\\npadding_front\\n+\\npadding_back\\nD_{out} = D_{in} + \\\\text{padding\\\\_front} + \\\\text{padding\\\\_back}\\nD\\no\\nu\\nt'),\n",
       " Document(metadata={}, page_content='D\\no\\nu\\nt\\n\\u200b\\n=\\nD\\nin\\n\\u200b\\n+\\npadding_front\\n+\\npadding_back\\nH\\no\\nu\\nt\\n=\\nH\\ni\\nn\\n+\\npadding_top\\n+\\npadding_bottom\\nH_{out} = H_{in} + \\\\text{padding\\\\_top} + \\\\text{padding\\\\_bottom}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nH\\nin\\n\\u200b\\n+\\npadding_top\\n+\\npadding_bottom\\nW\\no\\nu\\nt\\n=\\nW\\ni\\nn\\n+\\npadding_left\\n+\\npadding_right\\nW_{out} = W_{in} + \\\\text{padding\\\\_left} + \\\\text{padding\\\\_right}\\nW\\no\\nu\\nt\\n\\u200b\\n=\\nW\\nin\\n\\u200b\\n+\\npadding_left\\n+\\npadding_right\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nCircularPad3d\\n(\\n3\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n16\\n,\\n3\\n,\\n8\\n,\\n320\\n,\\n480\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>'),\n",
       " Document(metadata={}, page_content='3\\n,\\n8\\n,\\n320\\n,\\n480\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# using different paddings for different sides\\n>>>\\nm\\n=\\nnn\\n.\\nCircularPad3d\\n((\\n3\\n,\\n3\\n,\\n6\\n,\\n6\\n,\\n1\\n,\\n1\\n))\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ELU\\n¶\\nclass\\ntorch.nn.\\nELU\\n(\\nalpha\\n=\\n1.0\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the Exponential Linear Unit (ELU) function, element-wise.\\nMethod described in the paper:\\nFast and Accurate Deep Network Learning by Exponential Linear\\nUnits (ELUs)\\n.\\nELU is defined as:\\nELU\\n(\\nx\\n)\\n=\\n{\\nx\\n,\\nif\\nx\\n>\\n0\\nα\\n∗\\n(\\nexp\\n\\u2061\\n(\\nx\\n)\\n−\\n1\\n)\\n,\\nif\\nx\\n≤\\n0\\n\\\\text{ELU}(x) = \\\\begin{cases}\\nx, & \\\\text{ if } x > 0\\\\\\\\\\n\\\\alpha * (\\\\exp(x) - 1), & \\\\text{ if } x \\\\leq 0\\n\\\\end{cases}\\nELU\\n(\\nx\\n)\\n=\\n{\\nx\\n,\\nα\\n∗\\n(\\nexp\\n(\\nx\\n)\\n−\\n1\\n)\\n,\\n\\u200b\\nif\\nx\\n>'),\n",
       " Document(metadata={}, page_content='(\\nx\\n)\\n=\\n{\\nx\\n,\\nα\\n∗\\n(\\nexp\\n(\\nx\\n)\\n−\\n1\\n)\\n,\\n\\u200b\\nif\\nx\\n>\\n0\\nif\\nx\\n≤\\n0\\n\\u200b\\nParameters\\nalpha\\n(\\nfloat\\n) – the\\nα\\n\\\\alpha\\nα\\nvalue for the ELU formulation. Default: 1.0\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nELU\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.'),\n",
       " Document(metadata={}, page_content='Previous\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Hardshrink\\n¶\\nclass\\ntorch.nn.\\nHardshrink\\n(\\nlambd\\n=\\n0.5\\n)\\n[source]\\n[source]\\n¶\\nApplies the Hard Shrinkage (Hardshrink) function element-wise.\\nHardshrink is defined as:\\nHardShrink\\n(\\nx\\n)\\n=\\n{\\nx\\n,\\nif\\nx\\n>\\nλ\\nx\\n,\\nif\\nx\\n<\\n−\\nλ\\n0\\n,\\notherwise\\n\\\\text{HardShrink}(x) =\\n\\\\begin{cases}\\nx, & \\\\text{ if } x > \\\\lambda \\\\\\\\\\nx, & \\\\text{ if } x < -\\\\lambda \\\\\\\\\\n0, & \\\\text{ otherwise }\\n\\\\end{cases}\\nHardShrink\\n(\\nx\\n)\\n=\\n⎩\\n⎨\\n⎧\\n\\u200b\\nx\\n,\\nx\\n,\\n0\\n,\\n\\u200b\\nif\\nx\\n>\\nλ\\nif\\nx\\n<\\n−\\nλ\\notherwise\\n\\u200b\\nParameters\\nlambd\\n(\\nfloat\\n) – the\\nλ\\n\\\\lambda\\nλ'),\n",
       " Document(metadata={}, page_content='\\u200b\\nParameters\\nlambd\\n(\\nfloat\\n) – the\\nλ\\n\\\\lambda\\nλ\\nvalue for the Hardshrink formulation. Default: 0.5\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nHardshrink\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Hardsigmoid\\n¶\\nclass\\ntorch.nn.\\nHardsigmoid\\n(\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the Hardsigmoid function element-wise.\\nHardsigmoid is defined as:\\nHardsigmoid\\n(\\nx\\n)\\n=\\n{\\n0\\nif\\nx\\n≤\\n−\\n3\\n,\\n1\\nif\\nx\\n≥\\n+\\n3\\n,\\nx\\n/\\n6\\n+\\n1\\n/\\n2\\notherwise\\n\\\\text{Hardsigmoid}(x) = \\\\begin{cases}\\n    0 & \\\\text{if~} x \\\\le -3, \\\\\\\\\\n    1 & \\\\text{if~} x \\\\ge +3, \\\\\\\\\\n    x / 6 + 1 / 2 & \\\\text{otherwise}\\n\\\\end{cases}\\nHardsigmoid\\n(\\nx\\n)\\n=\\n⎩\\n⎨\\n⎧\\n\\u200b\\n0\\n1\\nx\\n/6\\n+\\n1/2\\n\\u200b\\nif\\nx\\n≤\\n−\\n3\\n,\\nif\\nx\\n≥\\n+\\n3\\n,\\notherwise\\n\\u200b\\nParameters\\ninplace\\n(\\nbool'),\n",
       " Document(metadata={}, page_content='x\\n≥\\n+\\n3\\n,\\notherwise\\n\\u200b\\nParameters\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nHardsigmoid\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Hardtanh\\n¶\\nclass\\ntorch.nn.\\nHardtanh\\n(\\nmin_val\\n=\\n-1.0\\n,\\nmax_val\\n=\\n1.0\\n,\\ninplace\\n=\\nFalse\\n,\\nmin_value\\n=\\nNone\\n,\\nmax_value\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies the HardTanh function element-wise.\\nHardTanh is defined as:\\nHardTanh\\n(\\nx\\n)\\n=\\n{\\nmax_val\\nif\\nx\\n>\\nmax_val\\nmin_val\\nif\\nx\\n<\\nmin_val\\nx\\notherwise\\n\\\\text{HardTanh}(x) = \\\\begin{cases}\\n    \\\\text{max\\\\_val} & \\\\text{ if } x > \\\\text{ max\\\\_val } \\\\\\\\\\n    \\\\text{min\\\\_val} & \\\\text{ if } x < \\\\text{ min\\\\_val } \\\\\\\\\\n    x & \\\\text{ otherwise } \\\\\\\\\\n\\\\end{cases}\\nHardTanh\\n(\\nx'),\n",
       " Document(metadata={}, page_content='\\\\end{cases}\\nHardTanh\\n(\\nx\\n)\\n=\\n⎩\\n⎨\\n⎧\\n\\u200b\\nmax_val\\nmin_val\\nx\\n\\u200b\\nif\\nx\\n>\\nmax_val\\nif\\nx\\n<\\nmin_val\\notherwise\\n\\u200b\\nParameters\\nmin_val\\n(\\nfloat\\n) – minimum value of the linear region range. Default: -1\\nmax_val\\n(\\nfloat\\n) – maximum value of the linear region range. Default: 1\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nKeyword arguments\\nmin_value\\nand\\nmax_value\\nhave been deprecated in favor of\\nmin_val\\nand\\nmax_val\\n.\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗'),\n",
       " Document(metadata={}, page_content='.\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nHardtanh\\n(\\n-\\n2\\n,\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Hardswish\\n¶\\nclass\\ntorch.nn.\\nHardswish\\n(\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the Hardswish function, element-wise.\\nMethod described in the paper:\\nSearching for MobileNetV3\\n.\\nHardswish is defined as:\\nHardswish\\n(\\nx\\n)\\n=\\n{\\n0\\nif\\nx\\n≤\\n−\\n3\\n,\\nx\\nif\\nx\\n≥\\n+\\n3\\n,\\nx\\n⋅\\n(\\nx\\n+\\n3\\n)\\n/\\n6\\notherwise\\n\\\\text{Hardswish}(x) = \\\\begin{cases}\\n    0 & \\\\text{if~} x \\\\le -3, \\\\\\\\\\n    x & \\\\text{if~} x \\\\ge +3, \\\\\\\\\\n    x \\\\cdot (x + 3) /6 & \\\\text{otherwise}\\n\\\\end{cases}\\nHardswish\\n(\\nx\\n)\\n=\\n⎩\\n⎨\\n⎧\\n\\u200b\\n0\\nx\\nx\\n⋅\\n(\\nx\\n+\\n3\\n)\\n/6\\n\\u200b\\nif\\nx\\n≤\\n−\\n3'),\n",
       " Document(metadata={}, page_content='(\\nx\\n)\\n=\\n⎩\\n⎨\\n⎧\\n\\u200b\\n0\\nx\\nx\\n⋅\\n(\\nx\\n+\\n3\\n)\\n/6\\n\\u200b\\nif\\nx\\n≤\\n−\\n3\\n,\\nif\\nx\\n≥\\n+\\n3\\n,\\notherwise\\n\\u200b\\nParameters\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nHardswish\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LeakyReLU\\n¶\\nclass\\ntorch.nn.\\nLeakyReLU\\n(\\nnegative_slope\\n=\\n0.01\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the LeakyReLU function element-wise.\\nLeakyReLU\\n(\\nx\\n)\\n=\\nmax\\n\\u2061\\n(\\n0\\n,\\nx\\n)\\n+\\nnegative_slope\\n∗\\nmin\\n\\u2061\\n(\\n0\\n,\\nx\\n)\\n\\\\text{LeakyReLU}(x) = \\\\max(0, x) + \\\\text{negative\\\\_slope} * \\\\min(0, x)\\nLeakyReLU\\n(\\nx\\n)\\n=\\nmax\\n(\\n0\\n,\\nx\\n)\\n+\\nnegative_slope\\n∗\\nmin\\n(\\n0\\n,\\nx\\n)\\nor\\nLeakyReLU\\n(\\nx\\n)\\n=\\n{\\nx\\n,\\nif\\nx\\n≥\\n0\\nnegative_slope\\n×\\nx\\n,\\notherwise\\n\\\\text{LeakyReLU}(x) =\\n\\\\begin{cases}\\nx, & \\\\text{ if } x \\\\geq 0 \\\\\\\\'),\n",
       " Document(metadata={}, page_content='\\\\begin{cases}\\nx, & \\\\text{ if } x \\\\geq 0 \\\\\\\\\\n\\\\text{negative\\\\_slope} \\\\times x, & \\\\text{ otherwise }\\n\\\\end{cases}\\nLeakyReLU\\n(\\nx\\n)\\n=\\n{\\nx\\n,\\nnegative_slope\\n×\\nx\\n,\\n\\u200b\\nif\\nx\\n≥\\n0\\notherwise\\n\\u200b\\nParameters\\nnegative_slope\\n(\\nfloat\\n) – Controls the angle of the negative slope (which is used for\\nnegative input values). Default: 1e-2\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\nwhere\\n*\\nmeans, any number of additional\\ndimensions\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)'),\n",
       " Document(metadata={}, page_content='dimensions\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nLeakyReLU\\n(\\n0.1\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LogSigmoid\\n¶\\nclass\\ntorch.nn.\\nLogSigmoid\\n(\\n*\\nargs\\n,\\n**\\nkwargs\\n)\\n[source]\\n[source]\\n¶\\nApplies the Logsigmoid function element-wise.\\nLogSigmoid\\n(\\nx\\n)\\n=\\nlog\\n\\u2061\\n(\\n1\\n1\\n+\\nexp\\n\\u2061\\n(\\n−\\nx\\n)\\n)\\n\\\\text{LogSigmoid}(x) = \\\\log\\\\left(\\\\frac{ 1 }{ 1 + \\\\exp(-x)}\\\\right)\\nLogSigmoid\\n(\\nx\\n)\\n=\\nlo\\ng\\n(\\n1\\n+\\nexp\\n(\\n−\\nx\\n)\\n1\\n\\u200b\\n)\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nLogSigmoid\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n='),\n",
       " Document(metadata={}, page_content='()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='MultiheadAttention\\n¶\\nclass\\ntorch.nn.\\nMultiheadAttention\\n(\\nembed_dim\\n,\\nnum_heads\\n,\\ndropout\\n=\\n0.0\\n,\\nbias\\n=\\nTrue\\n,\\nadd_bias_kv\\n=\\nFalse\\n,\\nadd_zero_attn\\n=\\nFalse\\n,\\nkdim\\n=\\nNone\\n,\\nvdim\\n=\\nNone\\n,\\nbatch_first\\n=\\nFalse\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nAllows the model to jointly attend to information from different representation subspaces.\\nNote\\nSee\\nthis tutorial\\nfor an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.'),\n",
       " Document(metadata={}, page_content='transformer layers.\\nMethod described in the paper:\\nAttention Is All You Need\\n.\\nMulti-Head Attention is defined as:\\nMultiHead\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\nConcat\\n(\\nhead\\n1\\n,\\n…\\n,\\nhead\\nh\\n)\\nW\\nO\\n\\\\text{MultiHead}(Q, K, V) = \\\\text{Concat}(\\\\text{head}_1,\\\\dots,\\\\text{head}_h)W^O\\nMultiHead\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n=\\nConcat\\n(\\nhead\\n1\\n\\u200b\\n,\\n…\\n,\\nhead\\nh\\n\\u200b\\n)\\nW\\nO\\nwhere\\nhead\\ni\\n=\\nAttention\\n(\\nQ\\nW\\ni\\nQ\\n,\\nK\\nW\\ni\\nK\\n,\\nV\\nW\\ni\\nV\\n)\\n\\\\text{head}_i = \\\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\nhead\\ni\\n\\u200b\\n=\\nAttention\\n(\\nQ\\nW\\ni\\nQ\\n\\u200b\\n,\\nK\\nW\\ni\\nK\\n\\u200b\\n,\\nV\\nW\\ni\\nV\\n\\u200b\\n)\\n.'),\n",
       " Document(metadata={}, page_content='Attention\\n(\\nQ\\nW\\ni\\nQ\\n\\u200b\\n,\\nK\\nW\\ni\\nK\\n\\u200b\\n,\\nV\\nW\\ni\\nV\\n\\u200b\\n)\\n.\\nnn.MultiheadAttention\\nwill use the optimized implementations of\\nscaled_dot_product_attention()\\nwhen possible.\\nIn addition to support for the new\\nscaled_dot_product_attention()\\nfunction, for speeding up Inference, MHA will use\\nfastpath inference with support for Nested Tensors, iff:\\nself attention is being computed (i.e.,\\nquery\\n,\\nkey\\n, and\\nvalue\\nare the same tensor).\\ninputs are batched (3D) with\\nbatch_first==True'),\n",
       " Document(metadata={}, page_content='inputs are batched (3D) with\\nbatch_first==True\\nEither autograd is disabled (using\\ntorch.inference_mode\\nor\\ntorch.no_grad\\n) or no tensor argument\\nrequires_grad\\ntraining is disabled (using\\n.eval()\\n)\\nadd_bias_kv\\nis\\nFalse\\nadd_zero_attn\\nis\\nFalse\\nkdim\\nand\\nvdim\\nare equal to\\nembed_dim\\nif a\\nNestedTensor\\nis passed, neither\\nkey_padding_mask\\nnor\\nattn_mask\\nis passed\\nautocast is disabled\\nIf the optimized inference fastpath implementation is in use, a\\nNestedTensor\\ncan be passed for\\nquery\\n/\\nkey\\n/\\nvalue'),\n",
       " Document(metadata={}, page_content='can be passed for\\nquery\\n/\\nkey\\n/\\nvalue\\nto represent padding more efficiently than using a\\npadding mask. In this case, a\\nNestedTensor\\nwill be returned, and an additional speedup proportional to the fraction of the input\\nthat is padding can be expected.\\nParameters\\nembed_dim\\n– Total dimension of the model.\\nnum_heads\\n– Number of parallel attention heads. Note that\\nembed_dim\\nwill be split\\nacross\\nnum_heads\\n(i.e. each head will have dimension\\nembed_dim\\n//\\nnum_heads\\n).\\ndropout\\n– Dropout probability on'),\n",
       " Document(metadata={}, page_content='//\\nnum_heads\\n).\\ndropout\\n– Dropout probability on\\nattn_output_weights\\n. Default:\\n0.0\\n(no dropout).\\nbias\\n– If specified, adds bias to input / output projection layers. Default:\\nTrue\\n.\\nadd_bias_kv\\n– If specified, adds bias to the key and value sequences at dim=0. Default:\\nFalse\\n.\\nadd_zero_attn\\n– If specified, adds a new batch of zeros to the key and value sequences at dim=1.\\nDefault:\\nFalse\\n.\\nkdim\\n– Total number of features for keys. Default:\\nNone\\n(uses\\nkdim=embed_dim\\n).\\nvdim'),\n",
       " Document(metadata={}, page_content='None\\n(uses\\nkdim=embed_dim\\n).\\nvdim\\n– Total number of features for values. Default:\\nNone\\n(uses\\nvdim=embed_dim\\n).\\nbatch_first\\n– If\\nTrue\\n, then the input and output tensors are provided\\nas (batch, seq, feature). Default:\\nFalse\\n(seq, batch, feature).\\nExamples:\\n>>>\\nmultihead_attn\\n=\\nnn\\n.\\nMultiheadAttention\\n(\\nembed_dim\\n,\\nnum_heads\\n)\\n>>>\\nattn_output\\n,\\nattn_output_weights\\n=\\nmultihead_attn\\n(\\nquery\\n,\\nkey\\n,\\nvalue\\n)\\nforward\\n(\\nquery\\n,\\nkey\\n,\\nvalue\\n,\\nkey_padding_mask\\n=\\nNone\\n,\\nneed_weights\\n=\\nTrue\\n,\\nattn_mask\\n='),\n",
       " Document(metadata={}, page_content='=\\nNone\\n,\\nneed_weights\\n=\\nTrue\\n,\\nattn_mask\\n=\\nNone\\n,\\naverage_attn_weights\\n=\\nTrue\\n,\\nis_causal\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nCompute attention outputs using query, key, and value embeddings.\\nSupports optional parameters for padding, masks and attention weights.\\nParameters\\nquery\\n(\\nTensor\\n) – Query embeddings of shape\\n(\\nL\\n,\\nE\\nq\\n)\\n(L, E_q)\\n(\\nL\\n,\\nE\\nq\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nL\\n,\\nN\\n,\\nE\\nq\\n)\\n(L, N, E_q)\\n(\\nL\\n,\\nN\\n,\\nE\\nq\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nL\\n,\\nE\\nq\\n)\\n(N, L, E_q)\\n(\\nN\\n,\\nL\\n,\\nE\\nq\\n\\u200b\\n)\\nwhen'),\n",
       " Document(metadata={}, page_content='N\\n,\\nL\\n,\\nE\\nq\\n)\\n(N, L, E_q)\\n(\\nN\\n,\\nL\\n,\\nE\\nq\\n\\u200b\\n)\\nwhen\\nbatch_first=True\\n, where\\nL\\nL\\nL\\nis the target sequence length,\\nN\\nN\\nN\\nis the batch size, and\\nE\\nq\\nE_q\\nE\\nq\\n\\u200b\\nis the query embedding dimension\\nembed_dim\\n.\\nQueries are compared against key-value pairs to produce the output.\\nSee “Attention Is All You Need” for more details.\\nkey\\n(\\nTensor\\n) – Key embeddings of shape\\n(\\nS\\n,\\nE\\nk\\n)\\n(S, E_k)\\n(\\nS\\n,\\nE\\nk\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nS\\n,\\nN\\n,\\nE\\nk\\n)\\n(S, N, E_k)\\n(\\nS\\n,\\nN\\n,\\nE\\nk\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nS\\n,\\nE\\nk'),\n",
       " Document(metadata={}, page_content=',\\nE\\nk\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nS\\n,\\nE\\nk\\n)\\n(N, S, E_k)\\n(\\nN\\n,\\nS\\n,\\nE\\nk\\n\\u200b\\n)\\nwhen\\nbatch_first=True\\n, where\\nS\\nS\\nS\\nis the source sequence length,\\nN\\nN\\nN\\nis the batch size, and\\nE\\nk\\nE_k\\nE\\nk\\n\\u200b\\nis the key embedding dimension\\nkdim\\n.\\nSee “Attention Is All You Need” for more details.\\nvalue\\n(\\nTensor\\n) – Value embeddings of shape\\n(\\nS\\n,\\nE\\nv\\n)\\n(S, E_v)\\n(\\nS\\n,\\nE\\nv\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nS\\n,\\nN\\n,\\nE\\nv\\n)\\n(S, N, E_v)\\n(\\nS\\n,\\nN\\n,\\nE\\nv\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nS\\n,\\nE\\nv\\n)\\n(N, S, E_v)\\n(\\nN\\n,\\nS\\n,\\nE\\nv\\n\\u200b\\n)'),\n",
       " Document(metadata={}, page_content='or\\n(\\nN\\n,\\nS\\n,\\nE\\nv\\n)\\n(N, S, E_v)\\n(\\nN\\n,\\nS\\n,\\nE\\nv\\n\\u200b\\n)\\nwhen\\nbatch_first=True\\n, where\\nS\\nS\\nS\\nis the source\\nsequence length,\\nN\\nN\\nN\\nis the batch size, and\\nE\\nv\\nE_v\\nE\\nv\\n\\u200b\\nis the value embedding dimension\\nvdim\\n.\\nSee “Attention Is All You Need” for more details.\\nkey_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – If specified, a mask of shape\\n(\\nN\\n,\\nS\\n)\\n(N, S)\\n(\\nN\\n,\\nS\\n)\\nindicating which elements within\\nkey\\nto ignore for the purpose of attention (i.e. treat as “padding”). For unbatched\\nquery\\n, shape should be\\n(\\nS\\n)\\n(S)'),\n",
       " Document(metadata={}, page_content='query\\n, shape should be\\n(\\nS\\n)\\n(S)\\n(\\nS\\n)\\n.\\nBinary and float masks are supported.\\nFor a binary mask, a\\nTrue\\nvalue indicates that the corresponding\\nkey\\nvalue will be ignored for\\nthe purpose of attention. For a float mask, it will be directly added to the corresponding\\nkey\\nvalue.\\nneed_weights\\n(\\nbool\\n) – If specified, returns\\nattn_output_weights\\nin addition to\\nattn_outputs\\n.\\nSet\\nneed_weights=False\\nto use the optimized\\nscaled_dot_product_attention\\nand achieve the best performance for MHA.\\nDefault:'),\n",
       " Document(metadata={}, page_content='Default:\\nTrue\\n.\\nattn_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\\n(\\nL\\n,\\nS\\n)\\n(L, S)\\n(\\nL\\n,\\nS\\n)\\nor\\n(\\nN\\n⋅\\nnum_heads\\n,\\nL\\n,\\nS\\n)\\n(N\\\\cdot\\\\text{num\\\\_heads}, L, S)\\n(\\nN\\n⋅\\nnum_heads\\n,\\nL\\n,\\nS\\n)\\n, where\\nN\\nN\\nN\\nis the batch size,\\nL\\nL\\nL\\nis the target sequence length, and\\nS\\nS\\nS\\nis the source sequence length. A 2D mask will be\\nbroadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.'),\n",
       " Document(metadata={}, page_content='Binary and float masks are supported. For a binary mask, a\\nTrue\\nvalue indicates that the\\ncorresponding position is not allowed to attend. For a float mask, the mask values will be added to\\nthe attention weight.\\nIf both attn_mask and key_padding_mask are supplied, their types should match.\\naverage_attn_weights\\n(\\nbool\\n) – If true, indicates that the returned\\nattn_weights\\nshould be averaged across\\nheads. Otherwise,\\nattn_weights\\nare provided separately per head. Note that this flag only has an'),\n",
       " Document(metadata={}, page_content='effect when\\nneed_weights=True\\n. Default:\\nTrue\\n(i.e. average weights across heads)\\nis_causal\\n(\\nbool\\n) – If specified, applies a causal mask as attention mask.\\nDefault:\\nFalse\\n.\\nWarning:\\nis_causal\\nprovides a hint that\\nattn_mask\\nis the\\ncausal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.\\nReturn type\\nTuple\\n[\\nTensor\\n,\\nOptional\\n[\\nTensor\\n]]\\nOutputs:\\nattn_output\\n- Attention outputs of shape\\n(\\nL\\n,\\nE\\n)\\n(L, E)\\n(\\nL\\n,\\nE\\n)'),\n",
       " Document(metadata={}, page_content='(\\nL\\n,\\nE\\n)\\n(L, E)\\n(\\nL\\n,\\nE\\n)\\nwhen input is unbatched,\\n(\\nL\\n,\\nN\\n,\\nE\\n)\\n(L, N, E)\\n(\\nL\\n,\\nN\\n,\\nE\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nL\\n,\\nE\\n)\\n(N, L, E)\\n(\\nN\\n,\\nL\\n,\\nE\\n)\\nwhen\\nbatch_first=True\\n,\\nwhere\\nL\\nL\\nL\\nis the target sequence length,\\nN\\nN\\nN\\nis the batch size, and\\nE\\nE\\nE\\nis the\\nembedding dimension\\nembed_dim\\n.\\nattn_output_weights\\n- Only returned when\\nneed_weights=True\\n. If\\naverage_attn_weights=True\\n,\\nreturns attention weights averaged across heads of shape\\n(\\nL\\n,\\nS\\n)\\n(L, S)\\n(\\nL\\n,\\nS\\n)\\nwhen input is unbatched or\\n('),\n",
       " Document(metadata={}, page_content='S\\n)\\n(L, S)\\n(\\nL\\n,\\nS\\n)\\nwhen input is unbatched or\\n(\\nN\\n,\\nL\\n,\\nS\\n)\\n(N, L, S)\\n(\\nN\\n,\\nL\\n,\\nS\\n)\\n, where\\nN\\nN\\nN\\nis the batch size,\\nL\\nL\\nL\\nis the target sequence length, and\\nS\\nS\\nS\\nis the source sequence length. If\\naverage_attn_weights=False\\n, returns attention weights per\\nhead of shape\\n(\\nnum_heads\\n,\\nL\\n,\\nS\\n)\\n(\\\\text{num\\\\_heads}, L, S)\\n(\\nnum_heads\\n,\\nL\\n,\\nS\\n)\\nwhen input is unbatched or\\n(\\nN\\n,\\nnum_heads\\n,\\nL\\n,\\nS\\n)\\n(N, \\\\text{num\\\\_heads}, L, S)\\n(\\nN\\n,\\nnum_heads\\n,\\nL\\n,\\nS\\n)\\n.\\nNote\\nbatch_first'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nnum_heads\\n,\\nL\\n,\\nS\\n)\\n.\\nNote\\nbatch_first\\nargument is ignored for unbatched inputs.\\nmerge_masks\\n(\\nattn_mask\\n,\\nkey_padding_mask\\n,\\nquery\\n)\\n[source]\\n[source]\\n¶\\nDetermine mask type and combine masks if necessary.\\nIf only one mask is provided, that mask\\nand the corresponding mask type will be returned. If both masks are provided, they will be both\\nexpanded to shape\\n(batch_size,\\nnum_heads,\\nseq_len,\\nseq_len)\\n, combined with logical\\nor\\nand mask type 2 will be returned'),\n",
       " Document(metadata={}, page_content='or\\nand mask type 2 will be returned\\n:param attn_mask: attention mask of shape\\n(seq_len,\\nseq_len)\\n, mask type 0\\n:param key_padding_mask: padding mask of shape\\n(batch_size,\\nseq_len)\\n, mask type 1\\n:param query: query embeddings of shape\\n(batch_size,\\nseq_len,\\nembed_dim)\\nReturns\\nmerged mask\\nmask_type: merged mask type (0, 1, or 2)\\nReturn type\\nmerged_mask\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='PReLU\\n¶\\nclass\\ntorch.nn.\\nPReLU\\n(\\nnum_parameters\\n=\\n1\\n,\\ninit\\n=\\n0.25\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies the element-wise PReLU function.\\nPReLU\\n(\\nx\\n)\\n=\\nmax\\n\\u2061\\n(\\n0\\n,\\nx\\n)\\n+\\na\\n∗\\nmin\\n\\u2061\\n(\\n0\\n,\\nx\\n)\\n\\\\text{PReLU}(x) = \\\\max(0,x) + a * \\\\min(0,x)\\nPReLU\\n(\\nx\\n)\\n=\\nmax\\n(\\n0\\n,\\nx\\n)\\n+\\na\\n∗\\nmin\\n(\\n0\\n,\\nx\\n)\\nor\\nPReLU\\n(\\nx\\n)\\n=\\n{\\nx\\n,\\nif\\nx\\n≥\\n0\\na\\nx\\n,\\notherwise\\n\\\\text{PReLU}(x) =\\n\\\\begin{cases}\\nx, & \\\\text{ if } x \\\\ge 0 \\\\\\\\\\nax, & \\\\text{ otherwise }\\n\\\\end{cases}\\nPReLU\\n(\\nx\\n)\\n=\\n{\\nx\\n,\\na\\nx\\n,\\n\\u200b\\nif\\nx\\n≥\\n0\\notherwise\\n\\u200b'),\n",
       " Document(metadata={}, page_content='PReLU\\n(\\nx\\n)\\n=\\n{\\nx\\n,\\na\\nx\\n,\\n\\u200b\\nif\\nx\\n≥\\n0\\notherwise\\n\\u200b\\nHere\\na\\na\\na\\nis a learnable parameter. When called without arguments,\\nnn.PReLU()\\nuses a single\\nparameter\\na\\na\\na\\nacross all input channels. If called with\\nnn.PReLU(nChannels)\\n,\\na separate\\na\\na\\na\\nis used for each input channel.\\nNote\\nweight decay should not be used when learning\\na\\na\\na\\nfor good performance.\\nNote\\nChannel dim is the 2nd dim of input. When input has dims < 2, then there is\\nno channel dim and the number of channels = 1.\\nParameters'),\n",
       " Document(metadata={}, page_content='Parameters\\nnum_parameters\\n(\\nint\\n) – number of\\na\\na\\na\\nto learn.\\nAlthough it takes an int as input, there is only two values are legitimate:\\n1, or the number of channels at input. Default: 1\\ninit\\n(\\nfloat\\n) – the initial value of\\na\\na\\na\\n. Default: 0.25\\nShape:\\nInput:\\n(\\n∗\\n)\\n( *)\\n(\\n∗\\n)\\nwhere\\n*\\nmeans, any number of additional\\ndimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nVariables\\nweight\\n(\\nTensor\\n) – the learnable weights of shape (\\nnum_parameters\\n).\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nPReLU\\n()\\n>>>'),\n",
       " Document(metadata={}, page_content=').\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nPReLU\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ReLU\\n¶\\nclass\\ntorch.nn.\\nReLU\\n(\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the rectified linear unit function element-wise.\\nReLU\\n(\\nx\\n)\\n=\\n(\\nx\\n)\\n+\\n=\\nmax\\n\\u2061\\n(\\n0\\n,\\nx\\n)\\n\\\\text{ReLU}(x) = (x)^+ = \\\\max(0, x)\\nReLU\\n(\\nx\\n)\\n=\\n(\\nx\\n)\\n+\\n=\\nmax\\n(\\n0\\n,\\nx\\n)\\nParameters\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nReLU\\n()\\n>>>\\ninput\\n=\\ntorch'),\n",
       " Document(metadata={}, page_content='Examples:\\n>>>\\nm\\n=\\nnn\\n.\\nReLU\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nAn\\nimplementation\\nof\\nCReLU\\n-\\nhttps\\n:\\n//\\narxiv\\n.\\norg\\n/\\nabs\\n/\\n1603.05201\\n>>>\\nm\\n=\\nnn\\n.\\nReLU\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n.\\nunsqueeze\\n(\\n0\\n)\\n>>>\\noutput\\n=\\ntorch\\n.\\ncat\\n((\\nm\\n(\\ninput\\n),\\nm\\n(\\n-\\ninput\\n)))\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='ReLU6\\n¶\\nclass\\ntorch.nn.\\nReLU6\\n(\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the ReLU6 function element-wise.\\nReLU6\\n(\\nx\\n)\\n=\\nmin\\n\\u2061\\n(\\nmax\\n\\u2061\\n(\\n0\\n,\\nx\\n)\\n,\\n6\\n)\\n\\\\text{ReLU6}(x) = \\\\min(\\\\max(0,x), 6)\\nReLU6\\n(\\nx\\n)\\n=\\nmin\\n(\\nmax\\n(\\n0\\n,\\nx\\n)\\n,\\n6\\n)\\nParameters\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nReLU6\\n()\\n>>>\\ninput\\n=\\ntorch\\n.'),\n",
       " Document(metadata={}, page_content='>>>\\nm\\n=\\nnn\\n.\\nReLU6\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='RReLU\\n¶\\nclass\\ntorch.nn.\\nRReLU\\n(\\nlower\\n=\\n0.125\\n,\\nupper\\n=\\n0.3333333333333333\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the randomized leaky rectified linear unit function, element-wise.\\nMethod described in the paper:\\nEmpirical Evaluation of Rectified Activations in Convolutional Network\\n.\\nThe function is defined as:\\nRReLU\\n(\\nx\\n)\\n=\\n{\\nx\\nif\\nx\\n≥\\n0\\na\\nx\\notherwise\\n\\\\text{RReLU}(x) =\\n\\\\begin{cases}\\n    x & \\\\text{if } x \\\\geq 0 \\\\\\\\\\n    ax & \\\\text{ otherwise }\\n\\\\end{cases}\\nRReLU\\n(\\nx\\n)\\n=\\n{\\nx\\na\\nx\\n\\u200b\\nif\\nx\\n≥\\n0'),\n",
       " Document(metadata={}, page_content='\\\\end{cases}\\nRReLU\\n(\\nx\\n)\\n=\\n{\\nx\\na\\nx\\n\\u200b\\nif\\nx\\n≥\\n0\\notherwise\\n\\u200b\\nwhere\\na\\na\\na\\nis randomly sampled from uniform distribution\\nU\\n(\\nlower\\n,\\nupper\\n)\\n\\\\mathcal{U}(\\\\text{lower}, \\\\text{upper})\\nU\\n(\\nlower\\n,\\nupper\\n)\\nduring training while during\\nevaluation\\na\\na\\na\\nis fixed with\\na\\n=\\nlower\\n+\\nupper\\n2\\na = \\\\frac{\\\\text{lower} + \\\\text{upper}}{2}\\na\\n=\\n2\\nlower\\n+\\nupper\\n\\u200b\\n.\\nParameters\\nlower\\n(\\nfloat\\n) – lower bound of the uniform distribution. Default:\\n1\\n8\\n\\\\frac{1}{8}\\n8\\n1\\n\\u200b\\nupper\\n(\\nfloat'),\n",
       " Document(metadata={}, page_content='1\\n8\\n\\\\frac{1}{8}\\n8\\n1\\n\\u200b\\nupper\\n(\\nfloat\\n) – upper bound of the uniform distribution. Default:\\n1\\n3\\n\\\\frac{1}{3}\\n3\\n1\\n\\u200b\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nRReLU\\n(\\n0.1\\n,\\n0.3\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a'),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='SELU\\n¶\\nclass\\ntorch.nn.\\nSELU\\n(\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the SELU function element-wise.\\nSELU\\n(\\nx\\n)\\n=\\nscale\\n∗\\n(\\nmax\\n\\u2061\\n(\\n0\\n,\\nx\\n)\\n+\\nmin\\n\\u2061\\n(\\n0\\n,\\nα\\n∗\\n(\\nexp\\n\\u2061\\n(\\nx\\n)\\n−\\n1\\n)\\n)\\n)\\n\\\\text{SELU}(x) = \\\\text{scale} * (\\\\max(0,x) + \\\\min(0, \\\\alpha * (\\\\exp(x) - 1)))\\nSELU\\n(\\nx\\n)\\n=\\nscale\\n∗\\n(\\nmax\\n(\\n0\\n,\\nx\\n)\\n+\\nmin\\n(\\n0\\n,\\nα\\n∗\\n(\\nexp\\n(\\nx\\n)\\n−\\n1\\n)))\\nwith\\nα\\n=\\n1.6732632423543772848170429916717\\n\\\\alpha = 1.6732632423543772848170429916717\\nα\\n=\\n1.6732632423543772848170429916717\\nand\\nscale\\n='),\n",
       " Document(metadata={}, page_content=\"α\\n=\\n1.6732632423543772848170429916717\\nand\\nscale\\n=\\n1.0507009873554804934193349852946\\n\\\\text{scale} = 1.0507009873554804934193349852946\\nscale\\n=\\n1.0507009873554804934193349852946\\n.\\nWarning\\nWhen using\\nkaiming_normal\\nor\\nkaiming_normal_\\nfor initialisation,\\nnonlinearity='linear'\\nshould be used instead of\\nnonlinearity='selu'\\nin order to get\\nSelf-Normalizing Neural Networks\\n.\\nSee\\ntorch.nn.init.calculate_gain()\\nfor more information.\\nMore details can be found in the paper\\nSelf-Normalizing Neural Networks\\n.\"),\n",
       " Document(metadata={}, page_content='Self-Normalizing Neural Networks\\n.\\nParameters\\ninplace\\n(\\nbool\\n,\\noptional\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSELU\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='CELU\\n¶\\nclass\\ntorch.nn.\\nCELU\\n(\\nalpha\\n=\\n1.0\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the CELU function element-wise.\\nCELU\\n(\\nx\\n)\\n=\\nmax\\n\\u2061\\n(\\n0\\n,\\nx\\n)\\n+\\nmin\\n\\u2061\\n(\\n0\\n,\\nα\\n∗\\n(\\nexp\\n\\u2061\\n(\\nx\\n/\\nα\\n)\\n−\\n1\\n)\\n)\\n\\\\text{CELU}(x) = \\\\max(0,x) + \\\\min(0, \\\\alpha * (\\\\exp(x/\\\\alpha) - 1))\\nCELU\\n(\\nx\\n)\\n=\\nmax\\n(\\n0\\n,\\nx\\n)\\n+\\nmin\\n(\\n0\\n,\\nα\\n∗\\n(\\nexp\\n(\\nx\\n/\\nα\\n)\\n−\\n1\\n))\\nMore details can be found in the paper\\nContinuously Differentiable Exponential Linear Units\\n.\\nParameters\\nalpha\\n(\\nfloat\\n) – the\\nα\\n\\\\alpha\\nα'),\n",
       " Document(metadata={}, page_content='.\\nParameters\\nalpha\\n(\\nfloat\\n) – the\\nα\\n\\\\alpha\\nα\\nvalue for the CELU formulation. Default: 1.0\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nCELU\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"GELU\\n¶\\nclass\\ntorch.nn.\\nGELU\\n(\\napproximate\\n=\\n'none'\\n)\\n[source]\\n[source]\\n¶\\nApplies the Gaussian Error Linear Units function.\\nGELU\\n(\\nx\\n)\\n=\\nx\\n∗\\nΦ\\n(\\nx\\n)\\n\\\\text{GELU}(x) = x * \\\\Phi(x)\\nGELU\\n(\\nx\\n)\\n=\\nx\\n∗\\nΦ\\n(\\nx\\n)\\nwhere\\nΦ\\n(\\nx\\n)\\n\\\\Phi(x)\\nΦ\\n(\\nx\\n)\\nis the Cumulative Distribution Function for Gaussian Distribution.\\nWhen the approximate argument is ‘tanh’, Gelu is estimated with:\\nGELU\\n(\\nx\\n)\\n=\\n0.5\\n∗\\nx\\n∗\\n(\\n1\\n+\\nTanh\\n(\\n2\\n/\\nπ\\n∗\\n(\\nx\\n+\\n0.044715\\n∗\\nx\\n3\\n)\\n)\\n)\"),\n",
       " Document(metadata={}, page_content=\"∗\\n(\\n1\\n+\\nTanh\\n(\\n2\\n/\\nπ\\n∗\\n(\\nx\\n+\\n0.044715\\n∗\\nx\\n3\\n)\\n)\\n)\\n\\\\text{GELU}(x) = 0.5 * x * (1 + \\\\text{Tanh}(\\\\sqrt{2 / \\\\pi} * (x + 0.044715 * x^3)))\\nGELU\\n(\\nx\\n)\\n=\\n0.5\\n∗\\nx\\n∗\\n(\\n1\\n+\\nTanh\\n(\\n2/\\nπ\\n\\u200b\\n∗\\n(\\nx\\n+\\n0.044715\\n∗\\nx\\n3\\n)))\\nParameters\\napproximate\\n(\\nstr\\n,\\noptional\\n) – the gelu approximation algorithm to use:\\n'none'\\n|\\n'tanh'\\n. Default:\\n'none'\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nGELU\\n()\\n>>>\\ninput\\n=\\ntorch\"),\n",
       " Document(metadata={}, page_content='Examples:\\n>>>\\nm\\n=\\nnn\\n.\\nGELU\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Sigmoid\\n¶\\nclass\\ntorch.nn.\\nSigmoid\\n(\\n*\\nargs\\n,\\n**\\nkwargs\\n)\\n[source]\\n[source]\\n¶\\nApplies the Sigmoid function element-wise.\\nSigmoid\\n(\\nx\\n)\\n=\\nσ\\n(\\nx\\n)\\n=\\n1\\n1\\n+\\nexp\\n\\u2061\\n(\\n−\\nx\\n)\\n\\\\text{Sigmoid}(x) = \\\\sigma(x) = \\\\frac{1}{1 + \\\\exp(-x)}\\nSigmoid\\n(\\nx\\n)\\n=\\nσ\\n(\\nx\\n)\\n=\\n1\\n+\\nexp\\n(\\n−\\nx\\n)\\n1\\n\\u200b\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSigmoid\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='SiLU\\n¶\\nclass\\ntorch.nn.\\nSiLU\\n(\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the Sigmoid Linear Unit (SiLU) function, element-wise.\\nThe SiLU function is also known as the swish function.\\nsilu\\n(\\nx\\n)\\n=\\nx\\n∗\\nσ\\n(\\nx\\n)\\n,\\nwhere\\nσ\\n(\\nx\\n)\\nis\\xa0the\\xa0logistic\\xa0sigmoid.\\n\\\\text{silu}(x) = x * \\\\sigma(x), \\\\text{where } \\\\sigma(x) \\\\text{ is the logistic sigmoid.}\\nsilu\\n(\\nx\\n)\\n=\\nx\\n∗\\nσ\\n(\\nx\\n)\\n,\\nwhere\\nσ\\n(\\nx\\n)\\nis\\xa0the\\xa0logistic\\xa0sigmoid.\\nNote\\nSee\\nGaussian Error Linear Units (GELUs)'),\n",
       " Document(metadata={}, page_content='Note\\nSee\\nGaussian Error Linear Units (GELUs)\\nwhere the SiLU (Sigmoid Linear Unit) was originally coined, and see\\nSigmoid-Weighted Linear Units for Neural Network Function Approximation\\nin Reinforcement Learning\\nand\\nSwish:\\na Self-Gated Activation Function\\nwhere the SiLU was experimented with later.\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSiLU\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput'),\n",
       " Document(metadata={}, page_content='()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Mish\\n¶\\nclass\\ntorch.nn.\\nMish\\n(\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies the Mish function, element-wise.\\nMish: A Self Regularized Non-Monotonic Neural Activation Function.\\nMish\\n(\\nx\\n)\\n=\\nx\\n∗\\nTanh\\n(\\nSoftplus\\n(\\nx\\n)\\n)\\n\\\\text{Mish}(x) = x * \\\\text{Tanh}(\\\\text{Softplus}(x))\\nMish\\n(\\nx\\n)\\n=\\nx\\n∗\\nTanh\\n(\\nSoftplus\\n(\\nx\\n))\\nNote\\nSee\\nMish: A Self Regularized Non-Monotonic Neural Activation Function\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)'),\n",
       " Document(metadata={}, page_content='Output:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nMish\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Softplus\\n¶\\nclass\\ntorch.nn.\\nSoftplus\\n(\\nbeta\\n=\\n1.0\\n,\\nthreshold\\n=\\n20.0\\n)\\n[source]\\n[source]\\n¶\\nApplies the Softplus function element-wise.\\nSoftplus\\n(\\nx\\n)\\n=\\n1\\nβ\\n∗\\nlog\\n\\u2061\\n(\\n1\\n+\\nexp\\n\\u2061\\n(\\nβ\\n∗\\nx\\n)\\n)\\n\\\\text{Softplus}(x) = \\\\frac{1}{\\\\beta} * \\\\log(1 + \\\\exp(\\\\beta * x))\\nSoftplus\\n(\\nx\\n)\\n=\\nβ\\n1\\n\\u200b\\n∗\\nlo\\ng\\n(\\n1\\n+\\nexp\\n(\\nβ\\n∗\\nx\\n))\\nSoftPlus is a smooth approximation to the ReLU function and can be used\\nto constrain the output of a machine to always be positive.'),\n",
       " Document(metadata={}, page_content='For numerical stability the implementation reverts to the linear function\\nwhen\\ni\\nn\\np\\nu\\nt\\n×\\nβ\\n>\\nt\\nh\\nr\\ne\\ns\\nh\\no\\nl\\nd\\ninput \\\\times \\\\beta > threshold\\nin\\np\\nu\\nt\\n×\\nβ\\n>\\nt\\nh\\nres\\nh\\no\\nl\\nd\\n.\\nParameters\\nbeta\\n(\\nfloat\\n) – the\\nβ\\n\\\\beta\\nβ\\nvalue for the Softplus formulation. Default: 1\\nthreshold\\n(\\nfloat\\n) – values above this revert to a linear function. Default: 20\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.'),\n",
       " Document(metadata={}, page_content=', same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSoftplus\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Softshrink\\n¶\\nclass\\ntorch.nn.\\nSoftshrink\\n(\\nlambd\\n=\\n0.5\\n)\\n[source]\\n[source]\\n¶\\nApplies the soft shrinkage function element-wise.\\nSoftShrinkage\\n(\\nx\\n)\\n=\\n{\\nx\\n−\\nλ\\n,\\nif\\nx\\n>\\nλ\\nx\\n+\\nλ\\n,\\nif\\nx\\n<\\n−\\nλ\\n0\\n,\\notherwise\\n\\\\text{SoftShrinkage}(x) =\\n\\\\begin{cases}\\nx - \\\\lambda, & \\\\text{ if } x > \\\\lambda \\\\\\\\\\nx + \\\\lambda, & \\\\text{ if } x < -\\\\lambda \\\\\\\\\\n0, & \\\\text{ otherwise }\\n\\\\end{cases}\\nSoftShrinkage\\n(\\nx\\n)\\n=\\n⎩\\n⎨\\n⎧\\n\\u200b\\nx\\n−\\nλ\\n,\\nx\\n+\\nλ\\n,\\n0\\n,\\n\\u200b\\nif\\nx\\n>\\nλ\\nif\\nx\\n<\\n−\\nλ\\notherwise\\n\\u200b\\nParameters\\nlambd\\n(\\nfloat\\n) – the\\nλ\\n\\\\lambda\\nλ'),\n",
       " Document(metadata={}, page_content='\\u200b\\nParameters\\nlambd\\n(\\nfloat\\n) – the\\nλ\\n\\\\lambda\\nλ\\n(must be no less than zero) value for the Softshrink formulation. Default: 0.5\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSoftshrink\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Softsign\\n¶\\nclass\\ntorch.nn.\\nSoftsign\\n(\\n*\\nargs\\n,\\n**\\nkwargs\\n)\\n[source]\\n[source]\\n¶\\nApplies the element-wise Softsign function.\\nSoftSign\\n(\\nx\\n)\\n=\\nx\\n1\\n+\\n∣\\nx\\n∣\\n\\\\text{SoftSign}(x) = \\\\frac{x}{ 1 + |x|}\\nSoftSign\\n(\\nx\\n)\\n=\\n1\\n+\\n∣\\nx\\n∣\\nx\\n\\u200b\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSoftsign\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.'),\n",
       " Document(metadata={}, page_content='Previous\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Tanh\\n¶\\nclass\\ntorch.nn.\\nTanh\\n(\\n*\\nargs\\n,\\n**\\nkwargs\\n)\\n[source]\\n[source]\\n¶\\nApplies the Hyperbolic Tangent (Tanh) function element-wise.\\nTanh is defined as:\\nTanh\\n(\\nx\\n)\\n=\\ntanh\\n\\u2061\\n(\\nx\\n)\\n=\\nexp\\n\\u2061\\n(\\nx\\n)\\n−\\nexp\\n\\u2061\\n(\\n−\\nx\\n)\\nexp\\n\\u2061\\n(\\nx\\n)\\n+\\nexp\\n\\u2061\\n(\\n−\\nx\\n)\\n\\\\text{Tanh}(x) = \\\\tanh(x) = \\\\frac{\\\\exp(x) - \\\\exp(-x)} {\\\\exp(x) + \\\\exp(-x)}\\nTanh\\n(\\nx\\n)\\n=\\ntanh\\n(\\nx\\n)\\n=\\nexp\\n(\\nx\\n)\\n+\\nexp\\n(\\n−\\nx\\n)\\nexp\\n(\\nx\\n)\\n−\\nexp\\n(\\n−\\nx\\n)\\n\\u200b\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)'),\n",
       " Document(metadata={}, page_content='Output:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nTanh\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Tanhshrink\\n¶\\nclass\\ntorch.nn.\\nTanhshrink\\n(\\n*\\nargs\\n,\\n**\\nkwargs\\n)\\n[source]\\n[source]\\n¶\\nApplies the element-wise Tanhshrink function.\\nTanhshrink\\n(\\nx\\n)\\n=\\nx\\n−\\ntanh\\n\\u2061\\n(\\nx\\n)\\n\\\\text{Tanhshrink}(x) = x - \\\\tanh(x)\\nTanhshrink\\n(\\nx\\n)\\n=\\nx\\n−\\ntanh\\n(\\nx\\n)\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nTanhshrink\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Threshold\\n¶\\nclass\\ntorch.nn.\\nThreshold\\n(\\nthreshold\\n,\\nvalue\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nThresholds each element of the input Tensor.\\nThreshold is defined as:\\ny\\n=\\n{\\nx\\n,\\nif\\nx\\n>\\nthreshold\\nvalue\\n,\\notherwise\\ny =\\n\\\\begin{cases}\\nx, &\\\\text{ if } x > \\\\text{threshold} \\\\\\\\\\n\\\\text{value}, &\\\\text{ otherwise }\\n\\\\end{cases}\\ny\\n=\\n{\\nx\\n,\\nvalue\\n,\\n\\u200b\\nif\\nx\\n>\\nthreshold\\notherwise\\n\\u200b\\nParameters\\nthreshold\\n(\\nfloat\\n) – The value to threshold at\\nvalue\\n(\\nfloat\\n) – The value to replace with\\ninplace\\n(\\nbool'),\n",
       " Document(metadata={}, page_content=') – The value to replace with\\ninplace\\n(\\nbool\\n) – can optionally do the operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nThreshold\\n(\\n0.1\\n,\\n20\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='GLU\\n¶\\nclass\\ntorch.nn.\\nGLU\\n(\\ndim\\n=\\n-1\\n)\\n[source]\\n[source]\\n¶\\nApplies the gated linear unit function.\\nG\\nL\\nU\\n(\\na\\n,\\nb\\n)\\n=\\na\\n⊗\\nσ\\n(\\nb\\n)\\n{GLU}(a, b)= a \\\\otimes \\\\sigma(b)\\nG\\nLU\\n(\\na\\n,\\nb\\n)\\n=\\na\\n⊗\\nσ\\n(\\nb\\n)\\nwhere\\na\\na\\na\\nis the first half\\nof the input matrices and\\nb\\nb\\nb\\nis the second half.\\nParameters\\ndim\\n(\\nint\\n) – the dimension on which to split the input. Default: -1\\nShape:\\nInput:\\n(\\n∗\\n1\\n,\\nN\\n,\\n∗\\n2\\n)\\n(\\\\ast_1, N, \\\\ast_2)\\n(\\n∗\\n1\\n\\u200b\\n,\\nN\\n,\\n∗\\n2\\n\\u200b\\n)\\nwhere\\n*\\nmeans, any number of additional\\ndimensions\\nOutput:\\n(\\n∗\\n1\\n,\\nM\\n,\\n∗'),\n",
       " Document(metadata={}, page_content='dimensions\\nOutput:\\n(\\n∗\\n1\\n,\\nM\\n,\\n∗\\n2\\n)\\n(\\\\ast_1, M, \\\\ast_2)\\n(\\n∗\\n1\\n\\u200b\\n,\\nM\\n,\\n∗\\n2\\n\\u200b\\n)\\nwhere\\nM\\n=\\nN\\n/\\n2\\nM=N/2\\nM\\n=\\nN\\n/2\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nGLU\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n4\\n,\\n2\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Softmin\\n¶\\nclass\\ntorch.nn.\\nSoftmin\\n(\\ndim\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies the Softmin function to an n-dimensional input Tensor.\\nRescales them so that the elements of the n-dimensional output Tensor\\nlie in the range\\n[0, 1]\\nand sum to 1.\\nSoftmin is defined as:\\nSoftmin\\n(\\nx\\ni\\n)\\n=\\nexp\\n\\u2061\\n(\\n−\\nx\\ni\\n)\\n∑\\nj\\nexp\\n\\u2061\\n(\\n−\\nx\\nj\\n)\\n\\\\text{Softmin}(x_{i}) = \\\\frac{\\\\exp(-x_i)}{\\\\sum_j \\\\exp(-x_j)}\\nSoftmin\\n(\\nx\\ni\\n\\u200b\\n)\\n=\\n∑\\nj\\n\\u200b\\nexp\\n(\\n−\\nx\\nj\\n\\u200b\\n)\\nexp\\n(\\n−\\nx\\ni\\n\\u200b\\n)\\n\\u200b\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\nwhere\\n*'),\n",
       " Document(metadata={}, page_content='−\\nx\\ni\\n\\u200b\\n)\\n\\u200b\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\nwhere\\n*\\nmeans, any number of additional\\ndimensions\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input\\nParameters\\ndim\\n(\\nint\\n) – A dimension along which Softmin will be computed (so every slice\\nalong dim will sum to 1).\\nReturns\\na Tensor of the same dimension and shape as the input, with\\nvalues in the range [0, 1]\\nReturn type\\nNone\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSoftmin\\n(\\ndim\\n=\\n1\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='(\\n2\\n,\\n3\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Softmax\\n¶\\nclass\\ntorch.nn.\\nSoftmax\\n(\\ndim\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies the Softmax function to an n-dimensional input Tensor.\\nRescales them so that the elements of the n-dimensional output Tensor\\nlie in the range [0,1] and sum to 1.\\nSoftmax is defined as:\\nSoftmax\\n(\\nx\\ni\\n)\\n=\\nexp\\n\\u2061\\n(\\nx\\ni\\n)\\n∑\\nj\\nexp\\n\\u2061\\n(\\nx\\nj\\n)\\n\\\\text{Softmax}(x_{i}) = \\\\frac{\\\\exp(x_i)}{\\\\sum_j \\\\exp(x_j)}\\nSoftmax\\n(\\nx\\ni\\n\\u200b\\n)\\n=\\n∑\\nj\\n\\u200b\\nexp\\n(\\nx\\nj\\n\\u200b\\n)\\nexp\\n(\\nx\\ni\\n\\u200b\\n)\\n\\u200b\\nWhen the input Tensor is a sparse tensor then the unspecified'),\n",
       " Document(metadata={}, page_content='values are treated as\\n-inf\\n.\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\nwhere\\n*\\nmeans, any number of additional\\ndimensions\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input\\nReturns\\na Tensor of the same dimension and shape as the input with\\nvalues in the range [0, 1]\\nParameters\\ndim\\n(\\nint\\n) – A dimension along which Softmax will be computed (so every slice\\nalong dim will sum to 1).\\nReturn type\\nNone\\nNote\\nThis module doesn’t work directly with NLLLoss,'),\n",
       " Document(metadata={}, page_content='This module doesn’t work directly with NLLLoss,\\nwhich expects the Log to be computed between the Softmax and itself.\\nUse\\nLogSoftmax\\ninstead (it’s faster and has better numerical properties).\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSoftmax\\n(\\ndim\\n=\\n1\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Softmax2d\\n¶\\nclass\\ntorch.nn.\\nSoftmax2d\\n(\\n*\\nargs\\n,\\n**\\nkwargs\\n)\\n[source]\\n[source]\\n¶\\nApplies SoftMax over features to each spatial location.\\nWhen given an image of\\nChannels\\nx\\nHeight\\nx\\nWidth\\n, it will\\napply\\nSoftmax\\nto each location\\n(\\nC\\nh\\na\\nn\\nn\\ne\\nl\\ns\\n,\\nh\\ni\\n,\\nw\\nj\\n)\\n(Channels, h_i, w_j)\\n(\\nC\\nhann\\ne\\nl\\ns\\n,\\nh\\ni\\n\\u200b\\n,\\nw\\nj\\n\\u200b\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)'),\n",
       " Document(metadata={}, page_content=')\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(same shape as input)\\nReturns\\na Tensor of the same dimension and shape as the input with\\nvalues in the range [0, 1]\\nReturn type\\nNone\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSoftmax2d\\n()\\n>>>\\n# you softmax over the 2nd dimension\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n,\\n12\\n,\\n13\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LogSoftmax\\n¶\\nclass\\ntorch.nn.\\nLogSoftmax\\n(\\ndim\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies the\\nlog\\n\\u2061\\n(\\nSoftmax\\n(\\nx\\n)\\n)\\n\\\\log(\\\\text{Softmax}(x))\\nlo\\ng\\n(\\nSoftmax\\n(\\nx\\n))\\nfunction to an n-dimensional input Tensor.\\nThe LogSoftmax formulation can be simplified as:\\nLogSoftmax\\n(\\nx\\ni\\n)\\n=\\nlog\\n\\u2061\\n(\\nexp\\n\\u2061\\n(\\nx\\ni\\n)\\n∑\\nj\\nexp\\n\\u2061\\n(\\nx\\nj\\n)\\n)\\n\\\\text{LogSoftmax}(x_{i}) = \\\\log\\\\left(\\\\frac{\\\\exp(x_i) }{ \\\\sum_j \\\\exp(x_j)} \\\\right)\\nLogSoftmax\\n(\\nx\\ni\\n\\u200b\\n)\\n=\\nlo\\ng\\n(\\n∑\\nj\\n\\u200b\\nexp\\n(\\nx\\nj\\n\\u200b\\n)\\nexp\\n(\\nx\\ni\\n\\u200b\\n)\\n\\u200b\\n)\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\nwhere\\n*'),\n",
       " Document(metadata={}, page_content='x\\ni\\n\\u200b\\n)\\n\\u200b\\n)\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\nwhere\\n*\\nmeans, any number of additional\\ndimensions\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input\\nParameters\\ndim\\n(\\nint\\n) – A dimension along which LogSoftmax will be computed.\\nReturns\\na Tensor of the same dimension and shape as the input with\\nvalues in the range [-inf, 0)\\nReturn type\\nNone\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nLogSoftmax\\n(\\ndim\\n=\\n1\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.'),\n",
       " Document(metadata={}, page_content='Previous\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AdaptiveLogSoftmaxWithLoss\\n¶\\nclass\\ntorch.nn.\\nAdaptiveLogSoftmaxWithLoss\\n(\\nin_features\\n,\\nn_classes\\n,\\ncutoffs\\n,\\ndiv_value\\n=\\n4.0\\n,\\nhead_bias\\n=\\nFalse\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nEfficient softmax approximation.\\nAs described in\\nEfficient softmax approximation for GPUs by Edouard Grave, Armand Joulin,\\nMoustapha Cissé, David Grangier, and Hervé Jégou\\n.\\nAdaptive softmax is an approximate strategy for training models with large'),\n",
       " Document(metadata={}, page_content='output spaces. It is most effective when the label distribution is highly\\nimbalanced, for example in natural language modelling, where the word\\nfrequency distribution approximately follows the\\nZipf’s law\\n.\\nAdaptive softmax partitions the labels into several clusters, according to\\ntheir frequency. These clusters may contain different number of targets\\neach.\\nAdditionally, clusters containing less frequent labels assign lower\\ndimensional embeddings to those labels, which speeds up the computation.'),\n",
       " Document(metadata={}, page_content='For each minibatch, only clusters for which at least one target is\\npresent are evaluated.\\nThe idea is that the clusters which are accessed frequently\\n(like the first one, containing most frequent labels), should also be cheap\\nto compute – that is, contain a small number of assigned labels.\\nWe highly recommend taking a look at the original paper for more details.\\ncutoffs\\nshould be an ordered Sequence of integers sorted\\nin the increasing order.'),\n",
       " Document(metadata={}, page_content='in the increasing order.\\nIt controls number of clusters and the partitioning of targets into\\nclusters. For example setting\\ncutoffs\\n=\\n[10,\\n100,\\n1000]\\nmeans that first\\n10\\ntargets will be assigned\\nto the ‘head’ of the adaptive softmax, targets\\n11, 12, …, 100\\nwill be\\nassigned to the first cluster, and targets\\n101, 102, …, 1000\\nwill be\\nassigned to the second cluster, while targets\\n1001, 1002, …, n_classes - 1\\nwill be assigned\\nto the last, third cluster.\\ndiv_value'),\n",
       " Document(metadata={}, page_content='to the last, third cluster.\\ndiv_value\\nis used to compute the size of each additional cluster,\\nwhich is given as\\n⌊\\nin_features\\ndiv_value\\ni\\nd\\nx\\n⌋\\n\\\\left\\\\lfloor\\\\frac{\\\\texttt{in\\\\_features}}{\\\\texttt{div\\\\_value}^{idx}}\\\\right\\\\rfloor\\n⌊\\ndiv_value\\ni\\nd\\nx\\nin_features\\n\\u200b\\n⌋\\n,\\nwhere\\ni\\nd\\nx\\nidx\\ni\\nd\\nx\\nis the cluster index (with clusters\\nfor less frequent words having larger indices,\\nand indices starting from\\n1\\n1\\n1\\n).\\nhead_bias\\nif set to True, adds a bias term to the ‘head’ of the'),\n",
       " Document(metadata={}, page_content='adaptive softmax. See paper for details. Set to False in the official\\nimplementation.\\nWarning\\nLabels passed as inputs to this module should be sorted according to\\ntheir frequency. This means that the most frequent label should be\\nrepresented by the index\\n0\\n, and the least frequent\\nlabel should be represented by the index\\nn_classes - 1\\n.\\nNote\\nThis module returns a\\nNamedTuple\\nwith\\noutput\\nand\\nloss\\nfields. See further documentation for details.\\nNote\\nTo compute log-probabilities for all classes, the'),\n",
       " Document(metadata={}, page_content='To compute log-probabilities for all classes, the\\nlog_prob\\nmethod can be used.\\nParameters\\nin_features\\n(\\nint\\n) – Number of features in the input tensor\\nn_classes\\n(\\nint\\n) – Number of classes in the dataset\\ncutoffs\\n(\\nSequence\\n) – Cutoffs used to assign targets to their buckets\\ndiv_value\\n(\\nfloat\\n,\\noptional\\n) – value used as an exponent to compute sizes\\nof the clusters. Default: 4.0\\nhead_bias\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, adds a bias term to the ‘head’ of the\\nadaptive softmax. Default:\\nFalse'),\n",
       " Document(metadata={}, page_content='adaptive softmax. Default:\\nFalse\\nReturns\\noutput\\nis a Tensor of size\\nN\\ncontaining computed target\\nlog probabilities for each example\\nloss\\nis a Scalar representing the computed negative\\nlog likelihood loss\\nReturn type\\nNamedTuple\\nwith\\noutput\\nand\\nloss\\nfields\\nShape:\\ninput:\\n(\\nN\\n,\\nin_features\\n)\\n(N, \\\\texttt{in\\\\_features})\\n(\\nN\\n,\\nin_features\\n)\\nor\\n(\\nin_features\\n)\\n(\\\\texttt{in\\\\_features})\\n(\\nin_features\\n)\\ntarget:\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\n)\\n()\\n(\\n)\\nwhere each value satisfies\\n0\\n<\\n=\\ntarget[i]\\n<\\n=\\nn_classes'),\n",
       " Document(metadata={}, page_content='0\\n<\\n=\\ntarget[i]\\n<\\n=\\nn_classes\\n0 <= \\\\texttt{target[i]} <= \\\\texttt{n\\\\_classes}\\n0\\n<=\\ntarget[i]\\n<=\\nn_classes\\noutput1:\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\n)\\n()\\n(\\n)\\noutput2:\\nScalar\\nlog_prob\\n(\\ninput\\n)\\n[source]\\n[source]\\n¶\\nCompute log probabilities for all\\nn_classes\\n\\\\texttt{n\\\\_classes}\\nn_classes\\n.\\nParameters\\ninput\\n(\\nTensor\\n) – a minibatch of examples\\nReturns\\nlog-probabilities of for each class\\nc\\nc\\nc\\nin range\\n0\\n<\\n=\\nc\\n<\\n=\\nn_classes\\n0 <= c <= \\\\texttt{n\\\\_classes}\\n0\\n<=\\nc\\n<=\\nn_classes\\n, where\\nn_classes\\n\\\\texttt{n\\\\_classes}'),\n",
       " Document(metadata={}, page_content='n_classes\\n, where\\nn_classes\\n\\\\texttt{n\\\\_classes}\\nn_classes\\nis a\\nparameter passed to\\nAdaptiveLogSoftmaxWithLoss\\nconstructor.\\nReturn type\\nTensor\\nShape:\\nInput:\\n(\\nN\\n,\\nin_features\\n)\\n(N, \\\\texttt{in\\\\_features})\\n(\\nN\\n,\\nin_features\\n)\\nOutput:\\n(\\nN\\n,\\nn_classes\\n)\\n(N, \\\\texttt{n\\\\_classes})\\n(\\nN\\n,\\nn_classes\\n)\\npredict\\n(\\ninput\\n)\\n[source]\\n[source]\\n¶\\nReturn the class with the highest probability for each example in the input minibatch.\\nThis is equivalent to\\nself.log_prob(input).argmax(dim=1)'),\n",
       " Document(metadata={}, page_content='self.log_prob(input).argmax(dim=1)\\n, but is more efficient in some cases.\\nParameters\\ninput\\n(\\nTensor\\n) – a minibatch of examples\\nReturns\\na class with the highest probability for each example\\nReturn type\\noutput (\\nTensor\\n)\\nShape:\\nInput:\\n(\\nN\\n,\\nin_features\\n)\\n(N, \\\\texttt{in\\\\_features})\\n(\\nN\\n,\\nin_features\\n)\\nOutput:\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='BatchNorm1d\\n¶\\nclass\\ntorch.nn.\\nBatchNorm1d\\n(\\nnum_features\\n,\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Batch Normalization over a 2D or 3D input.\\nMethod described in the paper\\nBatch Normalization: Accelerating Deep Network Training by Reducing\\nInternal Covariate Shift\\n.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ\\ny = \\\\frac{x - \\\\mathrm{E}[x]}{\\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\ny\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ'),\n",
       " Document(metadata={}, page_content='y\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ\\nThe mean and standard-deviation are calculated per-dimension over\\nthe mini-batches and\\nγ\\n\\\\gamma\\nγ\\nand\\nβ\\n\\\\beta\\nβ\\nare learnable parameter vectors\\nof size\\nC\\n(where\\nC\\nis the number of features or channels of the input). By default, the\\nelements of\\nγ\\n\\\\gamma\\nγ\\nare set to 1 and the elements of\\nβ\\n\\\\beta\\nβ\\nare set to 0.\\nAt train time in the forward pass, the variance is calculated via the biased estimator,\\nequivalent to\\ntorch.var(input,\\nunbiased=False)'),\n",
       " Document(metadata={}, page_content='equivalent to\\ntorch.var(input,\\nunbiased=False)\\n. However, the value stored in the\\nmoving average of the variance is calculated via the unbiased  estimator, equivalent to\\ntorch.var(input,\\nunbiased=True)\\n.\\nAlso by default, during training this layer keeps running estimates of its\\ncomputed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default\\nmomentum\\nof 0.1.\\nIf\\ntrack_running_stats\\nis set to\\nFalse\\n, this layer then does not'),\n",
       " Document(metadata={}, page_content='is set to\\nFalse\\n, this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.\\nNote\\nThis\\nmomentum\\nargument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here is\\nx\\n^\\nnew\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_t\\nx\\n^\\nnew\\n\\u200b\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum'),\n",
       " Document(metadata={}, page_content='x\\n^\\nnew\\n\\u200b\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\u200b\\n,\\nwhere\\nx\\n^\\n\\\\hat{x}\\nx\\n^\\nis the estimated statistic and\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis the\\nnew observed value.\\nBecause the Batch Normalization is done over the\\nC\\ndimension, computing statistics\\non\\n(N, L)\\nslices, it’s common terminology to call this Temporal Batch Normalization.\\nParameters\\nnum_features\\n(\\nint\\n) – number of features or channels\\nC\\nC\\nC\\nof the input\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability.\\nDefault: 1e-5\\nmomentum'),\n",
       " Document(metadata={}, page_content='Default: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var\\ncomputation. Can be set to\\nNone\\nfor cumulative moving average\\n(i.e. simple average). Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters. Default:\\nTrue\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,'),\n",
       " Document(metadata={}, page_content='False\\n,\\nthis module does not track such statistics, and initializes statistics\\nbuffers\\nrunning_mean\\nand\\nrunning_var\\nas\\nNone\\n.\\nWhen these buffers are\\nNone\\n, this module always uses batch statistics.\\nin both training and eval modes. Default:\\nTrue\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n)\\n(N, C)\\n(\\nN\\n,\\nC\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n, where\\nN\\nN\\nN\\nis the batch size,\\nC\\nC\\nC\\nis the number of features or channels, and\\nL\\nL\\nL\\nis the sequence length\\nOutput:\\n(\\nN\\n,\\nC\\n)\\n(N, C)\\n(\\nN\\n,\\nC\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)'),\n",
       " Document(metadata={}, page_content=',\\nC\\n)\\n(N, C)\\n(\\nN\\n,\\nC\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(same shape as input)\\nExamples:\\n>>>\\n# With Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nBatchNorm1d\\n(\\n100\\n)\\n>>>\\n# Without Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nBatchNorm1d\\n(\\n100\\n,\\naffine\\n=\\nFalse\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n100\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='BatchNorm2d\\n¶\\nclass\\ntorch.nn.\\nBatchNorm2d\\n(\\nnum_features\\n,\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Batch Normalization over a 4D input.\\n4D is a mini-batch of 2D inputs\\nwith additional channel dimension. Method described in the paper\\nBatch Normalization: Accelerating Deep Network Training by Reducing\\nInternal Covariate Shift\\n.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ'),\n",
       " Document(metadata={}, page_content='.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ\\ny = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\ny\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ\\nThe mean and standard-deviation are calculated per-dimension over\\nthe mini-batches and\\nγ\\n\\\\gamma\\nγ\\nand\\nβ\\n\\\\beta\\nβ\\nare learnable parameter vectors\\nof size\\nC\\n(where\\nC\\nis the input size). By default, the elements of\\nγ\\n\\\\gamma\\nγ\\nare set\\nto 1 and the elements of\\nβ\\n\\\\beta\\nβ\\nare set to 0. At train time in the forward pass, the'),\n",
       " Document(metadata={}, page_content='standard-deviation is calculated via the biased estimator, equivalent to\\ntorch.var(input,\\nunbiased=False)\\n. However, the value stored in the moving average of the\\nstandard-deviation is calculated via the unbiased  estimator, equivalent to\\ntorch.var(input,\\nunbiased=True)\\n.\\nAlso by default, during training this layer keeps running estimates of its\\ncomputed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default\\nmomentum\\nof 0.1.\\nIf'),\n",
       " Document(metadata={}, page_content='momentum\\nof 0.1.\\nIf\\ntrack_running_stats\\nis set to\\nFalse\\n, this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.\\nNote\\nThis\\nmomentum\\nargument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here is\\nx\\n^\\nnew\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_t\\nx\\n^'),\n",
       " Document(metadata={}, page_content='x\\n^\\nnew\\n\\u200b\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\u200b\\n,\\nwhere\\nx\\n^\\n\\\\hat{x}\\nx\\n^\\nis the estimated statistic and\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis the\\nnew observed value.\\nBecause the Batch Normalization is done over the\\nC\\ndimension, computing statistics\\non\\n(N, H, W)\\nslices, it’s common terminology to call this Spatial Batch Normalization.\\nParameters\\nnum_features\\n(\\nint\\n) –\\nC\\nC\\nC\\nfrom an expected input of size\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\neps\\n(\\nfloat'),\n",
       " Document(metadata={}, page_content=',\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability.\\nDefault: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var\\ncomputation. Can be set to\\nNone\\nfor cumulative moving average\\n(i.e. simple average). Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters. Default:\\nTrue\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue'),\n",
       " Document(metadata={}, page_content='(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics, and initializes statistics\\nbuffers\\nrunning_mean\\nand\\nrunning_var\\nas\\nNone\\n.\\nWhen these buffers are\\nNone\\n, this module always uses batch statistics.\\nin both training and eval modes. Default:\\nTrue\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(same shape as input)\\nExamples:\\n>>>\\n# With Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nBatchNorm2d\\n(\\n100\\n)\\n>>>\\n# Without Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nBatchNorm2d\\n(\\n100\\n,\\naffine\\n=\\nFalse\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n100\\n,\\n35\\n,\\n45\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='BatchNorm3d\\n¶\\nclass\\ntorch.nn.\\nBatchNorm3d\\n(\\nnum_features\\n,\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Batch Normalization over a 5D input.\\n5D is a mini-batch of 3D inputs with additional channel dimension as described in the paper\\nBatch Normalization: Accelerating Deep Network Training by Reducing\\nInternal Covariate Shift\\n.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ'),\n",
       " Document(metadata={}, page_content='.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ\\ny = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\ny\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ\\nThe mean and standard-deviation are calculated per-dimension over\\nthe mini-batches and\\nγ\\n\\\\gamma\\nγ\\nand\\nβ\\n\\\\beta\\nβ\\nare learnable parameter vectors\\nof size\\nC\\n(where\\nC\\nis the input size). By default, the elements of\\nγ\\n\\\\gamma\\nγ\\nare set\\nto 1 and the elements of\\nβ\\n\\\\beta\\nβ\\nare set to 0. At train time in the forward pass, the'),\n",
       " Document(metadata={}, page_content='standard-deviation is calculated via the biased estimator, equivalent to\\ntorch.var(input,\\nunbiased=False)\\n. However, the value stored in the moving average of the\\nstandard-deviation is calculated via the unbiased  estimator, equivalent to\\ntorch.var(input,\\nunbiased=True)\\n.\\nAlso by default, during training this layer keeps running estimates of its\\ncomputed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default\\nmomentum\\nof 0.1.\\nIf'),\n",
       " Document(metadata={}, page_content='momentum\\nof 0.1.\\nIf\\ntrack_running_stats\\nis set to\\nFalse\\n, this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.\\nNote\\nThis\\nmomentum\\nargument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here is\\nx\\n^\\nnew\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_t\\nx\\n^'),\n",
       " Document(metadata={}, page_content='x\\n^\\nnew\\n\\u200b\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\u200b\\n,\\nwhere\\nx\\n^\\n\\\\hat{x}\\nx\\n^\\nis the estimated statistic and\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis the\\nnew observed value.\\nBecause the Batch Normalization is done over the\\nC\\ndimension, computing statistics\\non\\n(N, D, H, W)\\nslices, it’s common terminology to call this Volumetric Batch Normalization\\nor Spatio-temporal Batch Normalization.\\nParameters\\nnum_features\\n(\\nint\\n) –\\nC\\nC\\nC\\nfrom an expected input of size\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\neps'),\n",
       " Document(metadata={}, page_content='H\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability.\\nDefault: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var\\ncomputation. Can be set to\\nNone\\nfor cumulative moving average\\n(i.e. simple average). Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters. Default:\\nTrue\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to'),\n",
       " Document(metadata={}, page_content='(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics, and initializes statistics\\nbuffers\\nrunning_mean\\nand\\nrunning_var\\nas\\nNone\\n.\\nWhen these buffers are\\nNone\\n, this module always uses batch statistics.\\nin both training and eval modes. Default:\\nTrue\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,'),\n",
       " Document(metadata={}, page_content='C\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(same shape as input)\\nExamples:\\n>>>\\n# With Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nBatchNorm3d\\n(\\n100\\n)\\n>>>\\n# Without Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nBatchNorm3d\\n(\\n100\\n,\\naffine\\n=\\nFalse\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n100\\n,\\n35\\n,\\n45\\n,\\n10\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LazyBatchNorm1d\\n¶\\nclass\\ntorch.nn.\\nLazyBatchNorm1d\\n(\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.BatchNorm1d\\nmodule with lazy initialization.\\nLazy initialization based on the\\nnum_features\\nargument of the\\nBatchNorm1d\\nthat is inferred\\nfrom the\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\n,\\nbias\\n,\\nrunning_mean\\nand\\nrunning_var\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin'),\n",
       " Document(metadata={}, page_content='.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability.\\nDefault: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var\\ncomputation. Can be set to\\nNone\\nfor cumulative moving average\\n(i.e. simple average). Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has'),\n",
       " Document(metadata={}, page_content='True\\n, this module has\\nlearnable affine parameters. Default:\\nTrue\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics, and initializes statistics\\nbuffers\\nrunning_mean\\nand\\nrunning_var\\nas\\nNone\\n.\\nWhen these buffers are\\nNone\\n, this module always uses batch statistics.\\nin both training and eval modes. Default:\\nTrue\\ncls_to_become\\n[source]\\n¶\\nalias of\\nBatchNorm1d\\nNext'),\n",
       " Document(metadata={}, page_content='[source]\\n¶\\nalias of\\nBatchNorm1d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LazyBatchNorm2d\\n¶\\nclass\\ntorch.nn.\\nLazyBatchNorm2d\\n(\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.BatchNorm2d\\nmodule with lazy initialization.\\nLazy initialization is done for the\\nnum_features\\nargument of the\\nBatchNorm2d\\nthat is inferred\\nfrom the\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\n,\\nbias\\n,\\nrunning_mean\\nand\\nrunning_var\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin'),\n",
       " Document(metadata={}, page_content='.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability.\\nDefault: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var\\ncomputation. Can be set to\\nNone\\nfor cumulative moving average\\n(i.e. simple average). Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has'),\n",
       " Document(metadata={}, page_content='True\\n, this module has\\nlearnable affine parameters. Default:\\nTrue\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics, and initializes statistics\\nbuffers\\nrunning_mean\\nand\\nrunning_var\\nas\\nNone\\n.\\nWhen these buffers are\\nNone\\n, this module always uses batch statistics.\\nin both training and eval modes. Default:\\nTrue\\ncls_to_become\\n[source]\\n¶\\nalias of\\nBatchNorm2d\\nNext'),\n",
       " Document(metadata={}, page_content='[source]\\n¶\\nalias of\\nBatchNorm2d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LazyBatchNorm3d\\n¶\\nclass\\ntorch.nn.\\nLazyBatchNorm3d\\n(\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.BatchNorm3d\\nmodule with lazy initialization.\\nLazy initialization is done for the\\nnum_features\\nargument of the\\nBatchNorm3d\\nthat is inferred\\nfrom the\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\n,\\nbias\\n,\\nrunning_mean\\nand\\nrunning_var\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin'),\n",
       " Document(metadata={}, page_content='.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability.\\nDefault: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var\\ncomputation. Can be set to\\nNone\\nfor cumulative moving average\\n(i.e. simple average). Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has'),\n",
       " Document(metadata={}, page_content='True\\n, this module has\\nlearnable affine parameters. Default:\\nTrue\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics, and initializes statistics\\nbuffers\\nrunning_mean\\nand\\nrunning_var\\nas\\nNone\\n.\\nWhen these buffers are\\nNone\\n, this module always uses batch statistics.\\nin both training and eval modes. Default:\\nTrue\\ncls_to_become\\n[source]\\n¶\\nalias of\\nBatchNorm3d\\nNext'),\n",
       " Document(metadata={}, page_content='[source]\\n¶\\nalias of\\nBatchNorm3d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='GroupNorm\\n¶\\nclass\\ntorch.nn.\\nGroupNorm\\n(\\nnum_groups\\n,\\nnum_channels\\n,\\neps\\n=\\n1e-05\\n,\\naffine\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Group Normalization over a mini-batch of inputs.\\nThis layer implements the operation as described in\\nthe paper\\nGroup Normalization\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ\\ny = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\ny\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ\\nThe input channels are separated into\\nnum_groups'),\n",
       " Document(metadata={}, page_content='The input channels are separated into\\nnum_groups\\ngroups, each containing\\nnum_channels\\n/\\nnum_groups\\nchannels.\\nnum_channels\\nmust be divisible by\\nnum_groups\\n. The mean and standard-deviation are calculated\\nseparately over the each group.\\nγ\\n\\\\gamma\\nγ\\nand\\nβ\\n\\\\beta\\nβ\\nare learnable\\nper-channel affine transform parameter vectors of size\\nnum_channels\\nif\\naffine\\nis\\nTrue\\n.\\nThe variance is calculated via the biased estimator, equivalent to\\ntorch.var(input, unbiased=False)\\n.'),\n",
       " Document(metadata={}, page_content='torch.var(input, unbiased=False)\\n.\\nThis layer uses statistics computed from input data in both training and\\nevaluation modes.\\nParameters\\nnum_groups\\n(\\nint\\n) – number of groups to separate the channels into\\nnum_channels\\n(\\nint\\n) – number of channels expected in input\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability. Default: 1e-5\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module'),\n",
       " Document(metadata={}, page_content='True\\n, this module\\nhas learnable per-channel affine parameters initialized to ones (for weights)\\nand zeros (for biases). Default:\\nTrue\\n.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\n(N, C, *)\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\nwhere\\nC\\n=\\nnum_channels\\nC=\\\\text{num\\\\_channels}\\nC\\n=\\nnum_channels\\nOutput:\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\n(N, C, *)\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\n(same shape as input)\\nExamples:\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n6\\n,\\n10\\n,\\n10\\n)\\n>>>\\n# Separate 6 channels into 3 groups\\n>>>\\nm\\n=\\nnn\\n.\\nGroupNorm\\n(\\n3\\n,\\n6\\n)\\n>>>'),\n",
       " Document(metadata={}, page_content='>>>\\nm\\n=\\nnn\\n.\\nGroupNorm\\n(\\n3\\n,\\n6\\n)\\n>>>\\n# Separate 6 channels into 6 groups (equivalent with InstanceNorm)\\n>>>\\nm\\n=\\nnn\\n.\\nGroupNorm\\n(\\n6\\n,\\n6\\n)\\n>>>\\n# Put all 6 channels into a single group (equivalent with LayerNorm)\\n>>>\\nm\\n=\\nnn\\n.\\nGroupNorm\\n(\\n1\\n,\\n6\\n)\\n>>>\\n# Activating the module\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='SyncBatchNorm\\n¶\\nclass\\ntorch.nn.\\nSyncBatchNorm\\n(\\nnum_features\\n,\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\nprocess_group\\n=\\nNone\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Batch Normalization over a N-Dimensional input.\\nThe N-D input is a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper\\nBatch Normalization: Accelerating Deep Network Training by Reducing\\nInternal Covariate Shift\\n.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]'),\n",
       " Document(metadata={}, page_content='.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ\\ny = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\ny\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ\\nThe mean and standard-deviation are calculated per-dimension over all\\nmini-batches of the same process groups.\\nγ\\n\\\\gamma\\nγ\\nand\\nβ\\n\\\\beta\\nβ\\nare learnable parameter vectors of size\\nC\\n(where\\nC\\nis the input size).\\nBy default, the elements of\\nγ\\n\\\\gamma\\nγ\\nare sampled from\\nU\\n(\\n0\\n,\\n1\\n)\\n\\\\mathcal{U}(0, 1)\\nU\\n(\\n0\\n,\\n1\\n)\\nand the elements of\\nβ\\n\\\\beta'),\n",
       " Document(metadata={}, page_content='U\\n(\\n0\\n,\\n1\\n)\\nand the elements of\\nβ\\n\\\\beta\\nβ\\nare set to 0.\\nThe standard-deviation is calculated via the biased estimator, equivalent to\\ntorch.var(input, unbiased=False)\\n.\\nAlso by default, during training this layer keeps running estimates of its\\ncomputed mean and variance, which are then used for normalization during\\nevaluation. The running estimates are kept with a default\\nmomentum\\nof 0.1.\\nIf\\ntrack_running_stats\\nis set to\\nFalse\\n, this layer then does not'),\n",
       " Document(metadata={}, page_content='is set to\\nFalse\\n, this layer then does not\\nkeep running estimates, and batch statistics are instead used during\\nevaluation time as well.\\nNote\\nThis\\nmomentum\\nargument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here is\\nx\\n^\\nnew\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_t\\nx\\n^\\nnew\\n\\u200b\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum'),\n",
       " Document(metadata={}, page_content='x\\n^\\nnew\\n\\u200b\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\u200b\\n,\\nwhere\\nx\\n^\\n\\\\hat{x}\\nx\\n^\\nis the estimated statistic and\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis the\\nnew observed value.\\nBecause the Batch Normalization is done for each channel in the\\nC\\ndimension, computing\\nstatistics on\\n(N,\\n+)\\nslices, it’s common terminology to call this Volumetric Batch\\nNormalization or Spatio-temporal Batch Normalization.\\nCurrently\\nSyncBatchNorm\\nonly supports\\nDistributedDataParallel\\n(DDP) with single GPU per process. Use'),\n",
       " Document(metadata={}, page_content='(DDP) with single GPU per process. Use\\ntorch.nn.SyncBatchNorm.convert_sync_batchnorm()\\nto convert\\nBatchNorm*D\\nlayer to\\nSyncBatchNorm\\nbefore wrapping\\nNetwork with DDP.\\nParameters\\nnum_features\\n(\\nint\\n) –\\nC\\nC\\nC\\nfrom an expected input of size\\n(\\nN\\n,\\nC\\n,\\n+\\n)\\n(N, C, +)\\n(\\nN\\n,\\nC\\n,\\n+\\n)\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability.\\nDefault:\\n1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var\\ncomputation. Can be set to\\nNone'),\n",
       " Document(metadata={}, page_content='computation. Can be set to\\nNone\\nfor cumulative moving average\\n(i.e. simple average). Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters. Default:\\nTrue\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics, and initializes statistics\\nbuffers\\nrunning_mean\\nand\\nrunning_var\\nas\\nNone\\n.'),\n",
       " Document(metadata={}, page_content='buffers\\nrunning_mean\\nand\\nrunning_var\\nas\\nNone\\n.\\nWhen these buffers are\\nNone\\n, this module always uses batch statistics.\\nin both training and eval modes. Default:\\nTrue\\nprocess_group\\n(\\nOptional\\n[\\nAny\\n]\\n) – synchronization of stats happen within each process group\\nindividually. Default behavior is synchronization across the whole\\nworld\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\n+\\n)\\n(N, C, +)\\n(\\nN\\n,\\nC\\n,\\n+\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\n+\\n)\\n(N, C, +)\\n(\\nN\\n,\\nC\\n,\\n+\\n)\\n(same shape as input)\\nNote'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\n+\\n)\\n(same shape as input)\\nNote\\nSynchronization of batchnorm statistics occurs only while training, i.e.\\nsynchronization is disabled when\\nmodel.eval()\\nis set or if\\nself.training\\nis otherwise\\nFalse\\n.\\nExamples:\\n>>>\\n# With Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nSyncBatchNorm\\n(\\n100\\n)\\n>>>\\n# creating process group (optional)\\n>>>\\n# ranks is a list of int identifying rank ids.\\n>>>\\nranks\\n=\\nlist\\n(\\nrange\\n(\\n8\\n))\\n>>>\\nr1\\n,\\nr2\\n=\\nranks\\n[:\\n4\\n],\\nranks\\n[\\n4\\n:]\\n>>>'),\n",
       " Document(metadata={}, page_content='8\\n))\\n>>>\\nr1\\n,\\nr2\\n=\\nranks\\n[:\\n4\\n],\\nranks\\n[\\n4\\n:]\\n>>>\\n# Note: every rank calls into new_group for every\\n>>>\\n# process group created, even if that rank is not\\n>>>\\n# part of the group.\\n>>>\\nprocess_groups\\n=\\n[\\ntorch\\n.\\ndistributed\\n.\\nnew_group\\n(\\npids\\n)\\nfor\\npids\\nin\\n[\\nr1\\n,\\nr2\\n]]\\n>>>\\nprocess_group\\n=\\nprocess_groups\\n[\\n0\\nif\\ndist\\n.\\nget_rank\\n()\\n<=\\n3\\nelse\\n1\\n]\\n>>>\\n# Without Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nBatchNorm3d\\n(\\n100\\n,\\naffine\\n=\\nFalse\\n,\\nprocess_group\\n=\\nprocess_group\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n100\\n,'),\n",
       " Document(metadata={}, page_content=')\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n100\\n,\\n35\\n,\\n45\\n,\\n10\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\n# network is nn.BatchNorm layer\\n>>>\\nsync_bn_network\\n=\\nnn\\n.\\nSyncBatchNorm\\n.\\nconvert_sync_batchnorm\\n(\\nnetwork\\n,\\nprocess_group\\n)\\n>>>\\n# only single gpu per process is currently supported\\n>>>\\nddp_sync_bn_network\\n=\\ntorch\\n.\\nnn\\n.\\nparallel\\n.\\nDistributedDataParallel\\n(\\n>>>\\nsync_bn_network\\n,\\n>>>\\ndevice_ids\\n=\\n[\\nargs\\n.\\nlocal_rank\\n],\\n>>>\\noutput_device\\n=\\nargs\\n.\\nlocal_rank\\n)\\nclassmethod\\nconvert_sync_batchnorm\\n(\\nmodule\\n,'),\n",
       " Document(metadata={}, page_content=')\\nclassmethod\\nconvert_sync_batchnorm\\n(\\nmodule\\n,\\nprocess_group\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nConverts all\\nBatchNorm*D\\nlayers in the model to\\ntorch.nn.SyncBatchNorm\\nlayers.\\nParameters\\nmodule\\n(\\nnn.Module\\n) – module containing one or more\\nBatchNorm*D\\nlayers\\nprocess_group\\n(\\noptional\\n) – process group to scope synchronization,\\ndefault is the whole world\\nReturns\\nThe original\\nmodule\\nwith the converted\\ntorch.nn.SyncBatchNorm\\nlayers. If the original\\nmodule\\nis a\\nBatchNorm*D\\nlayer,\\na new'),\n",
       " Document(metadata={}, page_content='module\\nis a\\nBatchNorm*D\\nlayer,\\na new\\ntorch.nn.SyncBatchNorm\\nlayer object will be returned\\ninstead.\\nExample:\\n>>>\\n# Network with nn.BatchNorm layer\\n>>>\\nmodule\\n=\\ntorch\\n.\\nnn\\n.\\nSequential\\n(\\n>>>\\ntorch\\n.\\nnn\\n.\\nLinear\\n(\\n20\\n,\\n100\\n),\\n>>>\\ntorch\\n.\\nnn\\n.\\nBatchNorm1d\\n(\\n100\\n),\\n>>>\\n)\\n.\\ncuda\\n()\\n>>>\\n# creating process group (optional)\\n>>>\\n# ranks is a list of int identifying rank ids.\\n>>>\\nranks\\n=\\nlist\\n(\\nrange\\n(\\n8\\n))\\n>>>\\nr1\\n,\\nr2\\n=\\nranks\\n[:\\n4\\n],\\nranks\\n[\\n4\\n:]\\n>>>\\n# Note: every rank calls into new_group for every\\n>>>'),\n",
       " Document(metadata={}, page_content='>>>\\n# process group created, even if that rank is not\\n>>>\\n# part of the group.\\n>>>\\nprocess_groups\\n=\\n[\\ntorch\\n.\\ndistributed\\n.\\nnew_group\\n(\\npids\\n)\\nfor\\npids\\nin\\n[\\nr1\\n,\\nr2\\n]]\\n>>>\\nprocess_group\\n=\\nprocess_groups\\n[\\n0\\nif\\ndist\\n.\\nget_rank\\n()\\n<=\\n3\\nelse\\n1\\n]\\n>>>\\nsync_bn_module\\n=\\ntorch\\n.\\nnn\\n.\\nSyncBatchNorm\\n.\\nconvert_sync_batchnorm\\n(\\nmodule\\n,\\nprocess_group\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='InstanceNorm1d\\n¶\\nclass\\ntorch.nn.\\nInstanceNorm1d\\n(\\nnum_features\\n,\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nFalse\\n,\\ntrack_running_stats\\n=\\nFalse\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Instance Normalization.\\nThis operation applies Instance Normalization\\nover a 2D (unbatched) or 3D (batched) input as described in the paper\\nInstance Normalization: The Missing Ingredient for Fast Stylization\\n.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ'),\n",
       " Document(metadata={}, page_content='.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ\\ny = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\ny\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ\\nThe mean and standard-deviation are calculated per-dimension separately\\nfor each object in a mini-batch.\\nγ\\n\\\\gamma\\nγ\\nand\\nβ\\n\\\\beta\\nβ\\nare learnable parameter vectors\\nof size\\nC\\n(where\\nC\\nis the number of features or channels of the input) if\\naffine\\nis\\nTrue\\n.\\nThe variance is calculated via the biased estimator, equivalent to'),\n",
       " Document(metadata={}, page_content='torch.var(input, unbiased=False)\\n.\\nBy default, this layer uses instance statistics computed from input data in\\nboth training and evaluation modes.\\nIf\\ntrack_running_stats\\nis set to\\nTrue\\n, during training this\\nlayer keeps running estimates of its computed mean and variance, which are\\nthen used for normalization during evaluation. The running estimates are\\nkept with a default\\nmomentum\\nof 0.1.\\nNote\\nThis\\nmomentum\\nargument is different from one used in optimizer'),\n",
       " Document(metadata={}, page_content='argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here is\\nx\\n^\\nnew\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_t\\nx\\n^\\nnew\\n\\u200b\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\u200b\\n,\\nwhere\\nx\\n^\\n\\\\hat{x}\\nx\\n^\\nis the estimated statistic and\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis the\\nnew observed value.\\nNote\\nInstanceNorm1d\\nand\\nLayerNorm'),\n",
       " Document(metadata={}, page_content='Note\\nInstanceNorm1d\\nand\\nLayerNorm\\nare very similar, but\\nhave some subtle differences.\\nInstanceNorm1d\\nis applied\\non each channel of channeled data like multidimensional time series, but\\nLayerNorm\\nis usually applied on entire sample and often in NLP\\ntasks. Additionally,\\nLayerNorm\\napplies elementwise affine\\ntransform, while\\nInstanceNorm1d\\nusually don’t apply affine\\ntransform.\\nParameters\\nnum_features\\n(\\nint\\n) – number of features or channels\\nC\\nC\\nC\\nof the input\\neps\\n(\\nfloat'),\n",
       " Document(metadata={}, page_content='C\\nC\\nC\\nof the input\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability. Default: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var computation. Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault:\\nFalse\\n.\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this'),\n",
       " Document(metadata={}, page_content=') – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\nor\\n(\\nC\\n,\\nL\\n)\\n(C, L)\\n(\\nC\\n,\\nL\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\nor\\n(\\nC\\n,\\nL\\n)\\n(C, L)\\n(\\nC\\n,\\nL\\n)\\n(same shape as input)\\nExamples:\\n>>>\\n# Without Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nInstanceNorm1d\\n('),\n",
       " Document(metadata={}, page_content='>>>\\nm\\n=\\nnn\\n.\\nInstanceNorm1d\\n(\\n100\\n)\\n>>>\\n# With Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nInstanceNorm1d\\n(\\n100\\n,\\naffine\\n=\\nTrue\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n100\\n,\\n40\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='InstanceNorm2d\\n¶\\nclass\\ntorch.nn.\\nInstanceNorm2d\\n(\\nnum_features\\n,\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nFalse\\n,\\ntrack_running_stats\\n=\\nFalse\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Instance Normalization.\\nThis operation applies Instance Normalization\\nover a 4D input (a mini-batch of 2D inputs\\nwith additional channel dimension) as described in the paper\\nInstance Normalization: The Missing Ingredient for Fast Stylization\\n.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ'),\n",
       " Document(metadata={}, page_content='.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ\\ny = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\ny\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ\\nThe mean and standard-deviation are calculated per-dimension separately\\nfor each object in a mini-batch.\\nγ\\n\\\\gamma\\nγ\\nand\\nβ\\n\\\\beta\\nβ\\nare learnable parameter vectors\\nof size\\nC\\n(where\\nC\\nis the input size) if\\naffine\\nis\\nTrue\\n.\\nThe standard-deviation is calculated via the biased estimator, equivalent to\\ntorch.var(input, unbiased=False)\\n.'),\n",
       " Document(metadata={}, page_content='torch.var(input, unbiased=False)\\n.\\nBy default, this layer uses instance statistics computed from input data in\\nboth training and evaluation modes.\\nIf\\ntrack_running_stats\\nis set to\\nTrue\\n, during training this\\nlayer keeps running estimates of its computed mean and variance, which are\\nthen used for normalization during evaluation. The running estimates are\\nkept with a default\\nmomentum\\nof 0.1.\\nNote\\nThis\\nmomentum\\nargument is different from one used in optimizer'),\n",
       " Document(metadata={}, page_content='argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here is\\nx\\n^\\nnew\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_t\\nx\\n^\\nnew\\n\\u200b\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\u200b\\n,\\nwhere\\nx\\n^\\n\\\\hat{x}\\nx\\n^\\nis the estimated statistic and\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis the\\nnew observed value.\\nNote\\nInstanceNorm2d\\nand\\nLayerNorm'),\n",
       " Document(metadata={}, page_content='Note\\nInstanceNorm2d\\nand\\nLayerNorm\\nare very similar, but\\nhave some subtle differences.\\nInstanceNorm2d\\nis applied\\non each channel of channeled data like RGB images, but\\nLayerNorm\\nis usually applied on entire sample and often in NLP\\ntasks. Additionally,\\nLayerNorm\\napplies elementwise affine\\ntransform, while\\nInstanceNorm2d\\nusually don’t apply affine\\ntransform.\\nParameters\\nnum_features\\n(\\nint\\n) –\\nC\\nC\\nC\\nfrom an expected input of size\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)'),\n",
       " Document(metadata={}, page_content=')\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability. Default: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var computation. Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault:\\nFalse\\n.\\ntrack_running_stats\\n(\\nbool'),\n",
       " Document(metadata={}, page_content='Default:\\nFalse\\n.\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)'),\n",
       " Document(metadata={}, page_content='H\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(same shape as input)\\nExamples:\\n>>>\\n# Without Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nInstanceNorm2d\\n(\\n100\\n)\\n>>>\\n# With Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nInstanceNorm2d\\n(\\n100\\n,\\naffine\\n=\\nTrue\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n100\\n,\\n35\\n,\\n45\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='InstanceNorm3d\\n¶\\nclass\\ntorch.nn.\\nInstanceNorm3d\\n(\\nnum_features\\n,\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nFalse\\n,\\ntrack_running_stats\\n=\\nFalse\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Instance Normalization.\\nThis operation applies Instance Normalization\\nover a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper\\nInstance Normalization: The Missing Ingredient for Fast Stylization\\n.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ'),\n",
       " Document(metadata={}, page_content='.\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ\\ny = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\ny\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ\\nThe mean and standard-deviation are calculated per-dimension separately\\nfor each object in a mini-batch.\\nγ\\n\\\\gamma\\nγ\\nand\\nβ\\n\\\\beta\\nβ\\nare learnable parameter vectors\\nof size C (where C is the input size) if\\naffine\\nis\\nTrue\\n.\\nThe standard-deviation is calculated via the biased estimator, equivalent to\\ntorch.var(input, unbiased=False)\\n.'),\n",
       " Document(metadata={}, page_content='torch.var(input, unbiased=False)\\n.\\nBy default, this layer uses instance statistics computed from input data in\\nboth training and evaluation modes.\\nIf\\ntrack_running_stats\\nis set to\\nTrue\\n, during training this\\nlayer keeps running estimates of its computed mean and variance, which are\\nthen used for normalization during evaluation. The running estimates are\\nkept with a default\\nmomentum\\nof 0.1.\\nNote\\nThis\\nmomentum\\nargument is different from one used in optimizer'),\n",
       " Document(metadata={}, page_content='argument is different from one used in optimizer\\nclasses and the conventional notion of momentum. Mathematically, the\\nupdate rule for running statistics here is\\nx\\n^\\nnew\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_t\\nx\\n^\\nnew\\n\\u200b\\n=\\n(\\n1\\n−\\nmomentum\\n)\\n×\\nx\\n^\\n+\\nmomentum\\n×\\nx\\nt\\n\\u200b\\n,\\nwhere\\nx\\n^\\n\\\\hat{x}\\nx\\n^\\nis the estimated statistic and\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis the\\nnew observed value.\\nNote\\nInstanceNorm3d\\nand\\nLayerNorm'),\n",
       " Document(metadata={}, page_content='Note\\nInstanceNorm3d\\nand\\nLayerNorm\\nare very similar, but\\nhave some subtle differences.\\nInstanceNorm3d\\nis applied\\non each channel of channeled data like 3D models with RGB color, but\\nLayerNorm\\nis usually applied on entire sample and often in NLP\\ntasks. Additionally,\\nLayerNorm\\napplies elementwise affine\\ntransform, while\\nInstanceNorm3d\\nusually don’t apply affine\\ntransform.\\nParameters\\nnum_features\\n(\\nint\\n) –\\nC\\nC\\nC\\nfrom an expected input of size\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,'),\n",
       " Document(metadata={}, page_content='C\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability. Default: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var computation. Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault:\\nFalse\\n.'),\n",
       " Document(metadata={}, page_content='Default:\\nFalse\\n.\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(same shape as input)\\nExamples:\\n>>>\\n# Without Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nInstanceNorm3d\\n(\\n100\\n)\\n>>>\\n# With Learnable Parameters\\n>>>\\nm\\n=\\nnn\\n.\\nInstanceNorm3d\\n(\\n100\\n,\\naffine\\n=\\nTrue\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n100\\n,\\n35\\n,\\n45\\n,\\n10\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LazyInstanceNorm1d\\n¶\\nclass\\ntorch.nn.\\nLazyInstanceNorm1d\\n(\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.InstanceNorm1d\\nmodule with lazy initialization of the\\nnum_features\\nargument.\\nThe\\nnum_features\\nargument of the\\nInstanceNorm1d\\nis inferred from the\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\n,\\nbias\\n,\\nrunning_mean\\nand\\nrunning_var\\n.\\nCheck the'),\n",
       " Document(metadata={}, page_content=',\\nbias\\n,\\nrunning_mean\\nand\\nrunning_var\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nnum_features\\n–\\nC\\nC\\nC\\nfrom an expected input of size\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\nor\\n(\\nC\\n,\\nL\\n)\\n(C, L)\\n(\\nC\\n,\\nL\\n)\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability. Default: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var computation. Default: 0.1\\naffine\\n(\\nbool'),\n",
       " Document(metadata={}, page_content='affine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault:\\nFalse\\n.\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics and always uses batch\\nstatistics in both training and eval modes. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)'),\n",
       " Document(metadata={}, page_content='Input:\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\nor\\n(\\nC\\n,\\nL\\n)\\n(C, L)\\n(\\nC\\n,\\nL\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\nor\\n(\\nC\\n,\\nL\\n)\\n(C, L)\\n(\\nC\\n,\\nL\\n)\\n(same shape as input)\\ncls_to_become\\n[source]\\n¶\\nalias of\\nInstanceNorm1d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LazyInstanceNorm2d\\n¶\\nclass\\ntorch.nn.\\nLazyInstanceNorm2d\\n(\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.InstanceNorm2d\\nmodule with lazy initialization of the\\nnum_features\\nargument.\\nThe\\nnum_features\\nargument of the\\nInstanceNorm2d\\nis inferred from the\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\n,\\nbias\\n,\\nrunning_mean\\nand\\nrunning_var\\n.\\nCheck the'),\n",
       " Document(metadata={}, page_content=',\\nbias\\n,\\nrunning_mean\\nand\\nrunning_var\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nnum_features\\n–\\nC\\nC\\nC\\nfrom an expected input of size\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability. Default: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]'),\n",
       " Document(metadata={}, page_content='momentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var computation. Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault:\\nFalse\\n.\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics and always uses batch'),\n",
       " Document(metadata={}, page_content='statistics in both training and eval modes. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(same shape as input)\\ncls_to_become\\n[source]\\n¶\\nalias of\\nInstanceNorm2d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LazyInstanceNorm3d\\n¶\\nclass\\ntorch.nn.\\nLazyInstanceNorm3d\\n(\\neps\\n=\\n1e-05\\n,\\nmomentum\\n=\\n0.1\\n,\\naffine\\n=\\nTrue\\n,\\ntrack_running_stats\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.InstanceNorm3d\\nmodule with lazy initialization of the\\nnum_features\\nargument.\\nThe\\nnum_features\\nargument of the\\nInstanceNorm3d\\nis inferred from the\\ninput.size(1)\\n.\\nThe attributes that will be lazily initialized are\\nweight\\n,\\nbias\\n,\\nrunning_mean\\nand\\nrunning_var\\n.\\nCheck the'),\n",
       " Document(metadata={}, page_content=',\\nbias\\n,\\nrunning_mean\\nand\\nrunning_var\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nnum_features\\n–\\nC\\nC\\nC\\nfrom an expected input of size\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability. Default: 1e-5\\nmomentum\\n(\\nOptional\\n[\\nfloat\\n]'),\n",
       " Document(metadata={}, page_content='momentum\\n(\\nOptional\\n[\\nfloat\\n]\\n) – the value used for the running_mean and running_var computation. Default: 0.1\\naffine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module has\\nlearnable affine parameters, initialized the same way as done for batch normalization.\\nDefault:\\nFalse\\n.\\ntrack_running_stats\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this\\nmodule tracks the running mean and variance, and when set to\\nFalse\\n,\\nthis module does not track such statistics and always uses batch'),\n",
       " Document(metadata={}, page_content='statistics in both training and eval modes. Default:\\nFalse\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(same shape as input)\\ncls_to_become\\n[source]\\n¶\\nalias of\\nInstanceNorm3d\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LayerNorm\\n¶\\nclass\\ntorch.nn.\\nLayerNorm\\n(\\nnormalized_shape\\n,\\neps\\n=\\n1e-05\\n,\\nelementwise_affine\\n=\\nTrue\\n,\\nbias\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Layer Normalization over a mini-batch of inputs.\\nThis layer implements the operation as described in\\nthe paper\\nLayer Normalization\\ny\\n=\\nx\\n−\\nE\\n[\\nx\\n]\\nV\\na\\nr\\n[\\nx\\n]\\n+\\nϵ\\n∗\\nγ\\n+\\nβ\\ny = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\ny\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ'),\n",
       " Document(metadata={}, page_content='y\\n=\\nVar\\n[\\nx\\n]\\n+\\nϵ\\n\\u200b\\nx\\n−\\nE\\n[\\nx\\n]\\n\\u200b\\n∗\\nγ\\n+\\nβ\\nThe mean and standard-deviation are calculated over the last\\nD\\ndimensions, where\\nD\\nis the dimension of\\nnormalized_shape\\n. For example, if\\nnormalized_shape\\nis\\n(3,\\n5)\\n(a 2-dimensional shape), the mean and standard-deviation are computed over\\nthe last 2 dimensions of the input (i.e.\\ninput.mean((-2,\\n-1))\\n).\\nγ\\n\\\\gamma\\nγ\\nand\\nβ\\n\\\\beta\\nβ\\nare learnable affine transform parameters of\\nnormalized_shape\\nif\\nelementwise_affine\\nis\\nTrue\\n.'),\n",
       " Document(metadata={}, page_content='normalized_shape\\nif\\nelementwise_affine\\nis\\nTrue\\n.\\nThe variance is calculated via the biased estimator, equivalent to\\ntorch.var(input, unbiased=False)\\n.\\nNote\\nUnlike Batch Normalization and Instance Normalization, which applies\\nscalar scale and bias for each entire channel/plane with the\\naffine\\noption, Layer Normalization applies per-element scale and\\nbias with\\nelementwise_affine\\n.\\nThis layer uses statistics computed from input data in both training and\\nevaluation modes.\\nParameters'),\n",
       " Document(metadata={}, page_content='evaluation modes.\\nParameters\\nnormalized_shape\\n(\\nint\\nor\\nlist\\nor\\ntorch.Size\\n) –\\ninput shape from an expected input\\nof size\\n[\\n∗\\n×\\nnormalized_shape\\n[\\n0\\n]\\n×\\nnormalized_shape\\n[\\n1\\n]\\n×\\n…\\n×\\nnormalized_shape\\n[\\n−\\n1\\n]\\n]\\n[* \\\\times \\\\text{normalized\\\\_shape}[0] \\\\times \\\\text{normalized\\\\_shape}[1]\\n    \\\\times \\\\ldots \\\\times \\\\text{normalized\\\\_shape}[-1]]\\n[\\n∗\\n×\\nnormalized_shape\\n[\\n0\\n]\\n×\\nnormalized_shape\\n[\\n1\\n]\\n×\\n…\\n×\\nnormalized_shape\\n[\\n−\\n1\\n]]'),\n",
       " Document(metadata={}, page_content='[\\n1\\n]\\n×\\n…\\n×\\nnormalized_shape\\n[\\n−\\n1\\n]]\\nIf a single integer is used, it is treated as a singleton list, and this module will\\nnormalize over the last dimension which is expected to be of that specific size.\\neps\\n(\\nfloat\\n) – a value added to the denominator for numerical stability. Default: 1e-5\\nelementwise_affine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module\\nhas learnable per-element affine parameters initialized to ones (for weights)\\nand zeros (for biases). Default:\\nTrue\\n.\\nbias\\n('),\n",
       " Document(metadata={}, page_content='and zeros (for biases). Default:\\nTrue\\n.\\nbias\\n(\\nbool\\n) – If set to\\nFalse\\n, the layer will not learn an additive bias (only relevant if\\nelementwise_affine\\nis\\nTrue\\n). Default:\\nTrue\\n.\\nVariables\\nweight\\n– the learnable weights of the module of shape\\nnormalized_shape\\n\\\\text{normalized\\\\_shape}\\nnormalized_shape\\nwhen\\nelementwise_affine\\nis set to\\nTrue\\n.\\nThe values are initialized to 1.\\nbias\\n– the learnable bias of the module of shape\\nnormalized_shape\\n\\\\text{normalized\\\\_shape}\\nnormalized_shape\\nwhen'),\n",
       " Document(metadata={}, page_content='\\\\text{normalized\\\\_shape}\\nnormalized_shape\\nwhen\\nelementwise_affine\\nis set to\\nTrue\\n.\\nThe values are initialized to 0.\\nShape:\\nInput:\\n(\\nN\\n,\\n∗\\n)\\n(N, *)\\n(\\nN\\n,\\n∗\\n)\\nOutput:\\n(\\nN\\n,\\n∗\\n)\\n(N, *)\\n(\\nN\\n,\\n∗\\n)\\n(same shape as input)\\nExamples:\\n>>>\\n# NLP Example\\n>>>\\nbatch\\n,\\nsentence_length\\n,\\nembedding_dim\\n=\\n20\\n,\\n5\\n,\\n10\\n>>>\\nembedding\\n=\\ntorch\\n.\\nrandn\\n(\\nbatch\\n,\\nsentence_length\\n,\\nembedding_dim\\n)\\n>>>\\nlayer_norm\\n=\\nnn\\n.\\nLayerNorm\\n(\\nembedding_dim\\n)\\n>>>\\n# Activate module\\n>>>\\nlayer_norm\\n(\\nembedding\\n)\\n>>>\\n>>>\\n# Image Example'),\n",
       " Document(metadata={}, page_content='layer_norm\\n(\\nembedding\\n)\\n>>>\\n>>>\\n# Image Example\\n>>>\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n=\\n20\\n,\\n5\\n,\\n10\\n,\\n10\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n>>>\\n# Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\\n>>>\\n# as shown in the image below\\n>>>\\nlayer_norm\\n=\\nnn\\n.\\nLayerNorm\\n([\\nC\\n,\\nH\\n,\\nW\\n])\\n>>>\\noutput\\n=\\nlayer_norm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LocalResponseNorm\\n¶\\nclass\\ntorch.nn.\\nLocalResponseNorm\\n(\\nsize\\n,\\nalpha\\n=\\n0.0001\\n,\\nbeta\\n=\\n0.75\\n,\\nk\\n=\\n1.0\\n)\\n[source]\\n[source]\\n¶\\nApplies local response normalization over an input signal.\\nThe input signal is composed of several input planes, where channels occupy the second dimension.\\nApplies normalization across channels.\\nb\\nc\\n=\\na\\nc\\n(\\nk\\n+\\nα\\nn\\n∑\\nc\\n′\\n=\\nmax\\n\\u2061\\n(\\n0\\n,\\nc\\n−\\nn\\n/\\n2\\n)\\nmin\\n\\u2061\\n(\\nN\\n−\\n1\\n,\\nc\\n+\\nn\\n/\\n2\\n)\\na\\nc\\n′\\n2\\n)\\n−\\nβ\\nb_{c} = a_{c}\\\\left(k + \\\\frac{\\\\alpha}{n}'),\n",
       " Document(metadata={}, page_content=\"′\\n2\\n)\\n−\\nβ\\nb_{c} = a_{c}\\\\left(k + \\\\frac{\\\\alpha}{n}\\n\\\\sum_{c'=\\\\max(0, c-n/2)}^{\\\\min(N-1,c+n/2)}a_{c'}^2\\\\right)^{-\\\\beta}\\nb\\nc\\n\\u200b\\n=\\na\\nc\\n\\u200b\\n\\u200b\\nk\\n+\\nn\\nα\\n\\u200b\\nc\\n′\\n=\\nm\\na\\nx\\n(\\n0\\n,\\nc\\n−\\nn\\n/2\\n)\\n∑\\nm\\ni\\nn\\n(\\nN\\n−\\n1\\n,\\nc\\n+\\nn\\n/2\\n)\\n\\u200b\\na\\nc\\n′\\n2\\n\\u200b\\n\\u200b\\n−\\nβ\\nParameters\\nsize\\n(\\nint\\n) – amount of neighbouring channels used for normalization\\nalpha\\n(\\nfloat\\n) – multiplicative factor. Default: 0.0001\\nbeta\\n(\\nfloat\\n) – exponent. Default: 0.75\\nk\\n(\\nfloat\\n) – additive factor. Default: 1\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\n(N, C, *)\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\nOutput:\"),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\n∗\\n)\\n(N, C, *)\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\nOutput:\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\n(N, C, *)\\n(\\nN\\n,\\nC\\n,\\n∗\\n)\\n(same shape as input)\\nExamples:\\n>>>\\nlrn\\n=\\nnn\\n.\\nLocalResponseNorm\\n(\\n2\\n)\\n>>>\\nsignal_2d\\n=\\ntorch\\n.\\nrandn\\n(\\n32\\n,\\n5\\n,\\n24\\n,\\n24\\n)\\n>>>\\nsignal_4d\\n=\\ntorch\\n.\\nrandn\\n(\\n16\\n,\\n5\\n,\\n7\\n,\\n7\\n,\\n7\\n,\\n7\\n)\\n>>>\\noutput_2d\\n=\\nlrn\\n(\\nsignal_2d\\n)\\n>>>\\noutput_4d\\n=\\nlrn\\n(\\nsignal_4d\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='RMSNorm\\n¶\\nclass\\ntorch.nn.\\nRMSNorm\\n(\\nnormalized_shape\\n,\\neps\\n=\\nNone\\n,\\nelementwise_affine\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies Root Mean Square Layer Normalization over a mini-batch of inputs.\\nThis layer implements the operation as described in\\nthe paper\\nRoot Mean Square Layer Normalization\\ny\\ni\\n=\\nx\\ni\\nR\\nM\\nS\\n(\\nx\\n)\\n∗\\nγ\\ni\\n,\\nwhere\\nRMS\\n(\\nx\\n)\\n=\\nϵ\\n+\\n1\\nn\\n∑\\ni\\n=\\n1\\nn\\nx\\ni\\n2\\ny_i = \\\\frac{x_i}{\\\\mathrm{RMS}(x)} * \\\\gamma_i, \\\\quad'),\n",
       " Document(metadata={}, page_content='\\\\text{where} \\\\quad \\\\text{RMS}(x) = \\\\sqrt{\\\\epsilon + \\\\frac{1}{n} \\\\sum_{i=1}^{n} x_i^2}\\ny\\ni\\n\\u200b\\n=\\nRMS\\n(\\nx\\n)\\nx\\ni\\n\\u200b\\n\\u200b\\n∗\\nγ\\ni\\n\\u200b\\n,\\nwhere\\nRMS\\n(\\nx\\n)\\n=\\nϵ\\n+\\nn\\n1\\n\\u200b\\ni\\n=\\n1\\n∑\\nn\\n\\u200b\\nx\\ni\\n2\\n\\u200b\\n\\u200b\\nThe RMS is taken over the last\\nD\\ndimensions, where\\nD\\nis the dimension of\\nnormalized_shape\\n. For example, if\\nnormalized_shape\\nis\\n(3,\\n5)\\n(a 2-dimensional shape), the RMS is computed over\\nthe last 2 dimensions of the input.\\nParameters\\nnormalized_shape\\n(\\nint\\nor\\nlist\\nor\\ntorch.Size\\n) –\\ninput shape from an expected input\\nof size\\n[\\n∗'),\n",
       " Document(metadata={}, page_content='input shape from an expected input\\nof size\\n[\\n∗\\n×\\nnormalized_shape\\n[\\n0\\n]\\n×\\nnormalized_shape\\n[\\n1\\n]\\n×\\n…\\n×\\nnormalized_shape\\n[\\n−\\n1\\n]\\n]\\n[* \\\\times \\\\text{normalized\\\\_shape}[0] \\\\times \\\\text{normalized\\\\_shape}[1]\\n    \\\\times \\\\ldots \\\\times \\\\text{normalized\\\\_shape}[-1]]\\n[\\n∗\\n×\\nnormalized_shape\\n[\\n0\\n]\\n×\\nnormalized_shape\\n[\\n1\\n]\\n×\\n…\\n×\\nnormalized_shape\\n[\\n−\\n1\\n]]\\nIf a single integer is used, it is treated as a singleton list, and this module will'),\n",
       " Document(metadata={}, page_content='normalize over the last dimension which is expected to be of that specific size.\\neps\\n(\\nOptional\\n[\\nfloat\\n]\\n) – a value added to the denominator for numerical stability. Default:\\ntorch.finfo(x.dtype).eps()\\nelementwise_affine\\n(\\nbool\\n) – a boolean value that when set to\\nTrue\\n, this module\\nhas learnable per-element affine parameters initialized to ones (for weights). Default:\\nTrue\\n.\\nShape:\\nInput:\\n(\\nN\\n,\\n∗\\n)\\n(N, *)\\n(\\nN\\n,\\n∗\\n)\\nOutput:\\n(\\nN\\n,\\n∗\\n)\\n(N, *)\\n(\\nN\\n,\\n∗\\n)\\n(same shape as input)\\nExamples:\\n>>>'),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\n∗\\n)\\n(same shape as input)\\nExamples:\\n>>>\\nrms_norm\\n=\\nnn\\n.\\nRMSNorm\\n([\\n2\\n,\\n3\\n])\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n2\\n,\\n3\\n)\\n>>>\\nrms_norm\\n(\\ninput\\n)\\nextra_repr\\n(\\n)\\n[source]\\n[source]\\n¶\\nExtra information about the module.\\nReturn type\\nstr\\nforward\\n(\\nx\\n)\\n[source]\\n[source]\\n¶\\nRuns forward pass.\\nReturn type\\nTensor\\nreset_parameters\\n(\\n)\\n[source]\\n[source]\\n¶\\nResets parameters based on their initialization used in __init__.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme'),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='RNNBase\\n¶\\nclass\\ntorch.nn.\\nRNNBase\\n(\\nmode\\n,\\ninput_size\\n,\\nhidden_size\\n,\\nnum_layers\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\nbatch_first\\n=\\nFalse\\n,\\ndropout\\n=\\n0.0\\n,\\nbidirectional\\n=\\nFalse\\n,\\nproj_size\\n=\\n0\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nBase class for RNN modules (RNN, LSTM, GRU).\\nImplements aspects of RNNs shared by the RNN, LSTM, and GRU classes, such as module initialization\\nand utility methods for parameter storage management.\\nNote\\nThe forward method is not implemented by the RNNBase class.\\nNote'),\n",
       " Document(metadata={}, page_content='Note\\nLSTM and GRU classes override some methods implemented by RNNBase.\\nflatten_parameters\\n(\\n)\\n[source]\\n[source]\\n¶\\nReset parameter data pointer so that they can use faster code paths.\\nRight now, this works only if the module is on the GPU and cuDNN is enabled.\\nOtherwise, it’s a no-op.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"RNN\\n¶\\nclass\\ntorch.nn.\\nRNN\\n(\\ninput_size\\n,\\nhidden_size\\n,\\nnum_layers\\n=\\n1\\n,\\nnonlinearity\\n=\\n'tanh'\\n,\\nbias\\n=\\nTrue\\n,\\nbatch_first\\n=\\nFalse\\n,\\ndropout\\n=\\n0.0\\n,\\nbidirectional\\n=\\nFalse\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApply a multi-layer Elman RNN with\\ntanh\\n\\u2061\\n\\\\tanh\\ntanh\\nor\\nReLU\\n\\\\text{ReLU}\\nReLU\\nnon-linearity to an input sequence. For each element in the input sequence,\\neach layer computes the following function:\\nh\\nt\\n=\\ntanh\\n\\u2061\\n(\\nx\\nt\\nW\\ni\\nh\\nT\\n+\\nb\\ni\\nh\\n+\\nh\\nt\\n−\\n1\\nW\\nh\\nh\\nT\\n+\\nb\\nh\\nh\\n)\"),\n",
       " Document(metadata={}, page_content=\"(\\nx\\nt\\nW\\ni\\nh\\nT\\n+\\nb\\ni\\nh\\n+\\nh\\nt\\n−\\n1\\nW\\nh\\nh\\nT\\n+\\nb\\nh\\nh\\n)\\nh_t = \\\\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})\\nh\\nt\\n\\u200b\\n=\\ntanh\\n(\\nx\\nt\\n\\u200b\\nW\\nih\\nT\\n\\u200b\\n+\\nb\\nih\\n\\u200b\\n+\\nh\\nt\\n−\\n1\\n\\u200b\\nW\\nhh\\nT\\n\\u200b\\n+\\nb\\nhh\\n\\u200b\\n)\\nwhere\\nh\\nt\\nh_t\\nh\\nt\\n\\u200b\\nis the hidden state at time\\nt\\n,\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis\\nthe input at time\\nt\\n, and\\nh\\n(\\nt\\n−\\n1\\n)\\nh_{(t-1)}\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\nis the hidden state of the\\nprevious layer at time\\nt-1\\nor the initial hidden state at time\\n0\\n.\\nIf\\nnonlinearity\\nis\\n'relu'\\n, then\\nReLU\\n\\\\text{ReLU}\\nReLU\\nis used instead of\\ntanh\\n\\u2061\\n\\\\tanh\\ntanh\"),\n",
       " Document(metadata={}, page_content='ReLU\\nis used instead of\\ntanh\\n\\u2061\\n\\\\tanh\\ntanh\\n.\\n# Efficient implementation equivalent to the following with bidirectional=False\\ndef\\nforward\\n(\\nx\\n,\\nhx\\n=\\nNone\\n):\\nif\\nbatch_first\\n:\\nx\\n=\\nx\\n.\\ntranspose\\n(\\n0\\n,\\n1\\n)\\nseq_len\\n,\\nbatch_size\\n,\\n_\\n=\\nx\\n.\\nsize\\n()\\nif\\nhx\\nis\\nNone\\n:\\nhx\\n=\\ntorch\\n.\\nzeros\\n(\\nnum_layers\\n,\\nbatch_size\\n,\\nhidden_size\\n)\\nh_t_minus_1\\n=\\nhx\\nh_t\\n=\\nhx\\noutput\\n=\\n[]\\nfor\\nt\\nin\\nrange\\n(\\nseq_len\\n):\\nfor\\nlayer\\nin\\nrange\\n(\\nnum_layers\\n):\\nh_t\\n[\\nlayer\\n]\\n=\\ntorch\\n.\\ntanh\\n(\\nx\\n[\\nt\\n]\\n@\\nweight_ih\\n[\\nlayer\\n]\\n.\\nT\\n+\\nbias_ih\\n[\\nlayer'),\n",
       " Document(metadata={}, page_content='[\\nt\\n]\\n@\\nweight_ih\\n[\\nlayer\\n]\\n.\\nT\\n+\\nbias_ih\\n[\\nlayer\\n]\\n+\\nh_t_minus_1\\n[\\nlayer\\n]\\n@\\nweight_hh\\n[\\nlayer\\n]\\n.\\nT\\n+\\nbias_hh\\n[\\nlayer\\n]\\n)\\noutput\\n.\\nappend\\n(\\nh_t\\n[\\n-\\n1\\n])\\nh_t_minus_1\\n=\\nh_t\\noutput\\n=\\ntorch\\n.\\nstack\\n(\\noutput\\n)\\nif\\nbatch_first\\n:\\noutput\\n=\\noutput\\n.\\ntranspose\\n(\\n0\\n,\\n1\\n)\\nreturn\\noutput\\n,\\nh_t\\nParameters\\ninput_size\\n– The number of expected features in the input\\nx\\nhidden_size\\n– The number of features in the hidden state\\nh\\nnum_layers\\n– Number of recurrent layers. E.g., setting\\nnum_layers=2'),\n",
       " Document(metadata={}, page_content=\"num_layers=2\\nwould mean stacking two RNNs together to form a\\nstacked RNN\\n,\\nwith the second RNN taking in outputs of the first RNN and\\ncomputing the final results. Default: 1\\nnonlinearity\\n– The non-linearity to use. Can be either\\n'tanh'\\nor\\n'relu'\\n. Default:\\n'tanh'\\nbias\\n– If\\nFalse\\n, then the layer does not use bias weights\\nb_ih\\nand\\nb_hh\\n.\\nDefault:\\nTrue\\nbatch_first\\n– If\\nTrue\\n, then the input and output tensors are provided\\nas\\n(batch, seq, feature)\\ninstead of\\n(seq, batch, feature)\\n.\"),\n",
       " Document(metadata={}, page_content='instead of\\n(seq, batch, feature)\\n.\\nNote that this does not apply to hidden or cell states. See the\\nInputs/Outputs sections below for details.  Default:\\nFalse\\ndropout\\n– If non-zero, introduces a\\nDropout\\nlayer on the outputs of each\\nRNN layer except the last layer, with dropout probability equal to\\ndropout\\n. Default: 0\\nbidirectional\\n– If\\nTrue\\n, becomes a bidirectional RNN. Default:\\nFalse\\nInputs: input, hx\\ninput\\n: tensor of shape\\n(\\nL\\n,\\nH\\ni\\nn\\n)\\n(L, H_{in})\\n(\\nL\\n,\\nH\\nin\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nL\\n,'),\n",
       " Document(metadata={}, page_content='(\\nL\\n,\\nH\\nin\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nL\\n,\\nN\\n,\\nH\\ni\\nn\\n)\\n(L, N, H_{in})\\n(\\nL\\n,\\nN\\n,\\nH\\nin\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nL\\n,\\nH\\ni\\nn\\n)\\n(N, L, H_{in})\\n(\\nN\\n,\\nL\\n,\\nH\\nin\\n\\u200b\\n)\\nwhen\\nbatch_first=True\\ncontaining the features of\\nthe input sequence.  The input can also be a packed variable length sequence.\\nSee\\ntorch.nn.utils.rnn.pack_padded_sequence()\\nor\\ntorch.nn.utils.rnn.pack_sequence()\\nfor details.\\nhx\\n: tensor of shape\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu'),\n",
       " Document(metadata={}, page_content='(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nfor unbatched input or\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, N, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ncontaining the initial hidden\\nstate for the input sequence batch. Defaults to zeros if not provided.\\nwhere:\\nN\\n=\\nbatch\\xa0size\\nL\\n=\\nsequence\\xa0length\\nD\\n=\\n2\\nif\\xa0bidirectional=True\\xa0otherwise\\n1\\nH\\ni\\nn\\n=\\ninput_size\\nH\\no\\nu\\nt\\n=\\nhidden_size\\n\\\\begin{aligned}\\n    N ={} & \\\\text{batch size} \\\\\\\\\\n    L ={} & \\\\text{sequence length} \\\\\\\\'),\n",
       " Document(metadata={}, page_content='L ={} & \\\\text{sequence length} \\\\\\\\\\n    D ={} & 2 \\\\text{ if bidirectional=True otherwise } 1 \\\\\\\\\\n    H_{in} ={} & \\\\text{input\\\\_size} \\\\\\\\\\n    H_{out} ={} & \\\\text{hidden\\\\_size}\\n\\\\end{aligned}\\nN\\n=\\nL\\n=\\nD\\n=\\nH\\nin\\n\\u200b\\n=\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n\\u200b\\nbatch\\xa0size\\nsequence\\xa0length\\n2\\nif\\xa0bidirectional=True\\xa0otherwise\\n1\\ninput_size\\nhidden_size\\n\\u200b\\nOutputs: output, h_n\\noutput\\n: tensor of shape\\n(\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)\\n(L, D * H_{out})\\n(\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nL\\n,\\nN\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)\\n(L, N, D * H_{out})\\n(\\nL\\n,\\nN\\n,\\nD\\n∗\\nH\\no'),\n",
       " Document(metadata={}, page_content='∗\\nH\\no\\nu\\nt\\n)\\n(L, N, D * H_{out})\\n(\\nL\\n,\\nN\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)\\n(N, L, D * H_{out})\\n(\\nN\\n,\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nwhen\\nbatch_first=True\\ncontaining the output features\\n(h_t)\\nfrom the last layer of the RNN, for each\\nt\\n. If a\\ntorch.nn.utils.rnn.PackedSequence\\nhas been given as the input, the output\\nwill also be a packed sequence.\\nh_n\\n: tensor of shape\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)'),\n",
       " Document(metadata={}, page_content='(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nfor unbatched input or\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, N, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ncontaining the final hidden state\\nfor each element in the batch.\\nVariables\\nweight_ih_l[k]\\n– the learnable input-hidden weights of the k-th layer,\\nof shape\\n(hidden_size, input_size)\\nfor\\nk = 0\\n. Otherwise, the shape is\\n(hidden_size, num_directions * hidden_size)\\nweight_hh_l[k]\\n– the learnable hidden-hidden weights of the k-th layer,'),\n",
       " Document(metadata={}, page_content='of shape\\n(hidden_size, hidden_size)\\nbias_ih_l[k]\\n– the learnable input-hidden bias of the k-th layer,\\nof shape\\n(hidden_size)\\nbias_hh_l[k]\\n– the learnable hidden-hidden bias of the k-th layer,\\nof shape\\n(hidden_size)\\nNote\\nAll the weights and biases are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\n1\\nhidden_size\\nk = \\\\frac{1}{\\\\text{hidden\\\\_size}}\\nk\\n=\\nhidden_size\\n1\\n\\u200b\\nNote\\nFor bidirectional RNNs, forward and backward are directions 0 and 1 respectively.'),\n",
       " Document(metadata={}, page_content='Example of splitting the output layers when\\nbatch_first=False\\n:\\noutput.view(seq_len,\\nbatch,\\nnum_directions,\\nhidden_size)\\n.\\nNote\\nbatch_first\\nargument is ignored for unbatched inputs.\\nWarning\\nThere are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA.\\nYou can enforce deterministic behavior by setting the following environment variables:\\nOn CUDA 10.1, set environment variable\\nCUDA_LAUNCH_BLOCKING=1\\n.\\nThis may affect performance.'),\n",
       " Document(metadata={}, page_content='.\\nThis may affect performance.\\nOn CUDA 10.2 or later, set environment variable\\n(note the leading colon symbol)\\nCUBLAS_WORKSPACE_CONFIG=:16:8\\nor\\nCUBLAS_WORKSPACE_CONFIG=:4096:2\\n.\\nSee the\\ncuDNN 8 Release Notes\\nfor more information.\\nNote\\nIf the following conditions are satisfied:\\n1) cudnn is enabled,\\n2) input data is on the GPU\\n3) input data has dtype\\ntorch.float16\\n4) V100 GPU is used,\\n5) input data is not in\\nPackedSequence\\nformat\\npersistent algorithm can be selected to improve performance.'),\n",
       " Document(metadata={}, page_content='Examples:\\n>>>\\nrnn\\n=\\nnn\\n.\\nRNN\\n(\\n10\\n,\\n20\\n,\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n3\\n,\\n10\\n)\\n>>>\\nh0\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n,\\n20\\n)\\n>>>\\noutput\\n,\\nhn\\n=\\nrnn\\n(\\ninput\\n,\\nh0\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LSTM\\n¶\\nclass\\ntorch.nn.\\nLSTM\\n(\\ninput_size\\n,\\nhidden_size\\n,\\nnum_layers\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\nbatch_first\\n=\\nFalse\\n,\\ndropout\\n=\\n0.0\\n,\\nbidirectional\\n=\\nFalse\\n,\\nproj_size\\n=\\n0\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApply a multi-layer long short-term memory (LSTM) RNN to an input sequence.\\nFor each element in the input sequence, each layer computes the following\\nfunction:\\ni\\nt\\n=\\nσ\\n(\\nW\\ni\\ni\\nx\\nt\\n+\\nb\\ni\\ni\\n+\\nW\\nh\\ni\\nh\\nt\\n−\\n1\\n+\\nb\\nh\\ni\\n)\\nf\\nt\\n=\\nσ\\n(\\nW\\ni\\nf\\nx\\nt\\n+\\nb\\ni\\nf\\n+\\nW\\nh\\nf\\nh\\nt\\n−\\n1\\n+\\nb\\nh\\nf\\n)\\ng\\nt\\n=\\ntanh\\n\\u2061\\n('),\n",
       " Document(metadata={}, page_content='+\\nb\\ni\\nf\\n+\\nW\\nh\\nf\\nh\\nt\\n−\\n1\\n+\\nb\\nh\\nf\\n)\\ng\\nt\\n=\\ntanh\\n\\u2061\\n(\\nW\\ni\\ng\\nx\\nt\\n+\\nb\\ni\\ng\\n+\\nW\\nh\\ng\\nh\\nt\\n−\\n1\\n+\\nb\\nh\\ng\\n)\\no\\nt\\n=\\nσ\\n(\\nW\\ni\\no\\nx\\nt\\n+\\nb\\ni\\no\\n+\\nW\\nh\\no\\nh\\nt\\n−\\n1\\n+\\nb\\nh\\no\\n)\\nc\\nt\\n=\\nf\\nt\\n⊙\\nc\\nt\\n−\\n1\\n+\\ni\\nt\\n⊙\\ng\\nt\\nh\\nt\\n=\\no\\nt\\n⊙\\ntanh\\n\\u2061\\n(\\nc\\nt\\n)\\n\\\\begin{array}{ll} \\\\\\\\\\n    i_t = \\\\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\\\\\\n    f_t = \\\\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\\\\\\n    g_t = \\\\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\\\\\\n    o_t = \\\\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\\\\'),\n",
       " Document(metadata={}, page_content='c_t = f_t \\\\odot c_{t-1} + i_t \\\\odot g_t \\\\\\\\\\n    h_t = o_t \\\\odot \\\\tanh(c_t) \\\\\\\\\\n\\\\end{array}\\ni\\nt\\n\\u200b\\n=\\nσ\\n(\\nW\\nii\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\nii\\n\\u200b\\n+\\nW\\nhi\\n\\u200b\\nh\\nt\\n−\\n1\\n\\u200b\\n+\\nb\\nhi\\n\\u200b\\n)\\nf\\nt\\n\\u200b\\n=\\nσ\\n(\\nW\\ni\\nf\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\ni\\nf\\n\\u200b\\n+\\nW\\nh\\nf\\n\\u200b\\nh\\nt\\n−\\n1\\n\\u200b\\n+\\nb\\nh\\nf\\n\\u200b\\n)\\ng\\nt\\n\\u200b\\n=\\ntanh\\n(\\nW\\ni\\ng\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\ni\\ng\\n\\u200b\\n+\\nW\\nh\\ng\\n\\u200b\\nh\\nt\\n−\\n1\\n\\u200b\\n+\\nb\\nh\\ng\\n\\u200b\\n)\\no\\nt\\n\\u200b\\n=\\nσ\\n(\\nW\\ni\\no\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\ni\\no\\n\\u200b\\n+\\nW\\nh\\no\\n\\u200b\\nh\\nt\\n−\\n1\\n\\u200b\\n+\\nb\\nh\\no\\n\\u200b\\n)\\nc\\nt\\n\\u200b\\n=\\nf\\nt\\n\\u200b\\n⊙\\nc\\nt\\n−\\n1\\n\\u200b\\n+\\ni\\nt\\n\\u200b\\n⊙\\ng\\nt\\n\\u200b\\nh\\nt\\n\\u200b\\n=\\no\\nt\\n\\u200b\\n⊙\\ntanh\\n(\\nc\\nt\\n\\u200b\\n)\\n\\u200b\\nwhere\\nh\\nt\\nh_t\\nh\\nt\\n\\u200b\\nis the hidden state at time\\nt\\n,\\nc\\nt\\nc_t'),\n",
       " Document(metadata={}, page_content='h_t\\nh\\nt\\n\\u200b\\nis the hidden state at time\\nt\\n,\\nc\\nt\\nc_t\\nc\\nt\\n\\u200b\\nis the cell\\nstate at time\\nt\\n,\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis the input at time\\nt\\n,\\nh\\nt\\n−\\n1\\nh_{t-1}\\nh\\nt\\n−\\n1\\n\\u200b\\nis the hidden state of the layer at time\\nt-1\\nor the initial hidden\\nstate at time\\n0\\n, and\\ni\\nt\\ni_t\\ni\\nt\\n\\u200b\\n,\\nf\\nt\\nf_t\\nf\\nt\\n\\u200b\\n,\\ng\\nt\\ng_t\\ng\\nt\\n\\u200b\\n,\\no\\nt\\no_t\\no\\nt\\n\\u200b\\nare the input, forget, cell, and output gates, respectively.\\nσ\\n\\\\sigma\\nσ\\nis the sigmoid function, and\\n⊙\\n\\\\odot\\n⊙\\nis the Hadamard product.\\nIn a multilayer LSTM, the input\\nx\\nt\\n(\\nl\\n)\\nx^{(l)}_t\\nx\\nt\\n(\\nl\\n)'),\n",
       " Document(metadata={}, page_content='x\\nt\\n(\\nl\\n)\\nx^{(l)}_t\\nx\\nt\\n(\\nl\\n)\\n\\u200b\\nof the\\nl\\nl\\nl\\n-th layer\\n(\\nl\\n≥\\n2\\nl \\\\ge 2\\nl\\n≥\\n2\\n) is the hidden state\\nh\\nt\\n(\\nl\\n−\\n1\\n)\\nh^{(l-1)}_t\\nh\\nt\\n(\\nl\\n−\\n1\\n)\\n\\u200b\\nof the previous layer multiplied by\\ndropout\\nδ\\nt\\n(\\nl\\n−\\n1\\n)\\n\\\\delta^{(l-1)}_t\\nδ\\nt\\n(\\nl\\n−\\n1\\n)\\n\\u200b\\nwhere each\\nδ\\nt\\n(\\nl\\n−\\n1\\n)\\n\\\\delta^{(l-1)}_t\\nδ\\nt\\n(\\nl\\n−\\n1\\n)\\n\\u200b\\nis a Bernoulli random\\nvariable which is\\n0\\n0\\n0\\nwith probability\\ndropout\\n.\\nIf\\nproj_size\\n>\\n0\\nis specified, LSTM with projections will be used. This changes'),\n",
       " Document(metadata={}, page_content='the LSTM cell in the following way. First, the dimension of\\nh\\nt\\nh_t\\nh\\nt\\n\\u200b\\nwill be changed from\\nhidden_size\\nto\\nproj_size\\n(dimensions of\\nW\\nh\\ni\\nW_{hi}\\nW\\nhi\\n\\u200b\\nwill be changed accordingly).\\nSecond, the output hidden state of each layer will be multiplied by a learnable projection\\nmatrix:\\nh\\nt\\n=\\nW\\nh\\nr\\nh\\nt\\nh_t = W_{hr}h_t\\nh\\nt\\n\\u200b\\n=\\nW\\nh\\nr\\n\\u200b\\nh\\nt\\n\\u200b\\n. Note that as a consequence of this, the output\\nof LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact'),\n",
       " Document(metadata={}, page_content='dimensions of all variables. You can find more details in\\nhttps://arxiv.org/abs/1402.1128\\n.\\nParameters\\ninput_size\\n– The number of expected features in the input\\nx\\nhidden_size\\n– The number of features in the hidden state\\nh\\nnum_layers\\n– Number of recurrent layers. E.g., setting\\nnum_layers=2\\nwould mean stacking two LSTMs together to form a\\nstacked LSTM\\n,\\nwith the second LSTM taking in outputs of the first LSTM and\\ncomputing the final results. Default: 1\\nbias\\n– If\\nFalse'),\n",
       " Document(metadata={}, page_content='bias\\n– If\\nFalse\\n, then the layer does not use bias weights\\nb_ih\\nand\\nb_hh\\n.\\nDefault:\\nTrue\\nbatch_first\\n– If\\nTrue\\n, then the input and output tensors are provided\\nas\\n(batch, seq, feature)\\ninstead of\\n(seq, batch, feature)\\n.\\nNote that this does not apply to hidden or cell states. See the\\nInputs/Outputs sections below for details.  Default:\\nFalse\\ndropout\\n– If non-zero, introduces a\\nDropout\\nlayer on the outputs of each\\nLSTM layer except the last layer, with dropout probability equal to\\ndropout'),\n",
       " Document(metadata={}, page_content='dropout\\n. Default: 0\\nbidirectional\\n– If\\nTrue\\n, becomes a bidirectional LSTM. Default:\\nFalse\\nproj_size\\n– If\\n>\\n0\\n, will use LSTM with projections of corresponding size. Default: 0\\nInputs: input, (h_0, c_0)\\ninput\\n: tensor of shape\\n(\\nL\\n,\\nH\\ni\\nn\\n)\\n(L, H_{in})\\n(\\nL\\n,\\nH\\nin\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nL\\n,\\nN\\n,\\nH\\ni\\nn\\n)\\n(L, N, H_{in})\\n(\\nL\\n,\\nN\\n,\\nH\\nin\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nL\\n,\\nH\\ni\\nn\\n)\\n(N, L, H_{in})\\n(\\nN\\n,\\nL\\n,\\nH\\nin\\n\\u200b\\n)\\nwhen\\nbatch_first=True\\ncontaining the features of'),\n",
       " Document(metadata={}, page_content='when\\nbatch_first=True\\ncontaining the features of\\nthe input sequence.  The input can also be a packed variable length sequence.\\nSee\\ntorch.nn.utils.rnn.pack_padded_sequence()\\nor\\ntorch.nn.utils.rnn.pack_sequence()\\nfor details.\\nh_0\\n: tensor of shape\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nfor unbatched input or\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, N, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ncontaining the'),\n",
       " Document(metadata={}, page_content='(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ncontaining the\\ninitial hidden state for each element in the input sequence.\\nDefaults to zeros if (h_0, c_0) is not provided.\\nc_0\\n: tensor of shape\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\nc\\ne\\nl\\nl\\n)\\n(D * \\\\text{num\\\\_layers}, H_{cell})\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\nce\\nll\\n\\u200b\\n)\\nfor unbatched input or\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\nc\\ne\\nl\\nl\\n)\\n(D * \\\\text{num\\\\_layers}, N, H_{cell})\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\nce\\nll\\n\\u200b\\n)\\ncontaining the\\ninitial cell state for each element in the input sequence.'),\n",
       " Document(metadata={}, page_content='Defaults to zeros if (h_0, c_0) is not provided.\\nwhere:\\nN\\n=\\nbatch\\xa0size\\nL\\n=\\nsequence\\xa0length\\nD\\n=\\n2\\nif\\xa0bidirectional=True\\xa0otherwise\\n1\\nH\\ni\\nn\\n=\\ninput_size\\nH\\nc\\ne\\nl\\nl\\n=\\nhidden_size\\nH\\no\\nu\\nt\\n=\\nproj_size\\xa0if\\xa0proj_size\\n>\\n0\\notherwise\\xa0hidden_size\\n\\\\begin{aligned}\\n    N ={} & \\\\text{batch size} \\\\\\\\\\n    L ={} & \\\\text{sequence length} \\\\\\\\\\n    D ={} & 2 \\\\text{ if bidirectional=True otherwise } 1 \\\\\\\\\\n    H_{in} ={} & \\\\text{input\\\\_size} \\\\\\\\\\n    H_{cell} ={} & \\\\text{hidden\\\\_size} \\\\\\\\'),\n",
       " Document(metadata={}, page_content='H_{cell} ={} & \\\\text{hidden\\\\_size} \\\\\\\\\\n    H_{out} ={} & \\\\text{proj\\\\_size if } \\\\text{proj\\\\_size}>0 \\\\text{ otherwise hidden\\\\_size} \\\\\\\\\\n\\\\end{aligned}\\nN\\n=\\nL\\n=\\nD\\n=\\nH\\nin\\n\\u200b\\n=\\nH\\nce\\nll\\n\\u200b\\n=\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n\\u200b\\nbatch\\xa0size\\nsequence\\xa0length\\n2\\nif\\xa0bidirectional=True\\xa0otherwise\\n1\\ninput_size\\nhidden_size\\nproj_size\\xa0if\\nproj_size\\n>\\n0\\notherwise\\xa0hidden_size\\n\\u200b\\nOutputs: output, (h_n, c_n)\\noutput\\n: tensor of shape\\n(\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)\\n(L, D * H_{out})\\n(\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nL\\n,\\nN\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)'),\n",
       " Document(metadata={}, page_content='\\u200b\\n)\\nfor unbatched input,\\n(\\nL\\n,\\nN\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)\\n(L, N, D * H_{out})\\n(\\nL\\n,\\nN\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)\\n(N, L, D * H_{out})\\n(\\nN\\n,\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nwhen\\nbatch_first=True\\ncontaining the output features\\n(h_t)\\nfrom the last layer of the LSTM, for each\\nt\\n. If a\\ntorch.nn.utils.rnn.PackedSequence\\nhas been given as the input, the output\\nwill also be a packed sequence. When\\nbidirectional=True\\n,\\noutput\\nwill contain'),\n",
       " Document(metadata={}, page_content='bidirectional=True\\n,\\noutput\\nwill contain\\na concatenation of the forward and reverse hidden states at each time step in the sequence.\\nh_n\\n: tensor of shape\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nfor unbatched input or\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, N, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ncontaining the\\nfinal hidden state for each element in the sequence. When\\nbidirectional=True\\n,\\nh_n'),\n",
       " Document(metadata={}, page_content='bidirectional=True\\n,\\nh_n\\nwill contain a concatenation of the final forward and reverse hidden states, respectively.\\nc_n\\n: tensor of shape\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\nc\\ne\\nl\\nl\\n)\\n(D * \\\\text{num\\\\_layers}, H_{cell})\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\nce\\nll\\n\\u200b\\n)\\nfor unbatched input or\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\nc\\ne\\nl\\nl\\n)\\n(D * \\\\text{num\\\\_layers}, N, H_{cell})\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\nce\\nll\\n\\u200b\\n)\\ncontaining the\\nfinal cell state for each element in the sequence. When\\nbidirectional=True\\n,\\nc_n'),\n",
       " Document(metadata={}, page_content='bidirectional=True\\n,\\nc_n\\nwill contain a concatenation of the final forward and reverse cell states, respectively.\\nVariables\\nweight_ih_l[k]\\n– the learnable input-hidden weights of the\\nk\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer\\n(W_ii|W_if|W_ig|W_io)\\n, of shape\\n(4*hidden_size, input_size)\\nfor\\nk = 0\\n.\\nOtherwise, the shape is\\n(4*hidden_size, num_directions * hidden_size)\\n. If\\nproj_size\\n>\\n0\\nwas specified, the shape will be\\n(4*hidden_size, num_directions * proj_size)\\nfor\\nk > 0\\nweight_hh_l[k]'),\n",
       " Document(metadata={}, page_content='for\\nk > 0\\nweight_hh_l[k]\\n– the learnable hidden-hidden weights of the\\nk\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer\\n(W_hi|W_hf|W_hg|W_ho)\\n, of shape\\n(4*hidden_size, hidden_size)\\n. If\\nproj_size\\n>\\n0\\nwas specified, the shape will be\\n(4*hidden_size, proj_size)\\n.\\nbias_ih_l[k]\\n– the learnable input-hidden bias of the\\nk\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer\\n(b_ii|b_if|b_ig|b_io)\\n, of shape\\n(4*hidden_size)\\nbias_hh_l[k]\\n– the learnable hidden-hidden bias of the\\nk\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer\\n(b_hi|b_hf|b_hg|b_ho)\\n, of shape'),\n",
       " Document(metadata={}, page_content='k\\nt\\nh\\nlayer\\n(b_hi|b_hf|b_hg|b_ho)\\n, of shape\\n(4*hidden_size)\\nweight_hr_l[k]\\n– the learnable projection weights of the\\nk\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer\\nof shape\\n(proj_size, hidden_size)\\n. Only present when\\nproj_size\\n>\\n0\\nwas\\nspecified.\\nweight_ih_l[k]_reverse\\n– Analogous to\\nweight_ih_l[k]\\nfor the reverse direction.\\nOnly present when\\nbidirectional=True\\n.\\nweight_hh_l[k]_reverse\\n– Analogous to\\nweight_hh_l[k]\\nfor the reverse direction.\\nOnly present when\\nbidirectional=True\\n.\\nbias_ih_l[k]_reverse'),\n",
       " Document(metadata={}, page_content='bidirectional=True\\n.\\nbias_ih_l[k]_reverse\\n– Analogous to\\nbias_ih_l[k]\\nfor the reverse direction.\\nOnly present when\\nbidirectional=True\\n.\\nbias_hh_l[k]_reverse\\n– Analogous to\\nbias_hh_l[k]\\nfor the reverse direction.\\nOnly present when\\nbidirectional=True\\n.\\nweight_hr_l[k]_reverse\\n– Analogous to\\nweight_hr_l[k]\\nfor the reverse direction.\\nOnly present when\\nbidirectional=True\\nand\\nproj_size\\n>\\n0\\nwas specified.\\nNote\\nAll the weights and biases are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)'),\n",
       " Document(metadata={}, page_content='U\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\n1\\nhidden_size\\nk = \\\\frac{1}{\\\\text{hidden\\\\_size}}\\nk\\n=\\nhidden_size\\n1\\n\\u200b\\nNote\\nFor bidirectional LSTMs, forward and backward are directions 0 and 1 respectively.\\nExample of splitting the output layers when\\nbatch_first=False\\n:\\noutput.view(seq_len,\\nbatch,\\nnum_directions,\\nhidden_size)\\n.\\nNote\\nFor bidirectional LSTMs,\\nh_n\\nis not equivalent to the last element of\\noutput\\n; the'),\n",
       " Document(metadata={}, page_content='output\\n; the\\nformer contains the final forward and reverse hidden states, while the latter contains the\\nfinal forward hidden state and the initial reverse hidden state.\\nNote\\nbatch_first\\nargument is ignored for unbatched inputs.\\nNote\\nproj_size\\nshould be smaller than\\nhidden_size\\n.\\nWarning\\nThere are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA.\\nYou can enforce deterministic behavior by setting the following environment variables:'),\n",
       " Document(metadata={}, page_content='On CUDA 10.1, set environment variable\\nCUDA_LAUNCH_BLOCKING=1\\n.\\nThis may affect performance.\\nOn CUDA 10.2 or later, set environment variable\\n(note the leading colon symbol)\\nCUBLAS_WORKSPACE_CONFIG=:16:8\\nor\\nCUBLAS_WORKSPACE_CONFIG=:4096:2\\n.\\nSee the\\ncuDNN 8 Release Notes\\nfor more information.\\nNote\\nIf the following conditions are satisfied:\\n1) cudnn is enabled,\\n2) input data is on the GPU\\n3) input data has dtype\\ntorch.float16\\n4) V100 GPU is used,\\n5) input data is not in\\nPackedSequence\\nformat'),\n",
       " Document(metadata={}, page_content='5) input data is not in\\nPackedSequence\\nformat\\npersistent algorithm can be selected to improve performance.\\nExamples:\\n>>>\\nrnn\\n=\\nnn\\n.\\nLSTM\\n(\\n10\\n,\\n20\\n,\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n3\\n,\\n10\\n)\\n>>>\\nh0\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n,\\n20\\n)\\n>>>\\nc0\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n,\\n20\\n)\\n>>>\\noutput\\n,\\n(\\nhn\\n,\\ncn\\n)\\n=\\nrnn\\n(\\ninput\\n,\\n(\\nh0\\n,\\nc0\\n))\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='GRU\\n¶\\nclass\\ntorch.nn.\\nGRU\\n(\\ninput_size\\n,\\nhidden_size\\n,\\nnum_layers\\n=\\n1\\n,\\nbias\\n=\\nTrue\\n,\\nbatch_first\\n=\\nFalse\\n,\\ndropout\\n=\\n0.0\\n,\\nbidirectional\\n=\\nFalse\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\\nFor each element in the input sequence, each layer computes the following\\nfunction:\\nr\\nt\\n=\\nσ\\n(\\nW\\ni\\nr\\nx\\nt\\n+\\nb\\ni\\nr\\n+\\nW\\nh\\nr\\nh\\n(\\nt\\n−\\n1\\n)\\n+\\nb\\nh\\nr\\n)\\nz\\nt\\n=\\nσ\\n(\\nW\\ni\\nz\\nx\\nt\\n+\\nb\\ni\\nz\\n+\\nW\\nh\\nz\\nh\\n(\\nt\\n−\\n1\\n)\\n+\\nb\\nh\\nz\\n)\\nn\\nt\\n=\\ntanh\\n\\u2061\\n(\\nW\\ni\\nn\\nx\\nt\\n+\\nb'),\n",
       " Document(metadata={}, page_content='(\\nt\\n−\\n1\\n)\\n+\\nb\\nh\\nz\\n)\\nn\\nt\\n=\\ntanh\\n\\u2061\\n(\\nW\\ni\\nn\\nx\\nt\\n+\\nb\\ni\\nn\\n+\\nr\\nt\\n⊙\\n(\\nW\\nh\\nn\\nh\\n(\\nt\\n−\\n1\\n)\\n+\\nb\\nh\\nn\\n)\\n)\\nh\\nt\\n=\\n(\\n1\\n−\\nz\\nt\\n)\\n⊙\\nn\\nt\\n+\\nz\\nt\\n⊙\\nh\\n(\\nt\\n−\\n1\\n)\\n\\\\begin{array}{ll}\\n    r_t = \\\\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\\\\\\n    z_t = \\\\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\\\\\\n    n_t = \\\\tanh(W_{in} x_t + b_{in} + r_t \\\\odot (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\\\\\\n    h_t = (1 - z_t) \\\\odot n_t + z_t \\\\odot h_{(t-1)}\\n\\\\end{array}\\nr\\nt\\n\\u200b\\n=\\nσ\\n(\\nW\\ni\\nr\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\ni\\nr\\n\\u200b\\n+\\nW\\nh\\nr\\n\\u200b\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\n+\\nb'),\n",
       " Document(metadata={}, page_content='i\\nr\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\ni\\nr\\n\\u200b\\n+\\nW\\nh\\nr\\n\\u200b\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\n+\\nb\\nh\\nr\\n\\u200b\\n)\\nz\\nt\\n\\u200b\\n=\\nσ\\n(\\nW\\ni\\nz\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\ni\\nz\\n\\u200b\\n+\\nW\\nh\\nz\\n\\u200b\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\n+\\nb\\nh\\nz\\n\\u200b\\n)\\nn\\nt\\n\\u200b\\n=\\ntanh\\n(\\nW\\nin\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\nin\\n\\u200b\\n+\\nr\\nt\\n\\u200b\\n⊙\\n(\\nW\\nhn\\n\\u200b\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\n+\\nb\\nhn\\n\\u200b\\n))\\nh\\nt\\n\\u200b\\n=\\n(\\n1\\n−\\nz\\nt\\n\\u200b\\n)\\n⊙\\nn\\nt\\n\\u200b\\n+\\nz\\nt\\n\\u200b\\n⊙\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\n\\u200b\\nwhere\\nh\\nt\\nh_t\\nh\\nt\\n\\u200b\\nis the hidden state at time\\nt\\n,\\nx\\nt\\nx_t\\nx\\nt\\n\\u200b\\nis the input\\nat time\\nt\\n,\\nh\\n(\\nt\\n−\\n1\\n)\\nh_{(t-1)}\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\nis the hidden state of the layer\\nat time\\nt-1\\nor the initial hidden state at time\\n0\\n, and\\nr\\nt\\nr_t\\nr\\nt\\n\\u200b\\n,'),\n",
       " Document(metadata={}, page_content='0\\n, and\\nr\\nt\\nr_t\\nr\\nt\\n\\u200b\\n,\\nz\\nt\\nz_t\\nz\\nt\\n\\u200b\\n,\\nn\\nt\\nn_t\\nn\\nt\\n\\u200b\\nare the reset, update, and new gates, respectively.\\nσ\\n\\\\sigma\\nσ\\nis the sigmoid function, and\\n⊙\\n\\\\odot\\n⊙\\nis the Hadamard product.\\nIn a multilayer GRU, the input\\nx\\nt\\n(\\nl\\n)\\nx^{(l)}_t\\nx\\nt\\n(\\nl\\n)\\n\\u200b\\nof the\\nl\\nl\\nl\\n-th layer\\n(\\nl\\n≥\\n2\\nl \\\\ge 2\\nl\\n≥\\n2\\n) is the hidden state\\nh\\nt\\n(\\nl\\n−\\n1\\n)\\nh^{(l-1)}_t\\nh\\nt\\n(\\nl\\n−\\n1\\n)\\n\\u200b\\nof the previous layer multiplied by\\ndropout\\nδ\\nt\\n(\\nl\\n−\\n1\\n)\\n\\\\delta^{(l-1)}_t\\nδ\\nt\\n(\\nl\\n−\\n1\\n)\\n\\u200b\\nwhere each\\nδ\\nt\\n(\\nl\\n−\\n1\\n)\\n\\\\delta^{(l-1)}_t\\nδ\\nt\\n(\\nl\\n−\\n1\\n)'),\n",
       " Document(metadata={}, page_content='δ\\nt\\n(\\nl\\n−\\n1\\n)\\n\\\\delta^{(l-1)}_t\\nδ\\nt\\n(\\nl\\n−\\n1\\n)\\n\\u200b\\nis a Bernoulli random\\nvariable which is\\n0\\n0\\n0\\nwith probability\\ndropout\\n.\\nParameters\\ninput_size\\n– The number of expected features in the input\\nx\\nhidden_size\\n– The number of features in the hidden state\\nh\\nnum_layers\\n– Number of recurrent layers. E.g., setting\\nnum_layers=2\\nwould mean stacking two GRUs together to form a\\nstacked GRU\\n,\\nwith the second GRU taking in outputs of the first GRU and\\ncomputing the final results. Default: 1\\nbias\\n– If\\nFalse'),\n",
       " Document(metadata={}, page_content='bias\\n– If\\nFalse\\n, then the layer does not use bias weights\\nb_ih\\nand\\nb_hh\\n.\\nDefault:\\nTrue\\nbatch_first\\n– If\\nTrue\\n, then the input and output tensors are provided\\nas\\n(batch, seq, feature)\\ninstead of\\n(seq, batch, feature)\\n.\\nNote that this does not apply to hidden or cell states. See the\\nInputs/Outputs sections below for details.  Default:\\nFalse\\ndropout\\n– If non-zero, introduces a\\nDropout\\nlayer on the outputs of each\\nGRU layer except the last layer, with dropout probability equal to\\ndropout'),\n",
       " Document(metadata={}, page_content='dropout\\n. Default: 0\\nbidirectional\\n– If\\nTrue\\n, becomes a bidirectional GRU. Default:\\nFalse\\nInputs: input, h_0\\ninput\\n: tensor of shape\\n(\\nL\\n,\\nH\\ni\\nn\\n)\\n(L, H_{in})\\n(\\nL\\n,\\nH\\nin\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nL\\n,\\nN\\n,\\nH\\ni\\nn\\n)\\n(L, N, H_{in})\\n(\\nL\\n,\\nN\\n,\\nH\\nin\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nL\\n,\\nH\\ni\\nn\\n)\\n(N, L, H_{in})\\n(\\nN\\n,\\nL\\n,\\nH\\nin\\n\\u200b\\n)\\nwhen\\nbatch_first=True\\ncontaining the features of\\nthe input sequence.  The input can also be a packed variable length sequence.\\nSee\\ntorch.nn.utils.rnn.pack_padded_sequence()'),\n",
       " Document(metadata={}, page_content='See\\ntorch.nn.utils.rnn.pack_padded_sequence()\\nor\\ntorch.nn.utils.rnn.pack_sequence()\\nfor details.\\nh_0\\n: tensor of shape\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, N, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ncontaining the initial hidden state for the input sequence. Defaults to zeros if not provided.\\nwhere:\\nN\\n=\\nbatch\\xa0size\\nL\\n=\\nsequence\\xa0length\\nD\\n=\\n2\\nif\\xa0bidirectional=True\\xa0otherwise\\n1\\nH'),\n",
       " Document(metadata={}, page_content='D\\n=\\n2\\nif\\xa0bidirectional=True\\xa0otherwise\\n1\\nH\\ni\\nn\\n=\\ninput_size\\nH\\no\\nu\\nt\\n=\\nhidden_size\\n\\\\begin{aligned}\\n    N ={} & \\\\text{batch size} \\\\\\\\\\n    L ={} & \\\\text{sequence length} \\\\\\\\\\n    D ={} & 2 \\\\text{ if bidirectional=True otherwise } 1 \\\\\\\\\\n    H_{in} ={} & \\\\text{input\\\\_size} \\\\\\\\\\n    H_{out} ={} & \\\\text{hidden\\\\_size}\\n\\\\end{aligned}\\nN\\n=\\nL\\n=\\nD\\n=\\nH\\nin\\n\\u200b\\n=\\nH\\no\\nu\\nt\\n\\u200b\\n=\\n\\u200b\\nbatch\\xa0size\\nsequence\\xa0length\\n2\\nif\\xa0bidirectional=True\\xa0otherwise\\n1\\ninput_size\\nhidden_size\\n\\u200b\\nOutputs: output, h_n\\noutput\\n: tensor of shape\\n(\\nL\\n,\\nD\\n∗\\nH'),\n",
       " Document(metadata={}, page_content='output\\n: tensor of shape\\n(\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)\\n(L, D * H_{out})\\n(\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nfor unbatched input,\\n(\\nL\\n,\\nN\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)\\n(L, N, D * H_{out})\\n(\\nL\\n,\\nN\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nwhen\\nbatch_first=False\\nor\\n(\\nN\\n,\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n)\\n(N, L, D * H_{out})\\n(\\nN\\n,\\nL\\n,\\nD\\n∗\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nwhen\\nbatch_first=True\\ncontaining the output features\\n(h_t)\\nfrom the last layer of the GRU, for each\\nt\\n. If a\\ntorch.nn.utils.rnn.PackedSequence\\nhas been given as the input, the output\\nwill also be a packed sequence.\\nh_n'),\n",
       " Document(metadata={}, page_content='will also be a packed sequence.\\nh_n\\n: tensor of shape\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(D * \\\\text{num\\\\_layers}, N, H_{out})\\n(\\nD\\n∗\\nnum_layers\\n,\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ncontaining the final hidden state\\nfor the input sequence.\\nVariables\\nweight_ih_l[k]\\n– the learnable input-hidden weights of the\\nk\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer\\n(W_ir|W_iz|W_in), of shape\\n(3*hidden_size, input_size)\\nfor\\nk = 0\\n.'),\n",
       " Document(metadata={}, page_content='(3*hidden_size, input_size)\\nfor\\nk = 0\\n.\\nOtherwise, the shape is\\n(3*hidden_size, num_directions * hidden_size)\\nweight_hh_l[k]\\n– the learnable hidden-hidden weights of the\\nk\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer\\n(W_hr|W_hz|W_hn), of shape\\n(3*hidden_size, hidden_size)\\nbias_ih_l[k]\\n– the learnable input-hidden bias of the\\nk\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer\\n(b_ir|b_iz|b_in), of shape\\n(3*hidden_size)\\nbias_hh_l[k]\\n– the learnable hidden-hidden bias of the\\nk\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer'),\n",
       " Document(metadata={}, page_content='k\\nt\\nh\\n\\\\text{k}^{th}\\nk\\nt\\nh\\nlayer\\n(b_hr|b_hz|b_hn), of shape\\n(3*hidden_size)\\nNote\\nAll the weights and biases are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\n1\\nhidden_size\\nk = \\\\frac{1}{\\\\text{hidden\\\\_size}}\\nk\\n=\\nhidden_size\\n1\\n\\u200b\\nNote\\nFor bidirectional GRUs, forward and backward are directions 0 and 1 respectively.\\nExample of splitting the output layers when\\nbatch_first=False\\n:\\noutput.view(seq_len,\\nbatch,\\nnum_directions,\\nhidden_size)\\n.\\nNote\\nbatch_first'),\n",
       " Document(metadata={}, page_content='num_directions,\\nhidden_size)\\n.\\nNote\\nbatch_first\\nargument is ignored for unbatched inputs.\\nNote\\nThe calculation of new gate\\nn\\nt\\nn_t\\nn\\nt\\n\\u200b\\nsubtly differs from the original paper and other frameworks.\\nIn the original implementation, the Hadamard product\\n(\\n⊙\\n)\\n(\\\\odot)\\n(\\n⊙\\n)\\nbetween\\nr\\nt\\nr_t\\nr\\nt\\n\\u200b\\nand the\\nprevious hidden state\\nh\\n(\\nt\\n−\\n1\\n)\\nh_{(t-1)}\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\nis done before the multiplication with the weight matrix\\nW\\nand addition of bias:\\nn\\nt\\n=\\ntanh\\n\\u2061\\n(\\nW\\ni\\nn\\nx\\nt\\n+\\nb\\ni\\nn\\n+\\nW\\nh\\nn\\n(\\nr\\nt\\n⊙\\nh\\n(\\nt\\n−\\n1'),\n",
       " Document(metadata={}, page_content='\\u2061\\n(\\nW\\ni\\nn\\nx\\nt\\n+\\nb\\ni\\nn\\n+\\nW\\nh\\nn\\n(\\nr\\nt\\n⊙\\nh\\n(\\nt\\n−\\n1\\n)\\n)\\n+\\nb\\nh\\nn\\n)\\n\\\\begin{aligned}\\n    n_t = \\\\tanh(W_{in} x_t + b_{in} + W_{hn} ( r_t \\\\odot h_{(t-1)} ) + b_{hn})\\n\\\\end{aligned}\\nn\\nt\\n\\u200b\\n=\\ntanh\\n(\\nW\\nin\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\nin\\n\\u200b\\n+\\nW\\nhn\\n\\u200b\\n(\\nr\\nt\\n\\u200b\\n⊙\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\n)\\n+\\nb\\nhn\\n\\u200b\\n)\\n\\u200b\\nThis is in contrast to PyTorch implementation, which is done after\\nW\\nh\\nn\\nh\\n(\\nt\\n−\\n1\\n)\\nW_{hn} h_{(t-1)}\\nW\\nhn\\n\\u200b\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\nn\\nt\\n=\\ntanh\\n\\u2061\\n(\\nW\\ni\\nn\\nx\\nt\\n+\\nb\\ni\\nn\\n+\\nr\\nt\\n⊙\\n(\\nW\\nh\\nn\\nh\\n(\\nt\\n−\\n1\\n)\\n+\\nb\\nh\\nn\\n)\\n)\\n\\\\begin{aligned}'),\n",
       " Document(metadata={}, page_content='⊙\\n(\\nW\\nh\\nn\\nh\\n(\\nt\\n−\\n1\\n)\\n+\\nb\\nh\\nn\\n)\\n)\\n\\\\begin{aligned}\\n    n_t = \\\\tanh(W_{in} x_t + b_{in} + r_t \\\\odot (W_{hn} h_{(t-1)}+ b_{hn}))\\n\\\\end{aligned}\\nn\\nt\\n\\u200b\\n=\\ntanh\\n(\\nW\\nin\\n\\u200b\\nx\\nt\\n\\u200b\\n+\\nb\\nin\\n\\u200b\\n+\\nr\\nt\\n\\u200b\\n⊙\\n(\\nW\\nhn\\n\\u200b\\nh\\n(\\nt\\n−\\n1\\n)\\n\\u200b\\n+\\nb\\nhn\\n\\u200b\\n))\\n\\u200b\\nThis implementation differs on purpose for efficiency.\\nNote\\nIf the following conditions are satisfied:\\n1) cudnn is enabled,\\n2) input data is on the GPU\\n3) input data has dtype\\ntorch.float16\\n4) V100 GPU is used,\\n5) input data is not in\\nPackedSequence\\nformat'),\n",
       " Document(metadata={}, page_content='5) input data is not in\\nPackedSequence\\nformat\\npersistent algorithm can be selected to improve performance.\\nExamples:\\n>>>\\nrnn\\n=\\nnn\\n.\\nGRU\\n(\\n10\\n,\\n20\\n,\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n3\\n,\\n10\\n)\\n>>>\\nh0\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n,\\n20\\n)\\n>>>\\noutput\\n,\\nhn\\n=\\nrnn\\n(\\ninput\\n,\\nh0\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"RNNCell\\n¶\\nclass\\ntorch.nn.\\nRNNCell\\n(\\ninput_size\\n,\\nhidden_size\\n,\\nbias\\n=\\nTrue\\n,\\nnonlinearity\\n=\\n'tanh'\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nAn Elman RNN cell with tanh or ReLU non-linearity.\\nh\\n′\\n=\\ntanh\\n\\u2061\\n(\\nW\\ni\\nh\\nx\\n+\\nb\\ni\\nh\\n+\\nW\\nh\\nh\\nh\\n+\\nb\\nh\\nh\\n)\\nh' = \\\\tanh(W_{ih} x + b_{ih}  +  W_{hh} h + b_{hh})\\nh\\n′\\n=\\ntanh\\n(\\nW\\nih\\n\\u200b\\nx\\n+\\nb\\nih\\n\\u200b\\n+\\nW\\nhh\\n\\u200b\\nh\\n+\\nb\\nhh\\n\\u200b\\n)\\nIf\\nnonlinearity\\nis\\n‘relu’\\n, then ReLU is used in place of tanh.\\nParameters\\ninput_size\\n(\\nint\\n) – The number of expected features in the input\"),\n",
       " Document(metadata={}, page_content=\") – The number of expected features in the input\\nx\\nhidden_size\\n(\\nint\\n) – The number of features in the hidden state\\nh\\nbias\\n(\\nbool\\n) – If\\nFalse\\n, then the layer does not use bias weights\\nb_ih\\nand\\nb_hh\\n.\\nDefault:\\nTrue\\nnonlinearity\\n(\\nstr\\n) – The non-linearity to use. Can be either\\n'tanh'\\nor\\n'relu'\\n. Default:\\n'tanh'\\nInputs: input, hidden\\ninput\\n: tensor containing input features\\nhidden\\n: tensor containing the initial hidden state\\nDefaults to zero if not provided.\\nOutputs: h’\\nh’\\nof shape\"),\n",
       " Document(metadata={}, page_content='Outputs: h’\\nh’\\nof shape\\n(batch, hidden_size)\\n: tensor containing the next hidden state\\nfor each element in the batch\\nShape:\\ninput:\\n(\\nN\\n,\\nH\\ni\\nn\\n)\\n(N, H_{in})\\n(\\nN\\n,\\nH\\nin\\n\\u200b\\n)\\nor\\n(\\nH\\ni\\nn\\n)\\n(H_{in})\\n(\\nH\\nin\\n\\u200b\\n)\\ntensor containing input features where\\nH\\ni\\nn\\nH_{in}\\nH\\nin\\n\\u200b\\n=\\ninput_size\\n.\\nhidden:\\n(\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(N, H_{out})\\n(\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nH\\no\\nu\\nt\\n)\\n(H_{out})\\n(\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ntensor containing the initial hidden\\nstate where\\nH\\no\\nu\\nt\\nH_{out}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nhidden_size\\n. Defaults to zero if not provided.'),\n",
       " Document(metadata={}, page_content='=\\nhidden_size\\n. Defaults to zero if not provided.\\noutput:\\n(\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(N, H_{out})\\n(\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nH\\no\\nu\\nt\\n)\\n(H_{out})\\n(\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ntensor containing the next hidden state.\\nVariables\\nweight_ih\\n(\\ntorch.Tensor\\n) – the learnable input-hidden weights, of shape\\n(hidden_size, input_size)\\nweight_hh\\n(\\ntorch.Tensor\\n) – the learnable hidden-hidden weights, of shape\\n(hidden_size, hidden_size)\\nbias_ih\\n– the learnable input-hidden bias, of shape\\n(hidden_size)\\nbias_hh'),\n",
       " Document(metadata={}, page_content='(hidden_size)\\nbias_hh\\n– the learnable hidden-hidden bias, of shape\\n(hidden_size)\\nNote\\nAll the weights and biases are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\n1\\nhidden_size\\nk = \\\\frac{1}{\\\\text{hidden\\\\_size}}\\nk\\n=\\nhidden_size\\n1\\n\\u200b\\nExamples:\\n>>>\\nrnn\\n=\\nnn\\n.\\nRNNCell\\n(\\n10\\n,\\n20\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n6\\n,\\n3\\n,\\n10\\n)\\n>>>\\nhx\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n20\\n)\\n>>>\\noutput\\n=\\n[]\\n>>>\\nfor\\ni\\nin\\nrange\\n(\\n6\\n):\\n...\\nhx\\n=\\nrnn\\n(\\ninput\\n[\\ni\\n],\\nhx\\n)\\n...\\noutput\\n.\\nappend\\n(\\nhx\\n)'),\n",
       " Document(metadata={}, page_content='(\\ninput\\n[\\ni\\n],\\nhx\\n)\\n...\\noutput\\n.\\nappend\\n(\\nhx\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LSTMCell\\n¶\\nclass\\ntorch.nn.\\nLSTMCell\\n(\\ninput_size\\n,\\nhidden_size\\n,\\nbias\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA long short-term memory (LSTM) cell.\\ni\\n=\\nσ\\n(\\nW\\ni\\ni\\nx\\n+\\nb\\ni\\ni\\n+\\nW\\nh\\ni\\nh\\n+\\nb\\nh\\ni\\n)\\nf\\n=\\nσ\\n(\\nW\\ni\\nf\\nx\\n+\\nb\\ni\\nf\\n+\\nW\\nh\\nf\\nh\\n+\\nb\\nh\\nf\\n)\\ng\\n=\\ntanh\\n\\u2061\\n(\\nW\\ni\\ng\\nx\\n+\\nb\\ni\\ng\\n+\\nW\\nh\\ng\\nh\\n+\\nb\\nh\\ng\\n)\\no\\n=\\nσ\\n(\\nW\\ni\\no\\nx\\n+\\nb\\ni\\no\\n+\\nW\\nh\\no\\nh\\n+\\nb\\nh\\no\\n)\\nc\\n′\\n=\\nf\\n⊙\\nc\\n+\\ni\\n⊙\\ng\\nh\\n′\\n=\\no\\n⊙\\ntanh\\n\\u2061\\n(\\nc\\n′\\n)\\n\\\\begin{array}{ll}\\ni = \\\\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\\\\'),\n",
       " Document(metadata={}, page_content=\"f = \\\\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\\\\\\ng = \\\\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\\\\\\no = \\\\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\\\\\\nc' = f \\\\odot c + i \\\\odot g \\\\\\\\\\nh' = o \\\\odot \\\\tanh(c') \\\\\\\\\\n\\\\end{array}\\ni\\n=\\nσ\\n(\\nW\\nii\\n\\u200b\\nx\\n+\\nb\\nii\\n\\u200b\\n+\\nW\\nhi\\n\\u200b\\nh\\n+\\nb\\nhi\\n\\u200b\\n)\\nf\\n=\\nσ\\n(\\nW\\ni\\nf\\n\\u200b\\nx\\n+\\nb\\ni\\nf\\n\\u200b\\n+\\nW\\nh\\nf\\n\\u200b\\nh\\n+\\nb\\nh\\nf\\n\\u200b\\n)\\ng\\n=\\ntanh\\n(\\nW\\ni\\ng\\n\\u200b\\nx\\n+\\nb\\ni\\ng\\n\\u200b\\n+\\nW\\nh\\ng\\n\\u200b\\nh\\n+\\nb\\nh\\ng\\n\\u200b\\n)\\no\\n=\\nσ\\n(\\nW\\ni\\no\\n\\u200b\\nx\\n+\\nb\\ni\\no\\n\\u200b\\n+\\nW\\nh\\no\\n\\u200b\\nh\\n+\\nb\\nh\\no\\n\\u200b\\n)\\nc\\n′\\n=\\nf\\n⊙\\nc\\n+\\ni\\n⊙\\ng\\nh\\n′\\n=\\no\\n⊙\\ntanh\\n(\\nc\\n′\\n)\\n\\u200b\\nwhere\\nσ\\n\\\\sigma\\nσ\"),\n",
       " Document(metadata={}, page_content='+\\ni\\n⊙\\ng\\nh\\n′\\n=\\no\\n⊙\\ntanh\\n(\\nc\\n′\\n)\\n\\u200b\\nwhere\\nσ\\n\\\\sigma\\nσ\\nis the sigmoid function, and\\n⊙\\n\\\\odot\\n⊙\\nis the Hadamard product.\\nParameters\\ninput_size\\n(\\nint\\n) – The number of expected features in the input\\nx\\nhidden_size\\n(\\nint\\n) – The number of features in the hidden state\\nh\\nbias\\n(\\nbool\\n) – If\\nFalse\\n, then the layer does not use bias weights\\nb_ih\\nand\\nb_hh\\n. Default:\\nTrue\\nInputs: input, (h_0, c_0)\\ninput\\nof shape\\n(batch, input_size)\\nor\\n(input_size)\\n: tensor containing input features\\nh_0\\nof shape'),\n",
       " Document(metadata={}, page_content=': tensor containing input features\\nh_0\\nof shape\\n(batch, hidden_size)\\nor\\n(hidden_size)\\n: tensor containing the initial hidden state\\nc_0\\nof shape\\n(batch, hidden_size)\\nor\\n(hidden_size)\\n: tensor containing the initial cell state\\nIf\\n(h_0, c_0)\\nis not provided, both\\nh_0\\nand\\nc_0\\ndefault to zero.\\nOutputs: (h_1, c_1)\\nh_1\\nof shape\\n(batch, hidden_size)\\nor\\n(hidden_size)\\n: tensor containing the next hidden state\\nc_1\\nof shape\\n(batch, hidden_size)\\nor\\n(hidden_size)\\n: tensor containing the next cell state'),\n",
       " Document(metadata={}, page_content=': tensor containing the next cell state\\nVariables\\nweight_ih\\n(\\ntorch.Tensor\\n) – the learnable input-hidden weights, of shape\\n(4*hidden_size, input_size)\\nweight_hh\\n(\\ntorch.Tensor\\n) – the learnable hidden-hidden weights, of shape\\n(4*hidden_size, hidden_size)\\nbias_ih\\n– the learnable input-hidden bias, of shape\\n(4*hidden_size)\\nbias_hh\\n– the learnable hidden-hidden bias, of shape\\n(4*hidden_size)\\nNote\\nAll the weights and biases are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−'),\n",
       " Document(metadata={}, page_content='−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\n1\\nhidden_size\\nk = \\\\frac{1}{\\\\text{hidden\\\\_size}}\\nk\\n=\\nhidden_size\\n1\\n\\u200b\\nOn certain ROCm devices, when using float16 inputs this module will use\\ndifferent precision\\nfor backward.\\nExamples:\\n>>>\\nrnn\\n=\\nnn\\n.\\nLSTMCell\\n(\\n10\\n,\\n20\\n)\\n# (input_size, hidden_size)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n2\\n,\\n3\\n,\\n10\\n)\\n# (time_steps, batch, input_size)\\n>>>\\nhx\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n20\\n)\\n# (batch, hidden_size)\\n>>>\\ncx\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n20\\n)\\n>>>\\noutput\\n='),\n",
       " Document(metadata={}, page_content='>>>\\ncx\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n20\\n)\\n>>>\\noutput\\n=\\n[]\\n>>>\\nfor\\ni\\nin\\nrange\\n(\\ninput\\n.\\nsize\\n()[\\n0\\n]):\\n...\\nhx\\n,\\ncx\\n=\\nrnn\\n(\\ninput\\n[\\ni\\n],\\n(\\nhx\\n,\\ncx\\n))\\n...\\noutput\\n.\\nappend\\n(\\nhx\\n)\\n>>>\\noutput\\n=\\ntorch\\n.\\nstack\\n(\\noutput\\n,\\ndim\\n=\\n0\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='GRUCell\\n¶\\nclass\\ntorch.nn.\\nGRUCell\\n(\\ninput_size\\n,\\nhidden_size\\n,\\nbias\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA gated recurrent unit (GRU) cell.\\nr\\n=\\nσ\\n(\\nW\\ni\\nr\\nx\\n+\\nb\\ni\\nr\\n+\\nW\\nh\\nr\\nh\\n+\\nb\\nh\\nr\\n)\\nz\\n=\\nσ\\n(\\nW\\ni\\nz\\nx\\n+\\nb\\ni\\nz\\n+\\nW\\nh\\nz\\nh\\n+\\nb\\nh\\nz\\n)\\nn\\n=\\ntanh\\n\\u2061\\n(\\nW\\ni\\nn\\nx\\n+\\nb\\ni\\nn\\n+\\nr\\n⊙\\n(\\nW\\nh\\nn\\nh\\n+\\nb\\nh\\nn\\n)\\n)\\nh\\n′\\n=\\n(\\n1\\n−\\nz\\n)\\n⊙\\nn\\n+\\nz\\n⊙\\nh\\n\\\\begin{array}{ll}\\nr = \\\\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\\\\\\\nz = \\\\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\\\\\'),\n",
       " Document(metadata={}, page_content=\"n = \\\\tanh(W_{in} x + b_{in} + r \\\\odot (W_{hn} h + b_{hn})) \\\\\\\\\\nh' = (1 - z) \\\\odot n + z \\\\odot h\\n\\\\end{array}\\nr\\n=\\nσ\\n(\\nW\\ni\\nr\\n\\u200b\\nx\\n+\\nb\\ni\\nr\\n\\u200b\\n+\\nW\\nh\\nr\\n\\u200b\\nh\\n+\\nb\\nh\\nr\\n\\u200b\\n)\\nz\\n=\\nσ\\n(\\nW\\ni\\nz\\n\\u200b\\nx\\n+\\nb\\ni\\nz\\n\\u200b\\n+\\nW\\nh\\nz\\n\\u200b\\nh\\n+\\nb\\nh\\nz\\n\\u200b\\n)\\nn\\n=\\ntanh\\n(\\nW\\nin\\n\\u200b\\nx\\n+\\nb\\nin\\n\\u200b\\n+\\nr\\n⊙\\n(\\nW\\nhn\\n\\u200b\\nh\\n+\\nb\\nhn\\n\\u200b\\n))\\nh\\n′\\n=\\n(\\n1\\n−\\nz\\n)\\n⊙\\nn\\n+\\nz\\n⊙\\nh\\n\\u200b\\nwhere\\nσ\\n\\\\sigma\\nσ\\nis the sigmoid function, and\\n⊙\\n\\\\odot\\n⊙\\nis the Hadamard product.\\nParameters\\ninput_size\\n(\\nint\\n) – The number of expected features in the input\\nx\\nhidden_size\\n(\\nint\"),\n",
       " Document(metadata={}, page_content='x\\nhidden_size\\n(\\nint\\n) – The number of features in the hidden state\\nh\\nbias\\n(\\nbool\\n) – If\\nFalse\\n, then the layer does not use bias weights\\nb_ih\\nand\\nb_hh\\n. Default:\\nTrue\\nInputs: input, hidden\\ninput\\n: tensor containing input features\\nhidden\\n: tensor containing the initial hidden\\nstate for each element in the batch.\\nDefaults to zero if not provided.\\nOutputs: h’\\nh’\\n: tensor containing the next hidden state\\nfor each element in the batch\\nShape:\\ninput:\\n(\\nN\\n,\\nH\\ni\\nn\\n)\\n(N, H_{in})\\n(\\nN\\n,\\nH\\nin\\n\\u200b\\n)\\nor\\n(\\nH\\ni\\nn'),\n",
       " Document(metadata={}, page_content='N\\n,\\nH\\ni\\nn\\n)\\n(N, H_{in})\\n(\\nN\\n,\\nH\\nin\\n\\u200b\\n)\\nor\\n(\\nH\\ni\\nn\\n)\\n(H_{in})\\n(\\nH\\nin\\n\\u200b\\n)\\ntensor containing input features where\\nH\\ni\\nn\\nH_{in}\\nH\\nin\\n\\u200b\\n=\\ninput_size\\n.\\nhidden:\\n(\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(N, H_{out})\\n(\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nH\\no\\nu\\nt\\n)\\n(H_{out})\\n(\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ntensor containing the initial hidden\\nstate where\\nH\\no\\nu\\nt\\nH_{out}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nhidden_size\\n. Defaults to zero if not provided.\\noutput:\\n(\\nN\\n,\\nH\\no\\nu\\nt\\n)\\n(N, H_{out})\\n(\\nN\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nor\\n(\\nH\\no\\nu\\nt\\n)\\n(H_{out})\\n(\\nH\\no\\nu\\nt\\n\\u200b\\n)\\ntensor containing the next hidden state.'),\n",
       " Document(metadata={}, page_content='u\\nt\\n\\u200b\\n)\\ntensor containing the next hidden state.\\nVariables\\nweight_ih\\n(\\ntorch.Tensor\\n) – the learnable input-hidden weights, of shape\\n(3*hidden_size, input_size)\\nweight_hh\\n(\\ntorch.Tensor\\n) – the learnable hidden-hidden weights, of shape\\n(3*hidden_size, hidden_size)\\nbias_ih\\n– the learnable input-hidden bias, of shape\\n(3*hidden_size)\\nbias_hh\\n– the learnable hidden-hidden bias, of shape\\n(3*hidden_size)\\nNote\\nAll the weights and biases are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)'),\n",
       " Document(metadata={}, page_content='U\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\n1\\nhidden_size\\nk = \\\\frac{1}{\\\\text{hidden\\\\_size}}\\nk\\n=\\nhidden_size\\n1\\n\\u200b\\nOn certain ROCm devices, when using float16 inputs this module will use\\ndifferent precision\\nfor backward.\\nExamples:\\n>>>\\nrnn\\n=\\nnn\\n.\\nGRUCell\\n(\\n10\\n,\\n20\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n6\\n,\\n3\\n,\\n10\\n)\\n>>>\\nhx\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n20\\n)\\n>>>\\noutput\\n=\\n[]\\n>>>\\nfor\\ni\\nin\\nrange\\n(\\n6\\n):\\n...\\nhx\\n=\\nrnn\\n(\\ninput\\n[\\ni\\n],\\nhx\\n)\\n...\\noutput\\n.\\nappend\\n(\\nhx\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='],\\nhx\\n)\\n...\\noutput\\n.\\nappend\\n(\\nhx\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Transformer\\n¶\\nclass\\ntorch.nn.\\nTransformer\\n(\\nd_model=512\\n,\\nnhead=8\\n,\\nnum_encoder_layers=6\\n,\\nnum_decoder_layers=6\\n,\\ndim_feedforward=2048\\n,\\ndropout=0.1\\n,\\nactivation=<function\\nrelu>\\n,\\ncustom_encoder=None\\n,\\ncustom_decoder=None\\n,\\nlayer_norm_eps=1e-05\\n,\\nbatch_first=False\\n,\\nnorm_first=False\\n,\\nbias=True\\n,\\ndevice=None\\n,\\ndtype=None\\n)\\n[source]\\n[source]\\n¶\\nA transformer model.\\nNote\\nSee\\nthis tutorial\\nfor an in depth discussion of the performant building blocks PyTorch offers for building your own'),\n",
       " Document(metadata={}, page_content='transformer layers.\\nUser is able to modify the attributes as needed. The architecture\\nis based on the paper “Attention Is All You Need”. Ashish Vaswani, Noam Shazeer,\\nNiki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\\nIllia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\\nProcessing Systems, pages 6000-6010.\\nParameters\\nd_model\\n(\\nint\\n) – the number of expected features in the encoder/decoder inputs (default=512).\\nnhead\\n(\\nint'),\n",
       " Document(metadata={}, page_content='nhead\\n(\\nint\\n) – the number of heads in the multiheadattention models (default=8).\\nnum_encoder_layers\\n(\\nint\\n) – the number of sub-encoder-layers in the encoder (default=6).\\nnum_decoder_layers\\n(\\nint\\n) – the number of sub-decoder-layers in the decoder (default=6).\\ndim_feedforward\\n(\\nint\\n) – the dimension of the feedforward network model (default=2048).\\ndropout\\n(\\nfloat\\n) – the dropout value (default=0.1).\\nactivation\\n(\\nUnion\\n[\\nstr\\n,\\nCallable\\n[\\n[\\nTensor\\n]\\n,\\nTensor\\n]\\n]'),\n",
       " Document(metadata={}, page_content='Union\\n[\\nstr\\n,\\nCallable\\n[\\n[\\nTensor\\n]\\n,\\nTensor\\n]\\n]\\n) – the activation function of encoder/decoder intermediate layer, can be a string\\n(“relu” or “gelu”) or a unary callable. Default: relu\\ncustom_encoder\\n(\\nOptional\\n[\\nAny\\n]\\n) – custom encoder (default=None).\\ncustom_decoder\\n(\\nOptional\\n[\\nAny\\n]\\n) – custom decoder (default=None).\\nlayer_norm_eps\\n(\\nfloat\\n) – the eps value in layer normalization components (default=1e-5).\\nbatch_first\\n(\\nbool\\n) – If\\nTrue\\n, then the input and output tensors are provided'),\n",
       " Document(metadata={}, page_content=', then the input and output tensors are provided\\nas (batch, seq, feature). Default:\\nFalse\\n(seq, batch, feature).\\nnorm_first\\n(\\nbool\\n) – if\\nTrue\\n, encoder and decoder layers will perform LayerNorms before\\nother attention and feedforward operations, otherwise after. Default:\\nFalse\\n(after).\\nbias\\n(\\nbool\\n) – If set to\\nFalse\\n,\\nLinear\\nand\\nLayerNorm\\nlayers will not learn an additive\\nbias. Default:\\nTrue\\n.\\nExamples::\\n>>>\\ntransformer_model\\n=\\nnn\\n.\\nTransformer\\n(\\nnhead\\n=\\n16\\n,\\nnum_encoder_layers\\n=\\n12\\n)\\n>>>\\nsrc'),\n",
       " Document(metadata={}, page_content='(\\nnhead\\n=\\n16\\n,\\nnum_encoder_layers\\n=\\n12\\n)\\n>>>\\nsrc\\n=\\ntorch\\n.\\nrand\\n((\\n10\\n,\\n32\\n,\\n512\\n))\\n>>>\\ntgt\\n=\\ntorch\\n.\\nrand\\n((\\n20\\n,\\n32\\n,\\n512\\n))\\n>>>\\nout\\n=\\ntransformer_model\\n(\\nsrc\\n,\\ntgt\\n)\\nNote: A full example to apply nn.Transformer module for the word language model is available in\\nhttps://github.com/pytorch/examples/tree/master/word_language_model\\nforward\\n(\\nsrc\\n,\\ntgt\\n,\\nsrc_mask\\n=\\nNone\\n,\\ntgt_mask\\n=\\nNone\\n,\\nmemory_mask\\n=\\nNone\\n,\\nsrc_key_padding_mask\\n=\\nNone\\n,\\ntgt_key_padding_mask\\n=\\nNone\\n,\\nmemory_key_padding_mask\\n='),\n",
       " Document(metadata={}, page_content='=\\nNone\\n,\\nmemory_key_padding_mask\\n=\\nNone\\n,\\nsrc_is_causal\\n=\\nNone\\n,\\ntgt_is_causal\\n=\\nNone\\n,\\nmemory_is_causal\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nTake in and process masked source/target sequences.\\nNote\\nIf a boolean tensor is provided for any of the [src/tgt/memory]_mask arguments, positions with a\\nTrue\\nvalue are\\nnot allowed to participate in the attention,\\nwhich is the opposite of the definition for\\nattn_mask\\nin\\ntorch.nn.functional.scaled_dot_product_attention()\\n.\\nParameters\\nsrc\\n(\\nTensor'),\n",
       " Document(metadata={}, page_content='.\\nParameters\\nsrc\\n(\\nTensor\\n) – the sequence to the encoder (required).\\ntgt\\n(\\nTensor\\n) – the sequence to the decoder (required).\\nsrc_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the additive mask for the src sequence (optional).\\ntgt_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the additive mask for the tgt sequence (optional).\\nmemory_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the additive mask for the encoder output (optional).\\nsrc_key_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the Tensor mask for src keys per batch (optional).'),\n",
       " Document(metadata={}, page_content='tgt_key_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the Tensor mask for tgt keys per batch (optional).\\nmemory_key_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the Tensor mask for memory keys per batch (optional).\\nsrc_is_causal\\n(\\nOptional\\n[\\nbool\\n]\\n) – If specified, applies a causal mask as\\nsrc_mask\\n.\\nDefault:\\nNone\\n; try to detect a causal mask.\\nWarning:\\nsrc_is_causal\\nprovides a hint that\\nsrc_mask\\nis\\nthe causal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward'),\n",
       " Document(metadata={}, page_content='compatibility.\\ntgt_is_causal\\n(\\nOptional\\n[\\nbool\\n]\\n) – If specified, applies a causal mask as\\ntgt_mask\\n.\\nDefault:\\nNone\\n; try to detect a causal mask.\\nWarning:\\ntgt_is_causal\\nprovides a hint that\\ntgt_mask\\nis\\nthe causal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.\\nmemory_is_causal\\n(\\nbool\\n) – If specified, applies a causal mask as\\nmemory_mask\\n.\\nDefault:\\nFalse\\n.\\nWarning:\\nmemory_is_causal\\nprovides a hint that\\nmemory_mask'),\n",
       " Document(metadata={}, page_content='memory_is_causal\\nprovides a hint that\\nmemory_mask\\nis the causal mask. Providing incorrect\\nhints can result in incorrect execution, including\\nforward and backward compatibility.\\nReturn type\\nTensor\\nShape:\\nsrc:\\n(\\nS\\n,\\nE\\n)\\n(S, E)\\n(\\nS\\n,\\nE\\n)\\nfor unbatched input,\\n(\\nS\\n,\\nN\\n,\\nE\\n)\\n(S, N, E)\\n(\\nS\\n,\\nN\\n,\\nE\\n)\\nif\\nbatch_first=False\\nor\\n(N, S, E)\\nif\\nbatch_first=True\\n.\\ntgt:\\n(\\nT\\n,\\nE\\n)\\n(T, E)\\n(\\nT\\n,\\nE\\n)\\nfor unbatched input,\\n(\\nT\\n,\\nN\\n,\\nE\\n)\\n(T, N, E)\\n(\\nT\\n,\\nN\\n,\\nE\\n)\\nif\\nbatch_first=False\\nor\\n(N, T, E)\\nif\\nbatch_first=True\\n.'),\n",
       " Document(metadata={}, page_content='or\\n(N, T, E)\\nif\\nbatch_first=True\\n.\\nsrc_mask:\\n(\\nS\\n,\\nS\\n)\\n(S, S)\\n(\\nS\\n,\\nS\\n)\\nor\\n(\\nN\\n⋅\\nnum_heads\\n,\\nS\\n,\\nS\\n)\\n(N\\\\cdot\\\\text{num\\\\_heads}, S, S)\\n(\\nN\\n⋅\\nnum_heads\\n,\\nS\\n,\\nS\\n)\\n.\\ntgt_mask:\\n(\\nT\\n,\\nT\\n)\\n(T, T)\\n(\\nT\\n,\\nT\\n)\\nor\\n(\\nN\\n⋅\\nnum_heads\\n,\\nT\\n,\\nT\\n)\\n(N\\\\cdot\\\\text{num\\\\_heads}, T, T)\\n(\\nN\\n⋅\\nnum_heads\\n,\\nT\\n,\\nT\\n)\\n.\\nmemory_mask:\\n(\\nT\\n,\\nS\\n)\\n(T, S)\\n(\\nT\\n,\\nS\\n)\\n.\\nsrc_key_padding_mask:\\n(\\nS\\n)\\n(S)\\n(\\nS\\n)\\nfor unbatched input otherwise\\n(\\nN\\n,\\nS\\n)\\n(N, S)\\n(\\nN\\n,\\nS\\n)\\n.\\ntgt_key_padding_mask:\\n(\\nT\\n)\\n(T)\\n(\\nT\\n)\\nfor unbatched input otherwise\\n(\\nN\\n,'),\n",
       " Document(metadata={}, page_content='T\\n)\\n(T)\\n(\\nT\\n)\\nfor unbatched input otherwise\\n(\\nN\\n,\\nT\\n)\\n(N, T)\\n(\\nN\\n,\\nT\\n)\\n.\\nmemory_key_padding_mask:\\n(\\nS\\n)\\n(S)\\n(\\nS\\n)\\nfor unbatched input otherwise\\n(\\nN\\n,\\nS\\n)\\n(N, S)\\n(\\nN\\n,\\nS\\n)\\n.\\nNote: [src/tgt/memory]_mask ensures that position\\ni\\ni\\ni\\nis allowed to attend the unmasked\\npositions. If a BoolTensor is provided, positions with\\nTrue\\nare not allowed to attend while\\nFalse\\nvalues will be unchanged. If a FloatTensor\\nis provided, it will be added to the attention weight.'),\n",
       " Document(metadata={}, page_content='[src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\\nthe attention. If a BoolTensor is provided, the positions with the\\nvalue of\\nTrue\\nwill be ignored while the position with the value of\\nFalse\\nwill be unchanged.\\noutput:\\n(\\nT\\n,\\nE\\n)\\n(T, E)\\n(\\nT\\n,\\nE\\n)\\nfor unbatched input,\\n(\\nT\\n,\\nN\\n,\\nE\\n)\\n(T, N, E)\\n(\\nT\\n,\\nN\\n,\\nE\\n)\\nif\\nbatch_first=False\\nor\\n(N, T, E)\\nif\\nbatch_first=True\\n.\\nNote: Due to the multi-head attention architecture in the transformer model,'),\n",
       " Document(metadata={}, page_content='the output sequence length of a transformer is same as the input sequence\\n(i.e. target) length of the decoder.\\nwhere\\nS\\nS\\nS\\nis the source sequence length,\\nT\\nT\\nT\\nis the target sequence length,\\nN\\nN\\nN\\nis the\\nbatch size,\\nE\\nE\\nE\\nis the feature number\\nExamples\\n>>>\\noutput\\n=\\ntransformer_model\\n(\\nsrc\\n,\\ntgt\\n,\\nsrc_mask\\n=\\nsrc_mask\\n,\\ntgt_mask\\n=\\ntgt_mask\\n)\\nstatic\\ngenerate_square_subsequent_mask\\n(\\nsz\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nGenerate a square causal mask for the sequence.'),\n",
       " Document(metadata={}, page_content='¶\\nGenerate a square causal mask for the sequence.\\nThe masked positions are filled with float(‘-inf’). Unmasked positions are filled with float(0.0).\\nReturn type\\nTensor\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='TransformerEncoder\\n¶\\nclass\\ntorch.nn.\\nTransformerEncoder\\n(\\nencoder_layer\\n,\\nnum_layers\\n,\\nnorm\\n=\\nNone\\n,\\nenable_nested_tensor\\n=\\nTrue\\n,\\nmask_check\\n=\\nTrue\\n)\\n[source]\\n[source]\\n¶\\nTransformerEncoder is a stack of N encoder layers.\\nNote\\nSee\\nthis tutorial\\nfor an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.\\nUsers can build the BERT(\\nhttps://arxiv.org/abs/1810.04805\\n) model with corresponding parameters.\\nParameters\\nencoder_layer\\n('),\n",
       " Document(metadata={}, page_content='Parameters\\nencoder_layer\\n(\\nTransformerEncoderLayer\\n) – an instance of the TransformerEncoderLayer() class (required).\\nnum_layers\\n(\\nint\\n) – the number of sub-encoder-layers in the encoder (required).\\nnorm\\n(\\nOptional\\n[\\nModule\\n]\\n) – the layer normalization component (optional).\\nenable_nested_tensor\\n(\\nbool\\n) – if True, input will automatically convert to nested tensor\\n(and convert back on output). This will improve the overall performance of\\nTransformerEncoder when padding rate is high. Default:'),\n",
       " Document(metadata={}, page_content='True\\n(enabled).\\nExamples::\\n>>>\\nencoder_layer\\n=\\nnn\\n.\\nTransformerEncoderLayer\\n(\\nd_model\\n=\\n512\\n,\\nnhead\\n=\\n8\\n)\\n>>>\\ntransformer_encoder\\n=\\nnn\\n.\\nTransformerEncoder\\n(\\nencoder_layer\\n,\\nnum_layers\\n=\\n6\\n)\\n>>>\\nsrc\\n=\\ntorch\\n.\\nrand\\n(\\n10\\n,\\n32\\n,\\n512\\n)\\n>>>\\nout\\n=\\ntransformer_encoder\\n(\\nsrc\\n)\\nforward\\n(\\nsrc\\n,\\nmask\\n=\\nNone\\n,\\nsrc_key_padding_mask\\n=\\nNone\\n,\\nis_causal\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nPass the input through the encoder layers in turn.\\nParameters\\nsrc\\n(\\nTensor\\n) – the sequence to the encoder (required).\\nmask\\n('),\n",
       " Document(metadata={}, page_content='mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the src sequence (optional).\\nsrc_key_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the src keys per batch (optional).\\nis_causal\\n(\\nOptional\\n[\\nbool\\n]\\n) – If specified, applies a causal mask as\\nmask\\n.\\nDefault:\\nNone\\n; try to detect a causal mask.\\nWarning:\\nis_causal\\nprovides a hint that\\nmask\\nis the\\ncausal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.\\nReturn type\\nTensor\\nShape:'),\n",
       " Document(metadata={}, page_content='compatibility.\\nReturn type\\nTensor\\nShape:\\nsee the docs in\\nTransformer\\n.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='TransformerDecoder\\n¶\\nclass\\ntorch.nn.\\nTransformerDecoder\\n(\\ndecoder_layer\\n,\\nnum_layers\\n,\\nnorm\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nTransformerDecoder is a stack of N decoder layers.\\nNote\\nSee\\nthis tutorial\\nfor an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.\\nParameters\\ndecoder_layer\\n(\\nTransformerDecoderLayer\\n) – an instance of the TransformerDecoderLayer() class (required).\\nnum_layers\\n(\\nint'),\n",
       " Document(metadata={}, page_content='num_layers\\n(\\nint\\n) – the number of sub-decoder-layers in the decoder (required).\\nnorm\\n(\\nOptional\\n[\\nModule\\n]\\n) – the layer normalization component (optional).\\nExamples::\\n>>>\\ndecoder_layer\\n=\\nnn\\n.\\nTransformerDecoderLayer\\n(\\nd_model\\n=\\n512\\n,\\nnhead\\n=\\n8\\n)\\n>>>\\ntransformer_decoder\\n=\\nnn\\n.\\nTransformerDecoder\\n(\\ndecoder_layer\\n,\\nnum_layers\\n=\\n6\\n)\\n>>>\\nmemory\\n=\\ntorch\\n.\\nrand\\n(\\n10\\n,\\n32\\n,\\n512\\n)\\n>>>\\ntgt\\n=\\ntorch\\n.\\nrand\\n(\\n20\\n,\\n32\\n,\\n512\\n)\\n>>>\\nout\\n=\\ntransformer_decoder\\n(\\ntgt\\n,\\nmemory\\n)\\nforward\\n(\\ntgt\\n,\\nmemory\\n,\\ntgt_mask'),\n",
       " Document(metadata={}, page_content='tgt\\n,\\nmemory\\n)\\nforward\\n(\\ntgt\\n,\\nmemory\\n,\\ntgt_mask\\n=\\nNone\\n,\\nmemory_mask\\n=\\nNone\\n,\\ntgt_key_padding_mask\\n=\\nNone\\n,\\nmemory_key_padding_mask\\n=\\nNone\\n,\\ntgt_is_causal\\n=\\nNone\\n,\\nmemory_is_causal\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nPass the inputs (and mask) through the decoder layer in turn.\\nParameters\\ntgt\\n(\\nTensor\\n) – the sequence to the decoder (required).\\nmemory\\n(\\nTensor\\n) – the sequence from the last layer of the encoder (required).\\ntgt_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the tgt sequence (optional).'),\n",
       " Document(metadata={}, page_content=']\\n) – the mask for the tgt sequence (optional).\\nmemory_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the memory sequence (optional).\\ntgt_key_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the tgt keys per batch (optional).\\nmemory_key_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the memory keys per batch (optional).\\ntgt_is_causal\\n(\\nOptional\\n[\\nbool\\n]\\n) – If specified, applies a causal mask as\\ntgt\\nmask\\n.\\nDefault:\\nNone\\n; try to detect a causal mask.\\nWarning:\\ntgt_is_causal'),\n",
       " Document(metadata={}, page_content='Warning:\\ntgt_is_causal\\nprovides a hint that\\ntgt_mask\\nis\\nthe causal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.\\nmemory_is_causal\\n(\\nbool\\n) – If specified, applies a causal mask as\\nmemory\\nmask\\n.\\nDefault:\\nFalse\\n.\\nWarning:\\nmemory_is_causal\\nprovides a hint that\\nmemory_mask\\nis the causal mask. Providing incorrect\\nhints can result in incorrect execution, including\\nforward and backward compatibility.\\nReturn type\\nTensor\\nShape:'),\n",
       " Document(metadata={}, page_content='Return type\\nTensor\\nShape:\\nsee the docs in\\nTransformer\\n.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='TransformerEncoderLayer\\n¶\\nclass\\ntorch.nn.\\nTransformerEncoderLayer\\n(\\nd_model\\n,\\nnhead\\n,\\ndim_feedforward=2048\\n,\\ndropout=0.1\\n,\\nactivation=<function\\nrelu>\\n,\\nlayer_norm_eps=1e-05\\n,\\nbatch_first=False\\n,\\nnorm_first=False\\n,\\nbias=True\\n,\\ndevice=None\\n,\\ndtype=None\\n)\\n[source]\\n[source]\\n¶\\nTransformerEncoderLayer is made up of self-attn and feedforward network.\\nNote\\nSee\\nthis tutorial\\nfor an in depth discussion of the performant building blocks PyTorch offers for building your own\\ntransformer layers.'),\n",
       " Document(metadata={}, page_content='transformer layers.\\nThis standard encoder layer is based on the paper “Attention Is All You Need”.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nLukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\\nNeural Information Processing Systems, pages 6000-6010. Users may modify or implement\\nin a different way during application.\\nTransformerEncoderLayer can handle either traditional torch.tensor inputs,'),\n",
       " Document(metadata={}, page_content='or Nested Tensor inputs.  Derived classes are expected to similarly accept\\nboth input formats.  (Not all combinations of inputs are currently\\nsupported by TransformerEncoderLayer while Nested Tensor is in prototype\\nstate.)\\nIf you are implementing a custom layer, you may derive it either from\\nthe Module or TransformerEncoderLayer class.  If your custom layer\\nsupports both torch.Tensors and Nested Tensors inputs, make its\\nimplementation a derived class of TransformerEncoderLayer. If your custom'),\n",
       " Document(metadata={}, page_content='Layer supports only torch.Tensor inputs, derive its implementation from\\nModule.\\nParameters\\nd_model\\n(\\nint\\n) – the number of expected features in the input (required).\\nnhead\\n(\\nint\\n) – the number of heads in the multiheadattention models (required).\\ndim_feedforward\\n(\\nint\\n) – the dimension of the feedforward network model (default=2048).\\ndropout\\n(\\nfloat\\n) – the dropout value (default=0.1).\\nactivation\\n(\\nUnion\\n[\\nstr\\n,\\nCallable\\n[\\n[\\nTensor\\n]\\n,\\nTensor\\n]\\n]'),\n",
       " Document(metadata={}, page_content='Union\\n[\\nstr\\n,\\nCallable\\n[\\n[\\nTensor\\n]\\n,\\nTensor\\n]\\n]\\n) – the activation function of the intermediate layer, can be a string\\n(“relu” or “gelu”) or a unary callable. Default: relu\\nlayer_norm_eps\\n(\\nfloat\\n) – the eps value in layer normalization components (default=1e-5).\\nbatch_first\\n(\\nbool\\n) – If\\nTrue\\n, then the input and output tensors are provided\\nas (batch, seq, feature). Default:\\nFalse\\n(seq, batch, feature).\\nnorm_first\\n(\\nbool\\n) – if\\nTrue\\n, layer norm is done prior to attention and feedforward'),\n",
       " Document(metadata={}, page_content='operations, respectively. Otherwise it’s done after. Default:\\nFalse\\n(after).\\nbias\\n(\\nbool\\n) – If set to\\nFalse\\n,\\nLinear\\nand\\nLayerNorm\\nlayers will not learn an additive\\nbias. Default:\\nTrue\\n.\\nExamples::\\n>>>\\nencoder_layer\\n=\\nnn\\n.\\nTransformerEncoderLayer\\n(\\nd_model\\n=\\n512\\n,\\nnhead\\n=\\n8\\n)\\n>>>\\nsrc\\n=\\ntorch\\n.\\nrand\\n(\\n10\\n,\\n32\\n,\\n512\\n)\\n>>>\\nout\\n=\\nencoder_layer\\n(\\nsrc\\n)\\nAlternatively, when\\nbatch_first\\nis\\nTrue\\n:\\n>>>\\nencoder_layer\\n=\\nnn\\n.\\nTransformerEncoderLayer\\n(\\nd_model\\n=\\n512\\n,\\nnhead\\n=\\n8\\n,\\nbatch_first\\n=\\nTrue\\n)\\n>>>'),\n",
       " Document(metadata={}, page_content='=\\n512\\n,\\nnhead\\n=\\n8\\n,\\nbatch_first\\n=\\nTrue\\n)\\n>>>\\nsrc\\n=\\ntorch\\n.\\nrand\\n(\\n32\\n,\\n10\\n,\\n512\\n)\\n>>>\\nout\\n=\\nencoder_layer\\n(\\nsrc\\n)\\nFast path:\\nforward() will use a special optimized implementation described in\\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\\nif all of the following\\nconditions are met:\\nEither autograd is disabled (using\\ntorch.inference_mode\\nor\\ntorch.no_grad\\n) or no tensor\\nargument\\nrequires_grad\\ntraining is disabled (using\\n.eval()\\n)\\nbatch_first is\\nTrue'),\n",
       " Document(metadata={}, page_content='.eval()\\n)\\nbatch_first is\\nTrue\\nand the input is batched (i.e.,\\nsrc.dim()\\n==\\n3\\n)\\nactivation is one of:\\n\"relu\"\\n,\\n\"gelu\"\\n,\\ntorch.functional.relu\\n, or\\ntorch.functional.gelu\\nat most one of\\nsrc_mask\\nand\\nsrc_key_padding_mask\\nis passed\\nif src is a\\nNestedTensor\\n, neither\\nsrc_mask\\nnor\\nsrc_key_padding_mask\\nis passed\\nthe two\\nLayerNorm\\ninstances have a consistent\\neps\\nvalue (this will naturally be the case\\nunless the caller has manually modified one without modifying the other)'),\n",
       " Document(metadata={}, page_content='If the optimized implementation is in use, a\\nNestedTensor\\ncan be\\npassed for\\nsrc\\nto represent padding more efficiently than using a padding\\nmask. In this case, a\\nNestedTensor\\nwill be\\nreturned, and an additional speedup proportional to the fraction of the input that\\nis padding can be expected.\\nforward\\n(\\nsrc\\n,\\nsrc_mask\\n=\\nNone\\n,\\nsrc_key_padding_mask\\n=\\nNone\\n,\\nis_causal\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nPass the input through the encoder layer.\\nParameters\\nsrc\\n(\\nTensor'),\n",
       " Document(metadata={}, page_content='Parameters\\nsrc\\n(\\nTensor\\n) – the sequence to the encoder layer (required).\\nsrc_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the src sequence (optional).\\nsrc_key_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the src keys per batch (optional).\\nis_causal\\n(\\nbool\\n) – If specified, applies a causal mask as\\nsrc\\nmask\\n.\\nDefault:\\nFalse\\n.\\nWarning:\\nis_causal\\nprovides a hint that\\nsrc_mask\\nis the\\ncausal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward'),\n",
       " Document(metadata={}, page_content='compatibility.\\nReturn type\\nTensor\\nShape:\\nsee the docs in\\nTransformer\\n.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='TransformerDecoderLayer\\n¶\\nclass\\ntorch.nn.\\nTransformerDecoderLayer\\n(\\nd_model\\n,\\nnhead\\n,\\ndim_feedforward=2048\\n,\\ndropout=0.1\\n,\\nactivation=<function\\nrelu>\\n,\\nlayer_norm_eps=1e-05\\n,\\nbatch_first=False\\n,\\nnorm_first=False\\n,\\nbias=True\\n,\\ndevice=None\\n,\\ndtype=None\\n)\\n[source]\\n[source]\\n¶\\nTransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\\nNote\\nSee\\nthis tutorial\\nfor an in depth discussion of the performant building blocks PyTorch offers for building your own'),\n",
       " Document(metadata={}, page_content='transformer layers.\\nThis standard decoder layer is based on the paper “Attention Is All You Need”.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nLukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\\nNeural Information Processing Systems, pages 6000-6010. Users may modify or implement\\nin a different way during application.\\nParameters\\nd_model\\n(\\nint\\n) – the number of expected features in the input (required).\\nnhead\\n(\\nint'),\n",
       " Document(metadata={}, page_content='nhead\\n(\\nint\\n) – the number of heads in the multiheadattention models (required).\\ndim_feedforward\\n(\\nint\\n) – the dimension of the feedforward network model (default=2048).\\ndropout\\n(\\nfloat\\n) – the dropout value (default=0.1).\\nactivation\\n(\\nUnion\\n[\\nstr\\n,\\nCallable\\n[\\n[\\nTensor\\n]\\n,\\nTensor\\n]\\n]\\n) – the activation function of the intermediate layer, can be a string\\n(“relu” or “gelu”) or a unary callable. Default: relu\\nlayer_norm_eps\\n(\\nfloat'),\n",
       " Document(metadata={}, page_content='layer_norm_eps\\n(\\nfloat\\n) – the eps value in layer normalization components (default=1e-5).\\nbatch_first\\n(\\nbool\\n) – If\\nTrue\\n, then the input and output tensors are provided\\nas (batch, seq, feature). Default:\\nFalse\\n(seq, batch, feature).\\nnorm_first\\n(\\nbool\\n) – if\\nTrue\\n, layer norm is done prior to self attention, multihead\\nattention and feedforward operations, respectively. Otherwise it’s done after.\\nDefault:\\nFalse\\n(after).\\nbias\\n(\\nbool\\n) – If set to\\nFalse\\n,\\nLinear\\nand\\nLayerNorm'),\n",
       " Document(metadata={}, page_content='(\\nbool\\n) – If set to\\nFalse\\n,\\nLinear\\nand\\nLayerNorm\\nlayers will not learn an additive\\nbias. Default:\\nTrue\\n.\\nExamples::\\n>>>\\ndecoder_layer\\n=\\nnn\\n.\\nTransformerDecoderLayer\\n(\\nd_model\\n=\\n512\\n,\\nnhead\\n=\\n8\\n)\\n>>>\\nmemory\\n=\\ntorch\\n.\\nrand\\n(\\n10\\n,\\n32\\n,\\n512\\n)\\n>>>\\ntgt\\n=\\ntorch\\n.\\nrand\\n(\\n20\\n,\\n32\\n,\\n512\\n)\\n>>>\\nout\\n=\\ndecoder_layer\\n(\\ntgt\\n,\\nmemory\\n)\\nAlternatively, when\\nbatch_first\\nis\\nTrue\\n:\\n>>>\\ndecoder_layer\\n=\\nnn\\n.\\nTransformerDecoderLayer\\n(\\nd_model\\n=\\n512\\n,\\nnhead\\n=\\n8\\n,\\nbatch_first\\n=\\nTrue\\n)\\n>>>\\nmemory\\n=\\ntorch\\n.\\nrand\\n(\\n32\\n,\\n10'),\n",
       " Document(metadata={}, page_content='=\\nTrue\\n)\\n>>>\\nmemory\\n=\\ntorch\\n.\\nrand\\n(\\n32\\n,\\n10\\n,\\n512\\n)\\n>>>\\ntgt\\n=\\ntorch\\n.\\nrand\\n(\\n32\\n,\\n20\\n,\\n512\\n)\\n>>>\\nout\\n=\\ndecoder_layer\\n(\\ntgt\\n,\\nmemory\\n)\\nforward\\n(\\ntgt\\n,\\nmemory\\n,\\ntgt_mask\\n=\\nNone\\n,\\nmemory_mask\\n=\\nNone\\n,\\ntgt_key_padding_mask\\n=\\nNone\\n,\\nmemory_key_padding_mask\\n=\\nNone\\n,\\ntgt_is_causal\\n=\\nFalse\\n,\\nmemory_is_causal\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nPass the inputs (and mask) through the decoder layer.\\nParameters\\ntgt\\n(\\nTensor\\n) – the sequence to the decoder layer (required).\\nmemory\\n(\\nTensor'),\n",
       " Document(metadata={}, page_content='memory\\n(\\nTensor\\n) – the sequence from the last layer of the encoder (required).\\ntgt_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the tgt sequence (optional).\\nmemory_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the memory sequence (optional).\\ntgt_key_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the tgt keys per batch (optional).\\nmemory_key_padding_mask\\n(\\nOptional\\n[\\nTensor\\n]\\n) – the mask for the memory keys per batch (optional).\\ntgt_is_causal\\n(\\nbool\\n) – If specified, applies a causal mask as'),\n",
       " Document(metadata={}, page_content='(\\nbool\\n) – If specified, applies a causal mask as\\ntgt\\nmask\\n.\\nDefault:\\nFalse\\n.\\nWarning:\\ntgt_is_causal\\nprovides a hint that\\ntgt_mask\\nis\\nthe causal mask. Providing incorrect hints can result in\\nincorrect execution, including forward and backward\\ncompatibility.\\nmemory_is_causal\\n(\\nbool\\n) – If specified, applies a causal mask as\\nmemory\\nmask\\n.\\nDefault:\\nFalse\\n.\\nWarning:\\nmemory_is_causal\\nprovides a hint that\\nmemory_mask\\nis the causal mask. Providing incorrect'),\n",
       " Document(metadata={}, page_content='is the causal mask. Providing incorrect\\nhints can result in incorrect execution, including\\nforward and backward compatibility.\\nReturn type\\nTensor\\nShape:\\nsee the docs in\\nTransformer\\n.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Identity\\n¶\\nclass\\ntorch.nn.\\nIdentity\\n(\\n*\\nargs\\n,\\n**\\nkwargs\\n)\\n[source]\\n[source]\\n¶\\nA placeholder identity operator that is argument-insensitive.\\nParameters\\nargs\\n(\\nAny\\n) – any argument (unused)\\nkwargs\\n(\\nAny\\n) – any keyword argument (unused)\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nIdentity\\n(\\n54\\n,\\nunused_argument1\\n=\\n0.1\\n,\\nunused_argument2\\n=\\nFalse\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n128\\n,\\n20\\n)\\n>>>'),\n",
       " Document(metadata={}, page_content=')\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n128\\n,\\n20\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\nprint\\n(\\noutput\\n.\\nsize\\n())\\ntorch.Size([128, 20])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Linear\\n¶\\nclass\\ntorch.nn.\\nLinear\\n(\\nin_features\\n,\\nout_features\\n,\\nbias\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies an affine linear transformation to the incoming data:\\ny\\n=\\nx\\nA\\nT\\n+\\nb\\ny = xA^T + b\\ny\\n=\\nx\\nA\\nT\\n+\\nb\\n.\\nThis module supports\\nTensorFloat32\\n.\\nOn certain ROCm devices, when using float16 inputs this module will use\\ndifferent precision\\nfor backward.\\nParameters\\nin_features\\n(\\nint\\n) – size of each input sample\\nout_features\\n(\\nint\\n) – size of each output sample\\nbias\\n(\\nbool'),\n",
       " Document(metadata={}, page_content='(\\nint\\n) – size of each output sample\\nbias\\n(\\nbool\\n) – If set to\\nFalse\\n, the layer will not learn an additive bias.\\nDefault:\\nTrue\\nShape:\\nInput:\\n(\\n∗\\n,\\nH\\ni\\nn\\n)\\n(*, H_{in})\\n(\\n∗\\n,\\nH\\nin\\n\\u200b\\n)\\nwhere\\n∗\\n*\\n∗\\nmeans any number of\\ndimensions including none and\\nH\\ni\\nn\\n=\\nin_features\\nH_{in} = \\\\text{in\\\\_features}\\nH\\nin\\n\\u200b\\n=\\nin_features\\n.\\nOutput:\\n(\\n∗\\n,\\nH\\no\\nu\\nt\\n)\\n(*, H_{out})\\n(\\n∗\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nwhere all but the last dimension\\nare the same shape as the input and\\nH\\no\\nu\\nt\\n=\\nout_features\\nH_{out} = \\\\text{out\\\\_features}\\nH\\no'),\n",
       " Document(metadata={}, page_content='=\\nout_features\\nH_{out} = \\\\text{out\\\\_features}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nout_features\\n.\\nVariables\\nweight\\n(\\ntorch.Tensor\\n) – the learnable weights of the module of shape\\n(\\nout_features\\n,\\nin_features\\n)\\n(\\\\text{out\\\\_features}, \\\\text{in\\\\_features})\\n(\\nout_features\\n,\\nin_features\\n)\\n. The values are\\ninitialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\n, where\\nk\\n=\\n1\\nin_features\\nk = \\\\frac{1}{\\\\text{in\\\\_features}}\\nk\\n=\\nin_features\\n1\\n\\u200b\\nbias\\n– the learnable bias of the module of shape\\n('),\n",
       " Document(metadata={}, page_content='– the learnable bias of the module of shape\\n(\\nout_features\\n)\\n(\\\\text{out\\\\_features})\\n(\\nout_features\\n)\\n.\\nIf\\nbias\\nis\\nTrue\\n, the values are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\n1\\nin_features\\nk = \\\\frac{1}{\\\\text{in\\\\_features}}\\nk\\n=\\nin_features\\n1\\n\\u200b\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nLinear\\n(\\n20\\n,\\n30\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n128\\n,\\n20\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\n>>>\\nprint\\n(\\noutput\\n.\\nsize\\n())\\ntorch.Size([128, 30])\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='.\\nsize\\n())\\ntorch.Size([128, 30])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Bilinear\\n¶\\nclass\\ntorch.nn.\\nBilinear\\n(\\nin1_features\\n,\\nin2_features\\n,\\nout_features\\n,\\nbias\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nApplies a bilinear transformation to the incoming data:\\ny\\n=\\nx\\n1\\nT\\nA\\nx\\n2\\n+\\nb\\ny = x_1^T A x_2 + b\\ny\\n=\\nx\\n1\\nT\\n\\u200b\\nA\\nx\\n2\\n\\u200b\\n+\\nb\\n.\\nParameters\\nin1_features\\n(\\nint\\n) – size of each first input sample\\nin2_features\\n(\\nint\\n) – size of each second input sample\\nout_features\\n(\\nint\\n) – size of each output sample\\nbias\\n(\\nbool'),\n",
       " Document(metadata={}, page_content='(\\nint\\n) – size of each output sample\\nbias\\n(\\nbool\\n) – If set to False, the layer will not learn an additive bias.\\nDefault:\\nTrue\\nShape:\\nInput1:\\n(\\n∗\\n,\\nH\\ni\\nn\\n1\\n)\\n(*, H_{in1})\\n(\\n∗\\n,\\nH\\nin\\n1\\n\\u200b\\n)\\nwhere\\nH\\ni\\nn\\n1\\n=\\nin1_features\\nH_{in1}=\\\\text{in1\\\\_features}\\nH\\nin\\n1\\n\\u200b\\n=\\nin1_features\\nand\\n∗\\n*\\n∗\\nmeans any number of additional dimensions including none. All but the last dimension\\nof the inputs should be the same.\\nInput2:\\n(\\n∗\\n,\\nH\\ni\\nn\\n2\\n)\\n(*, H_{in2})\\n(\\n∗\\n,\\nH\\nin\\n2\\n\\u200b\\n)\\nwhere\\nH\\ni\\nn\\n2\\n=\\nin2_features'),\n",
       " Document(metadata={}, page_content='(\\n∗\\n,\\nH\\nin\\n2\\n\\u200b\\n)\\nwhere\\nH\\ni\\nn\\n2\\n=\\nin2_features\\nH_{in2}=\\\\text{in2\\\\_features}\\nH\\nin\\n2\\n\\u200b\\n=\\nin2_features\\n.\\nOutput:\\n(\\n∗\\n,\\nH\\no\\nu\\nt\\n)\\n(*, H_{out})\\n(\\n∗\\n,\\nH\\no\\nu\\nt\\n\\u200b\\n)\\nwhere\\nH\\no\\nu\\nt\\n=\\nout_features\\nH_{out}=\\\\text{out\\\\_features}\\nH\\no\\nu\\nt\\n\\u200b\\n=\\nout_features\\nand all but the last dimension are the same shape as the input.\\nVariables\\nweight\\n(\\ntorch.Tensor\\n) – the learnable weights of the module of shape\\n(\\nout_features\\n,\\nin1_features\\n,\\nin2_features\\n)\\n(\\\\text{out\\\\_features}, \\\\text{in1\\\\_features}, \\\\text{in2\\\\_features})\\n('),\n",
       " Document(metadata={}, page_content='(\\nout_features\\n,\\nin1_features\\n,\\nin2_features\\n)\\n.\\nThe values are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\n, where\\nk\\n=\\n1\\nin1_features\\nk = \\\\frac{1}{\\\\text{in1\\\\_features}}\\nk\\n=\\nin1_features\\n1\\n\\u200b\\nbias\\n– the learnable bias of the module of shape\\n(\\nout_features\\n)\\n(\\\\text{out\\\\_features})\\n(\\nout_features\\n)\\n.\\nIf\\nbias\\nis\\nTrue\\n, the values are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\n, where\\nk\\n=\\n1\\nin1_features'),\n",
       " Document(metadata={}, page_content='U\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\n, where\\nk\\n=\\n1\\nin1_features\\nk = \\\\frac{1}{\\\\text{in1\\\\_features}}\\nk\\n=\\nin1_features\\n1\\n\\u200b\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nBilinear\\n(\\n20\\n,\\n30\\n,\\n40\\n)\\n>>>\\ninput1\\n=\\ntorch\\n.\\nrandn\\n(\\n128\\n,\\n20\\n)\\n>>>\\ninput2\\n=\\ntorch\\n.\\nrandn\\n(\\n128\\n,\\n30\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput1\\n,\\ninput2\\n)\\n>>>\\nprint\\n(\\noutput\\n.\\nsize\\n())\\ntorch.Size([128, 40])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='LazyLinear\\n¶\\nclass\\ntorch.nn.\\nLazyLinear\\n(\\nout_features\\n,\\nbias\\n=\\nTrue\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA\\ntorch.nn.Linear\\nmodule where\\nin_features\\nis inferred.\\nIn this module, the\\nweight\\nand\\nbias\\nare of\\ntorch.nn.UninitializedParameter\\nclass. They will be initialized after the first call to\\nforward\\nis done and the\\nmodule will become a regular\\ntorch.nn.Linear\\nmodule. The\\nin_features\\nargument\\nof the\\nLinear\\nis inferred from the\\ninput.shape[-1]\\n.\\nCheck the'),\n",
       " Document(metadata={}, page_content='is inferred from the\\ninput.shape[-1]\\n.\\nCheck the\\ntorch.nn.modules.lazy.LazyModuleMixin\\nfor further documentation\\non lazy modules and their limitations.\\nParameters\\nout_features\\n(\\nint\\n) – size of each output sample\\nbias\\n(\\nUninitializedParameter\\n) – If set to\\nFalse\\n, the layer will not learn an additive bias.\\nDefault:\\nTrue\\nVariables\\nweight\\n(\\ntorch.nn.parameter.UninitializedParameter\\n) – the learnable weights of the module of shape\\n(\\nout_features\\n,\\nin_features\\n)'),\n",
       " Document(metadata={}, page_content='(\\nout_features\\n,\\nin_features\\n)\\n(\\\\text{out\\\\_features}, \\\\text{in\\\\_features})\\n(\\nout_features\\n,\\nin_features\\n)\\n. The values are\\ninitialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\n, where\\nk\\n=\\n1\\nin_features\\nk = \\\\frac{1}{\\\\text{in\\\\_features}}\\nk\\n=\\nin_features\\n1\\n\\u200b\\nbias\\n(\\ntorch.nn.parameter.UninitializedParameter\\n) – the learnable bias of the module of shape\\n(\\nout_features\\n)\\n(\\\\text{out\\\\_features})\\n(\\nout_features\\n)\\n.\\nIf\\nbias\\nis\\nTrue\\n, the values are initialized from\\nU\\n(\\n−\\nk\\n,'),\n",
       " Document(metadata={}, page_content='True\\n, the values are initialized from\\nU\\n(\\n−\\nk\\n,\\nk\\n)\\n\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})\\nU\\n(\\n−\\nk\\n\\u200b\\n,\\nk\\n\\u200b\\n)\\nwhere\\nk\\n=\\n1\\nin_features\\nk = \\\\frac{1}{\\\\text{in\\\\_features}}\\nk\\n=\\nin_features\\n1\\n\\u200b\\ncls_to_become\\n[source]\\n¶\\nalias of\\nLinear\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Dropout\\n¶\\nclass\\ntorch.nn.\\nDropout\\n(\\np\\n=\\n0.5\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nDuring training, randomly zeroes some of the elements of the input tensor with probability\\np\\n.\\nThe zeroed elements are chosen independently for each forward call and are sampled from a Bernoulli distribution.\\nEach channel will be zeroed out independently on every forward call.\\nThis has proven to be an effective technique for regularization and\\npreventing the co-adaptation of neurons as described in the paper'),\n",
       " Document(metadata={}, page_content='Improving neural networks by preventing co-adaptation of feature\\ndetectors\\n.\\nFurthermore, the outputs are scaled by a factor of\\n1\\n1\\n−\\np\\n\\\\frac{1}{1-p}\\n1\\n−\\np\\n1\\n\\u200b\\nduring\\ntraining. This means that during evaluation the module simply computes an\\nidentity function.\\nParameters\\np\\n(\\nfloat\\n) – probability of an element to be zeroed. Default: 0.5\\ninplace\\n(\\nbool\\n) – If set to\\nTrue\\n, will do this operation in-place. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n. Input can be of any shape\\nOutput:\\n(\\n∗\\n)\\n(*)\\n('),\n",
       " Document(metadata={}, page_content=')\\n. Input can be of any shape\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n. Output is of the same shape as input\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nDropout\\n(\\np\\n=\\n0.2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Dropout1d\\n¶\\nclass\\ntorch.nn.\\nDropout1d\\n(\\np\\n=\\n0.5\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nRandomly zero out entire channels.\\nA channel is a 1D feature map,\\ne.g., the\\nj\\nj\\nj\\n-th channel of the\\ni\\ni\\ni\\n-th sample in the\\nbatched input is a 1D tensor\\ninput\\n[\\ni\\n,\\nj\\n]\\n\\\\text{input}[i, j]\\ninput\\n[\\ni\\n,\\nj\\n]\\n.\\nEach channel will be zeroed out independently on every forward call with\\nprobability\\np\\nusing samples from a Bernoulli distribution.\\nUsually the input comes from\\nnn.Conv1d\\nmodules.\\nAs described in the paper'),\n",
       " Document(metadata={}, page_content='nn.Conv1d\\nmodules.\\nAs described in the paper\\nEfficient Object Localization Using Convolutional Networks\\n,\\nif adjacent pixels within feature maps are strongly correlated\\n(as is normally the case in early convolution layers) then i.i.d. dropout\\nwill not regularize the activations and will otherwise just result\\nin an effective learning rate decrease.\\nIn this case,\\nnn.Dropout1d()\\nwill help promote independence between\\nfeature maps and should be used instead.\\nParameters\\np\\n(\\nfloat\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='Parameters\\np\\n(\\nfloat\\n,\\noptional\\n) – probability of an element to be zero-ed.\\ninplace\\n(\\nbool\\n,\\noptional\\n) – If set to\\nTrue\\n, will do this operation\\nin-place\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\nor\\n(\\nC\\n,\\nL\\n)\\n(C, L)\\n(\\nC\\n,\\nL\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\nor\\n(\\nC\\n,\\nL\\n)\\n(C, L)\\n(\\nC\\n,\\nL\\n)\\n(same shape as input).\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nDropout1d\\n(\\np\\n=\\n0.2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='16\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Dropout2d\\n¶\\nclass\\ntorch.nn.\\nDropout2d\\n(\\np\\n=\\n0.5\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nRandomly zero out entire channels.\\nA channel is a 2D feature map,\\ne.g., the\\nj\\nj\\nj\\n-th channel of the\\ni\\ni\\ni\\n-th sample in the\\nbatched input is a 2D tensor\\ninput\\n[\\ni\\n,\\nj\\n]\\n\\\\text{input}[i, j]\\ninput\\n[\\ni\\n,\\nj\\n]\\n.\\nEach channel will be zeroed out independently on every forward call with\\nprobability\\np\\nusing samples from a Bernoulli distribution.\\nUsually the input comes from\\nnn.Conv2d\\nmodules.\\nAs described in the paper'),\n",
       " Document(metadata={}, page_content='nn.Conv2d\\nmodules.\\nAs described in the paper\\nEfficient Object Localization Using Convolutional Networks\\n,\\nif adjacent pixels within feature maps are strongly correlated\\n(as is normally the case in early convolution layers) then i.i.d. dropout\\nwill not regularize the activations and will otherwise just result\\nin an effective learning rate decrease.\\nIn this case,\\nnn.Dropout2d()\\nwill help promote independence between\\nfeature maps and should be used instead.\\nParameters\\np\\n(\\nfloat\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='Parameters\\np\\n(\\nfloat\\n,\\noptional\\n) – probability of an element to be zero-ed.\\ninplace\\n(\\nbool\\n,\\noptional\\n) – If set to\\nTrue\\n, will do this operation\\nin-place\\nWarning\\nDue to historical reasons, this class will perform 1D channel-wise dropout\\nfor 3D inputs (as done by\\nnn.Dropout1d\\n). Thus, it currently does NOT\\nsupport inputs without a batch dimension of shape\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n(C, H, W)\\n(\\nC\\n,\\nH\\n,\\nW\\n)\\n. This\\nbehavior will change in a future release to interpret 3D inputs as no-batch-dim'),\n",
       " Document(metadata={}, page_content='inputs. To maintain the old behavior, switch to\\nnn.Dropout1d\\n.\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\n(N, C, H, W)\\n(\\nN\\n,\\nC\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(N, C, L)\\n(\\nN\\n,\\nC\\n,\\nL\\n)\\n(same shape as input).\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nDropout2d\\n(\\np\\n=\\n0.2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n32\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme'),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Dropout3d\\n¶\\nclass\\ntorch.nn.\\nDropout3d\\n(\\np\\n=\\n0.5\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nRandomly zero out entire channels.\\nA channel is a 3D feature map,\\ne.g., the\\nj\\nj\\nj\\n-th channel of the\\ni\\ni\\ni\\n-th sample in the\\nbatched input is a 3D tensor\\ninput\\n[\\ni\\n,\\nj\\n]\\n\\\\text{input}[i, j]\\ninput\\n[\\ni\\n,\\nj\\n]\\n.\\nEach channel will be zeroed out independently on every forward call with\\nprobability\\np\\nusing samples from a Bernoulli distribution.\\nUsually the input comes from\\nnn.Conv3d\\nmodules.\\nAs described in the paper'),\n",
       " Document(metadata={}, page_content='nn.Conv3d\\nmodules.\\nAs described in the paper\\nEfficient Object Localization Using Convolutional Networks\\n,\\nif adjacent pixels within feature maps are strongly correlated\\n(as is normally the case in early convolution layers) then i.i.d. dropout\\nwill not regularize the activations and will otherwise just result\\nin an effective learning rate decrease.\\nIn this case,\\nnn.Dropout3d()\\nwill help promote independence between\\nfeature maps and should be used instead.\\nParameters\\np\\n(\\nfloat\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='Parameters\\np\\n(\\nfloat\\n,\\noptional\\n) – probability of an element to be zeroed.\\ninplace\\n(\\nbool\\n,\\noptional\\n) – If set to\\nTrue\\n, will do this operation\\nin-place\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(same shape as input).\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nDropout3d\\n(\\np\\n=\\n0.2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n('),\n",
       " Document(metadata={}, page_content='Dropout3d\\n(\\np\\n=\\n0.2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n4\\n,\\n32\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='AlphaDropout\\n¶\\nclass\\ntorch.nn.\\nAlphaDropout\\n(\\np\\n=\\n0.5\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nApplies Alpha Dropout over the input.\\nAlpha Dropout is a type of Dropout that maintains the self-normalizing\\nproperty.\\nFor an input with zero mean and unit standard deviation, the output of\\nAlpha Dropout maintains the original mean and standard deviation of the\\ninput.\\nAlpha Dropout goes hand-in-hand with SELU activation function, which ensures\\nthat the outputs have zero mean and unit standard deviation.'),\n",
       " Document(metadata={}, page_content='During training, it randomly masks some of the elements of the input\\ntensor with probability\\np\\nusing samples from a bernoulli distribution.\\nThe elements to masked are randomized on every forward call, and scaled\\nand shifted to maintain zero mean and unit standard deviation.\\nDuring evaluation the module simply computes an identity function.\\nMore details can be found in the paper\\nSelf-Normalizing Neural Networks\\n.\\nParameters\\np\\n(\\nfloat\\n) – probability of an element to be dropped. Default: 0.5'),\n",
       " Document(metadata={}, page_content='inplace\\n(\\nbool\\n,\\noptional\\n) – If set to\\nTrue\\n, will do this operation\\nin-place\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n. Input can be of any shape\\nOutput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n. Output is of the same shape as input\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nAlphaDropout\\n(\\np\\n=\\n0.2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='FeatureAlphaDropout\\n¶\\nclass\\ntorch.nn.\\nFeatureAlphaDropout\\n(\\np\\n=\\n0.5\\n,\\ninplace\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nRandomly masks out entire channels.\\nA channel is a feature map,\\ne.g. the\\nj\\nj\\nj\\n-th channel of the\\ni\\ni\\ni\\n-th sample in the batch input\\nis a tensor\\ninput\\n[\\ni\\n,\\nj\\n]\\n\\\\text{input}[i, j]\\ninput\\n[\\ni\\n,\\nj\\n]\\nof the input tensor). Instead of\\nsetting activations to zero, as in regular Dropout, the activations are set\\nto the negative saturation value of the SELU activation function. More details'),\n",
       " Document(metadata={}, page_content='can be found in the paper\\nSelf-Normalizing Neural Networks\\n.\\nEach element will be masked independently for each sample on every forward\\ncall with probability\\np\\nusing samples from a Bernoulli distribution.\\nThe elements to be masked are randomized on every forward call, and scaled\\nand shifted to maintain zero mean and unit variance.\\nUsually the input comes from\\nnn.AlphaDropout\\nmodules.\\nAs described in the paper\\nEfficient Object Localization Using Convolutional Networks\\n,'),\n",
       " Document(metadata={}, page_content=',\\nif adjacent pixels within feature maps are strongly correlated\\n(as is normally the case in early convolution layers) then i.i.d. dropout\\nwill not regularize the activations and will otherwise just result\\nin an effective learning rate decrease.\\nIn this case,\\nnn.AlphaDropout()\\nwill help promote independence between\\nfeature maps and should be used instead.\\nParameters\\np\\n(\\nfloat\\n,\\noptional\\n) – probability of an element to be zeroed. Default: 0.5\\ninplace\\n(\\nbool\\n,\\noptional\\n) – If set to\\nTrue'),\n",
       " Document(metadata={}, page_content='inplace\\n(\\nbool\\n,\\noptional\\n) – If set to\\nTrue\\n, will do this operation\\nin-place\\nShape:\\nInput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n.\\nOutput:\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(N, C, D, H, W)\\n(\\nN\\n,\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\nor\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(C, D, H, W)\\n(\\nC\\n,\\nD\\n,\\nH\\n,\\nW\\n)\\n(same shape as input).\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nFeatureAlphaDropout\\n(\\np\\n=\\n0.2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n20\\n,\\n16\\n,\\n4\\n,\\n32\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='32\\n,\\n32\\n)\\n>>>\\noutput\\n=\\nm\\n(\\ninput\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='Embedding\\n¶\\nclass\\ntorch.nn.\\nEmbedding\\n(\\nnum_embeddings\\n,\\nembedding_dim\\n,\\npadding_idx\\n=\\nNone\\n,\\nmax_norm\\n=\\nNone\\n,\\nnorm_type\\n=\\n2.0\\n,\\nscale_grad_by_freq\\n=\\nFalse\\n,\\nsparse\\n=\\nFalse\\n,\\n_weight\\n=\\nNone\\n,\\n_freeze\\n=\\nFalse\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nA simple lookup table that stores embeddings of a fixed dictionary and size.\\nThis module is often used to store word embeddings and retrieve them using indices.'),\n",
       " Document(metadata={}, page_content='The input to the module is a list of indices, and the output is the corresponding\\nword embeddings.\\nParameters\\nnum_embeddings\\n(\\nint\\n) – size of the dictionary of embeddings\\nembedding_dim\\n(\\nint\\n) – the size of each embedding vector\\npadding_idx\\n(\\nint\\n,\\noptional\\n) – If specified, the entries at\\npadding_idx\\ndo not contribute to the gradient;\\ntherefore, the embedding vector at\\npadding_idx\\nis not updated during training,\\ni.e. it remains as a fixed “pad”. For a newly constructed Embedding,'),\n",
       " Document(metadata={}, page_content='the embedding vector at\\npadding_idx\\nwill default to all zeros,\\nbut can be updated to another value to be used as the padding vector.\\nmax_norm\\n(\\nfloat\\n,\\noptional\\n) – If given, each embedding vector with norm larger than\\nmax_norm\\nis renormalized to have norm\\nmax_norm\\n.\\nnorm_type\\n(\\nfloat\\n,\\noptional\\n) – The p of the p-norm to compute for the\\nmax_norm\\noption. Default\\n2\\n.\\nscale_grad_by_freq\\n(\\nbool\\n,\\noptional\\n) – If given, this will scale gradients by the inverse of frequency of'),\n",
       " Document(metadata={}, page_content='the words in the mini-batch. Default\\nFalse\\n.\\nsparse\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, gradient w.r.t.\\nweight\\nmatrix will be a sparse tensor.\\nSee Notes for more details regarding sparse gradients.\\nVariables\\nweight\\n(\\nTensor\\n) – the learnable weights of the module of shape (num_embeddings, embedding_dim)\\ninitialized from\\nN\\n(\\n0\\n,\\n1\\n)\\n\\\\mathcal{N}(0, 1)\\nN\\n(\\n0\\n,\\n1\\n)\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, IntTensor or LongTensor of arbitrary shape containing the indices to extract\\nOutput:\\n(\\n∗\\n,\\nH\\n)\\n(*, H)\\n(\\n∗\\n,'),\n",
       " Document(metadata={}, page_content='Output:\\n(\\n∗\\n,\\nH\\n)\\n(*, H)\\n(\\n∗\\n,\\nH\\n)\\n, where\\n*\\nis the input shape and\\nH\\n=\\nembedding_dim\\nH=\\\\text{embedding\\\\_dim}\\nH\\n=\\nembedding_dim\\nNote\\nKeep in mind that only a limited number of optimizers support\\nsparse gradients: currently it’s\\noptim.SGD\\n(\\nCUDA\\nand\\nCPU\\n),\\noptim.SparseAdam\\n(\\nCUDA\\nand\\nCPU\\n) and\\noptim.Adagrad\\n(\\nCPU\\n)\\nNote\\nWhen\\nmax_norm\\nis not\\nNone\\n,\\nEmbedding\\n’s forward method will modify the\\nweight\\ntensor in-place. Since tensors needed for gradient computations cannot be'),\n",
       " Document(metadata={}, page_content='modified in-place, performing a differentiable operation on\\nEmbedding.weight\\nbefore\\ncalling\\nEmbedding\\n’s forward method requires cloning\\nEmbedding.weight\\nwhen\\nmax_norm\\nis not\\nNone\\n. For example:\\nn\\n,\\nd\\n,\\nm\\n=\\n3\\n,\\n5\\n,\\n7\\nembedding\\n=\\nnn\\n.\\nEmbedding\\n(\\nn\\n,\\nd\\n,\\nmax_norm\\n=\\n1.0\\n)\\nW\\n=\\ntorch\\n.\\nrandn\\n((\\nm\\n,\\nd\\n),\\nrequires_grad\\n=\\nTrue\\n)\\nidx\\n=\\ntorch\\n.\\ntensor\\n([\\n1\\n,\\n2\\n])\\na\\n=\\nembedding\\n.\\nweight\\n.\\nclone\\n()\\n@\\nW\\n.\\nt\\n()\\n# weight must be cloned for this to be differentiable\\nb\\n=\\nembedding\\n(\\nidx\\n)\\n@\\nW\\n.\\nt\\n()'),\n",
       " Document(metadata={}, page_content='b\\n=\\nembedding\\n(\\nidx\\n)\\n@\\nW\\n.\\nt\\n()\\n# modifies weight in-place\\nout\\n=\\n(\\na\\n.\\nunsqueeze\\n(\\n0\\n)\\n+\\nb\\n.\\nunsqueeze\\n(\\n1\\n))\\nloss\\n=\\nout\\n.\\nsigmoid\\n()\\n.\\nprod\\n()\\nloss\\n.\\nbackward\\n()\\nExamples:\\n>>>\\n# an Embedding module containing 10 tensors of size 3\\n>>>\\nembedding\\n=\\nnn\\n.\\nEmbedding\\n(\\n10\\n,\\n3\\n)\\n>>>\\n# a batch of 2 samples of 4 indices each\\n>>>\\ninput\\n=\\ntorch\\n.\\nLongTensor\\n([[\\n1\\n,\\n2\\n,\\n4\\n,\\n5\\n],\\n[\\n4\\n,\\n3\\n,\\n2\\n,\\n9\\n]])\\n>>>\\nembedding\\n(\\ninput\\n)\\ntensor([[[-0.0251, -1.6902,  0.7172],\\n[-0.6431,  0.0748,  0.6969],'),\n",
       " Document(metadata={}, page_content='[-0.6431,  0.0748,  0.6969],\\n[ 1.4970,  1.3448, -0.9685],\\n[-0.3677, -2.7265, -0.1685]],\\n[[ 1.4970,  1.3448, -0.9685],\\n[ 0.4362, -0.4004,  0.9400],\\n[-0.6431,  0.0748,  0.6969],\\n[ 0.9124, -2.3616,  1.1151]]])\\n>>>\\n# example with padding_idx\\n>>>\\nembedding\\n=\\nnn\\n.\\nEmbedding\\n(\\n10\\n,\\n3\\n,\\npadding_idx\\n=\\n0\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\nLongTensor\\n([[\\n0\\n,\\n2\\n,\\n0\\n,\\n5\\n]])\\n>>>\\nembedding\\n(\\ninput\\n)\\ntensor([[[ 0.0000,  0.0000,  0.0000],\\n[ 0.1535, -2.0309,  0.9315],\\n[ 0.0000,  0.0000,  0.0000],'),\n",
       " Document(metadata={}, page_content='[ 0.0000,  0.0000,  0.0000],\\n[-0.1655,  0.9897,  0.0635]]])\\n>>>\\n# example of changing `pad` vector\\n>>>\\npadding_idx\\n=\\n0\\n>>>\\nembedding\\n=\\nnn\\n.\\nEmbedding\\n(\\n3\\n,\\n3\\n,\\npadding_idx\\n=\\npadding_idx\\n)\\n>>>\\nembedding\\n.\\nweight\\nParameter containing:\\ntensor([[ 0.0000,  0.0000,  0.0000],\\n[-0.7895, -0.7089, -0.0364],\\n[ 0.6778,  0.5803,  0.2678]], requires_grad=True)\\n>>>\\nwith\\ntorch\\n.\\nno_grad\\n():\\n...\\nembedding\\n.\\nweight\\n[\\npadding_idx\\n]\\n=\\ntorch\\n.\\nones\\n(\\n3\\n)\\n>>>\\nembedding\\n.\\nweight\\nParameter containing:'),\n",
       " Document(metadata={}, page_content='3\\n)\\n>>>\\nembedding\\n.\\nweight\\nParameter containing:\\ntensor([[ 1.0000,  1.0000,  1.0000],\\n[-0.7895, -0.7089, -0.0364],\\n[ 0.6778,  0.5803,  0.2678]], requires_grad=True)\\nclassmethod\\nfrom_pretrained\\n(\\nembeddings\\n,\\nfreeze\\n=\\nTrue\\n,\\npadding_idx\\n=\\nNone\\n,\\nmax_norm\\n=\\nNone\\n,\\nnorm_type\\n=\\n2.0\\n,\\nscale_grad_by_freq\\n=\\nFalse\\n,\\nsparse\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nCreate Embedding instance from given 2-dimensional FloatTensor.\\nParameters\\nembeddings\\n(\\nTensor\\n) – FloatTensor containing weights for the Embedding.'),\n",
       " Document(metadata={}, page_content='First dimension is being passed to Embedding as\\nnum_embeddings\\n, second as\\nembedding_dim\\n.\\nfreeze\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, the tensor does not get updated in the learning process.\\nEquivalent to\\nembedding.weight.requires_grad\\n=\\nFalse\\n. Default:\\nTrue\\npadding_idx\\n(\\nint\\n,\\noptional\\n) – If specified, the entries at\\npadding_idx\\ndo not contribute to the gradient;\\ntherefore, the embedding vector at\\npadding_idx\\nis not updated during training,\\ni.e. it remains as a fixed “pad”.\\nmax_norm\\n(\\nfloat\\n,'),\n",
       " Document(metadata={}, page_content='max_norm\\n(\\nfloat\\n,\\noptional\\n) – See module initialization documentation.\\nnorm_type\\n(\\nfloat\\n,\\noptional\\n) – See module initialization documentation. Default\\n2\\n.\\nscale_grad_by_freq\\n(\\nbool\\n,\\noptional\\n) – See module initialization documentation. Default\\nFalse\\n.\\nsparse\\n(\\nbool\\n,\\noptional\\n) – See module initialization documentation.\\nExamples:\\n>>>\\n# FloatTensor containing pretrained weights\\n>>>\\nweight\\n=\\ntorch\\n.\\nFloatTensor\\n([[\\n1\\n,\\n2.3\\n,\\n3\\n],\\n[\\n4\\n,\\n5.1\\n,\\n6.3\\n]])\\n>>>\\nembedding\\n=\\nnn\\n.\\nEmbedding\\n.'),\n",
       " Document(metadata={}, page_content=',\\n5.1\\n,\\n6.3\\n]])\\n>>>\\nembedding\\n=\\nnn\\n.\\nEmbedding\\n.\\nfrom_pretrained\\n(\\nweight\\n)\\n>>>\\n# Get embeddings for index 1\\n>>>\\ninput\\n=\\ntorch\\n.\\nLongTensor\\n([\\n1\\n])\\n>>>\\nembedding\\n(\\ninput\\n)\\ntensor([[ 4.0000,  5.1000,  6.3000]])\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"EmbeddingBag\\n¶\\nclass\\ntorch.nn.\\nEmbeddingBag\\n(\\nnum_embeddings\\n,\\nembedding_dim\\n,\\nmax_norm\\n=\\nNone\\n,\\nnorm_type\\n=\\n2.0\\n,\\nscale_grad_by_freq\\n=\\nFalse\\n,\\nmode\\n=\\n'mean'\\n,\\nsparse\\n=\\nFalse\\n,\\n_weight\\n=\\nNone\\n,\\ninclude_last_offset\\n=\\nFalse\\n,\\npadding_idx\\n=\\nNone\\n,\\ndevice\\n=\\nNone\\n,\\ndtype\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nCompute sums or means of ‘bags’ of embeddings, without instantiating the intermediate embeddings.\\nFor bags of constant length, no\\nper_sample_weights\\n, no indices equal to\\npadding_idx\\n,\"),\n",
       " Document(metadata={}, page_content=', no indices equal to\\npadding_idx\\n,\\nand with 2D inputs, this class\\nwith\\nmode=\"sum\"\\nis equivalent to\\nEmbedding\\nfollowed by\\ntorch.sum(dim=1)\\n,\\nwith\\nmode=\"mean\"\\nis equivalent to\\nEmbedding\\nfollowed by\\ntorch.mean(dim=1)\\n,\\nwith\\nmode=\"max\"\\nis equivalent to\\nEmbedding\\nfollowed by\\ntorch.max(dim=1)\\n.\\nHowever,\\nEmbeddingBag\\nis much more time and memory efficient than using a chain of these\\noperations.\\nEmbeddingBag also supports per-sample weights as an argument to the forward'),\n",
       " Document(metadata={}, page_content='pass. This scales the output of the Embedding before performing a weighted\\nreduction as specified by\\nmode\\n. If\\nper_sample_weights\\nis passed, the\\nonly supported\\nmode\\nis\\n\"sum\"\\n, which computes a weighted sum according to\\nper_sample_weights\\n.\\nParameters\\nnum_embeddings\\n(\\nint\\n) – size of the dictionary of embeddings\\nembedding_dim\\n(\\nint\\n) – the size of each embedding vector\\nmax_norm\\n(\\nfloat\\n,\\noptional\\n) – If given, each embedding vector with norm larger than\\nmax_norm\\nis renormalized to have norm'),\n",
       " Document(metadata={}, page_content='max_norm\\nis renormalized to have norm\\nmax_norm\\n.\\nnorm_type\\n(\\nfloat\\n,\\noptional\\n) – The p of the p-norm to compute for the\\nmax_norm\\noption. Default\\n2\\n.\\nscale_grad_by_freq\\n(\\nbool\\n,\\noptional\\n) – if given, this will scale gradients by the inverse of frequency of\\nthe words in the mini-batch. Default\\nFalse\\n.\\nNote: this option is not supported when\\nmode=\"max\"\\n.\\nmode\\n(\\nstr\\n,\\noptional\\n) –\\n\"sum\"\\n,\\n\"mean\"\\nor\\n\"max\"\\n. Specifies the way to reduce the bag.\\n\"sum\"\\ncomputes the weighted sum, taking'),\n",
       " Document(metadata={}, page_content='\"sum\"\\ncomputes the weighted sum, taking\\nper_sample_weights\\ninto consideration.\\n\"mean\"\\ncomputes the average of the values\\nin the bag,\\n\"max\"\\ncomputes the max value over each bag.\\nDefault:\\n\"mean\"\\nsparse\\n(\\nbool\\n,\\noptional\\n) – if\\nTrue\\n, gradient w.r.t.\\nweight\\nmatrix will be a sparse tensor. See\\nNotes for more details regarding sparse gradients. Note: this option is not\\nsupported when\\nmode=\"max\"\\n.\\ninclude_last_offset\\n(\\nbool\\n,\\noptional\\n) – if\\nTrue\\n,\\noffsets'),\n",
       " Document(metadata={}, page_content='(\\nbool\\n,\\noptional\\n) – if\\nTrue\\n,\\noffsets\\nhas one additional element, where the last element\\nis equivalent to the size of\\nindices\\n. This matches the CSR format.\\npadding_idx\\n(\\nint\\n,\\noptional\\n) – If specified, the entries at\\npadding_idx\\ndo not contribute to the\\ngradient; therefore, the embedding vector at\\npadding_idx\\nis not updated\\nduring training, i.e. it remains as a fixed “pad”. For a newly constructed\\nEmbeddingBag, the embedding vector at\\npadding_idx\\nwill default to all'),\n",
       " Document(metadata={}, page_content=\"padding_idx\\nwill default to all\\nzeros, but can be updated to another value to be used as the padding vector.\\nNote that the embedding vector at\\npadding_idx\\nis excluded from the\\nreduction.\\nVariables\\nweight\\n(\\nTensor\\n) – the learnable weights of the module of shape\\n(num_embeddings, embedding_dim)\\ninitialized from\\nN\\n(\\n0\\n,\\n1\\n)\\n\\\\mathcal{N}(0, 1)\\nN\\n(\\n0\\n,\\n1\\n)\\n.\\nExamples:\\n>>>\\n# an EmbeddingBag module containing 10 tensors of size 3\\n>>>\\nembedding_sum\\n=\\nnn\\n.\\nEmbeddingBag\\n(\\n10\\n,\\n3\\n,\\nmode\\n=\\n'sum'\\n)\\n>>>\"),\n",
       " Document(metadata={}, page_content=\"=\\nnn\\n.\\nEmbeddingBag\\n(\\n10\\n,\\n3\\n,\\nmode\\n=\\n'sum'\\n)\\n>>>\\n# a batch of 2 samples of 4 indices each\\n>>>\\ninput\\n=\\ntorch\\n.\\ntensor\\n([\\n1\\n,\\n2\\n,\\n4\\n,\\n5\\n,\\n4\\n,\\n3\\n,\\n2\\n,\\n9\\n],\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\noffsets\\n=\\ntorch\\n.\\ntensor\\n([\\n0\\n,\\n4\\n],\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\nembedding_sum\\n(\\ninput\\n,\\noffsets\\n)\\ntensor([[-0.8861, -5.4350, -0.0523],\\n[ 1.1306, -2.5798, -1.0044]])\\n>>>\\n# Example with padding_idx\\n>>>\\nembedding_sum\\n=\\nnn\\n.\\nEmbeddingBag\\n(\\n10\\n,\\n3\\n,\\nmode\\n=\\n'sum'\\n,\\npadding_idx\\n=\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\ntensor\\n([\\n2\\n,\\n2\\n,\\n2\"),\n",
       " Document(metadata={}, page_content='=\\n2\\n)\\n>>>\\ninput\\n=\\ntorch\\n.\\ntensor\\n([\\n2\\n,\\n2\\n,\\n2\\n,\\n2\\n,\\n4\\n,\\n3\\n,\\n2\\n,\\n9\\n],\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\noffsets\\n=\\ntorch\\n.\\ntensor\\n([\\n0\\n,\\n4\\n],\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\nembedding_sum\\n(\\ninput\\n,\\noffsets\\n)\\ntensor([[ 0.0000,  0.0000,  0.0000],\\n[-0.7082,  3.2145, -2.6251]])\\n>>>\\n# An EmbeddingBag can be loaded from an Embedding like so\\n>>>\\nembedding\\n=\\nnn\\n.\\nEmbedding\\n(\\n10\\n,\\n3\\n,\\npadding_idx\\n=\\n2\\n)\\n>>>\\nembedding_sum\\n=\\nnn\\n.\\nEmbeddingBag\\n.\\nfrom_pretrained\\n(\\nembedding.weight,\\npadding_idx=embedding.padding_idx,'),\n",
       " Document(metadata={}, page_content=\"padding_idx=embedding.padding_idx,\\nmode='sum')\\nforward\\n(\\ninput\\n,\\noffsets\\n=\\nNone\\n,\\nper_sample_weights\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nForward pass of EmbeddingBag.\\nParameters\\ninput\\n(\\nTensor\\n) – Tensor containing bags of indices into the embedding matrix.\\noffsets\\n(\\nTensor\\n,\\noptional\\n) – Only used when\\ninput\\nis 1D.\\noffsets\\ndetermines\\nthe starting index position of each bag (sequence) in\\ninput\\n.\\nper_sample_weights\\n(\\nTensor\\n,\\noptional\\n) – a tensor of float / double weights, or None\"),\n",
       " Document(metadata={}, page_content=\") – a tensor of float / double weights, or None\\nto indicate all weights should be taken to be\\n1\\n. If specified,\\nper_sample_weights\\nmust have exactly the same shape as input and is treated as having the same\\noffsets\\n, if those are not\\nNone\\n. Only supported for\\nmode='sum'\\n.\\nReturns\\nTensor output shape of\\n(B, embedding_dim)\\n.\\nReturn type\\nTensor\\nNote\\nA few notes about\\ninput\\nand\\noffsets\\n:\\ninput\\nand\\noffsets\\nhave to be of the same type, either int or long\\nIf\\ninput\\nis 2D of shape\\n(B, N)\"),\n",
       " Document(metadata={}, page_content='If\\ninput\\nis 2D of shape\\n(B, N)\\n, it will be treated as\\nB\\nbags (sequences)\\neach of fixed length\\nN\\n, and this will return\\nB\\nvalues aggregated in a way\\ndepending on the\\nmode\\n.\\noffsets\\nis ignored and required to be\\nNone\\nin this case.\\nIf\\ninput\\nis 1D of shape\\n(N)\\n, it will be treated as a concatenation of\\nmultiple bags (sequences).\\noffsets\\nis required to be a 1D tensor containing the\\nstarting index positions of each bag in\\ninput\\n. Therefore, for\\noffsets\\nof shape\\n(B)\\n,\\ninput\\nwill be viewed as having\\nB'),\n",
       " Document(metadata={}, page_content=\"of shape\\n(B)\\n,\\ninput\\nwill be viewed as having\\nB\\nbags. Empty bags (i.e., having 0-length) will have\\nreturned vectors filled by zeros.\\nclassmethod\\nfrom_pretrained\\n(\\nembeddings\\n,\\nfreeze\\n=\\nTrue\\n,\\nmax_norm\\n=\\nNone\\n,\\nnorm_type\\n=\\n2.0\\n,\\nscale_grad_by_freq\\n=\\nFalse\\n,\\nmode\\n=\\n'mean'\\n,\\nsparse\\n=\\nFalse\\n,\\ninclude_last_offset\\n=\\nFalse\\n,\\npadding_idx\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nCreate EmbeddingBag instance from given 2-dimensional FloatTensor.\\nParameters\\nembeddings\\n(\\nTensor\"),\n",
       " Document(metadata={}, page_content='Parameters\\nembeddings\\n(\\nTensor\\n) – FloatTensor containing weights for the EmbeddingBag.\\nFirst dimension is being passed to EmbeddingBag as ‘num_embeddings’, second as ‘embedding_dim’.\\nfreeze\\n(\\nbool\\n,\\noptional\\n) – If\\nTrue\\n, the tensor does not get updated in the learning process.\\nEquivalent to\\nembeddingbag.weight.requires_grad\\n=\\nFalse\\n. Default:\\nTrue\\nmax_norm\\n(\\nfloat\\n,\\noptional\\n) – See module initialization documentation. Default:\\nNone\\nnorm_type\\n(\\nfloat\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='None\\nnorm_type\\n(\\nfloat\\n,\\noptional\\n) – See module initialization documentation. Default\\n2\\n.\\nscale_grad_by_freq\\n(\\nbool\\n,\\noptional\\n) – See module initialization documentation. Default\\nFalse\\n.\\nmode\\n(\\nstr\\n,\\noptional\\n) – See module initialization documentation. Default:\\n\"mean\"\\nsparse\\n(\\nbool\\n,\\noptional\\n) – See module initialization documentation. Default:\\nFalse\\n.\\ninclude_last_offset\\n(\\nbool\\n,\\noptional\\n) – See module initialization documentation. Default:\\nFalse\\n.\\npadding_idx\\n(\\nint\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='False\\n.\\npadding_idx\\n(\\nint\\n,\\noptional\\n) – See module initialization documentation. Default:\\nNone\\n.\\nReturn type\\nEmbeddingBag\\nExamples:\\n>>>\\n# FloatTensor containing pretrained weights\\n>>>\\nweight\\n=\\ntorch\\n.\\nFloatTensor\\n([[\\n1\\n,\\n2.3\\n,\\n3\\n],\\n[\\n4\\n,\\n5.1\\n,\\n6.3\\n]])\\n>>>\\nembeddingbag\\n=\\nnn\\n.\\nEmbeddingBag\\n.\\nfrom_pretrained\\n(\\nweight\\n)\\n>>>\\n# Get embeddings for index 1\\n>>>\\ninput\\n=\\ntorch\\n.\\nLongTensor\\n([[\\n1\\n,\\n0\\n]])\\n>>>\\nembeddingbag\\n(\\ninput\\n)\\ntensor([[ 2.5000,  3.7000,  4.6500]])\\nNext\\nPrevious'),\n",
       " Document(metadata={}, page_content='Next\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='CosineSimilarity\\n¶\\nclass\\ntorch.nn.\\nCosineSimilarity\\n(\\ndim\\n=\\n1\\n,\\neps\\n=\\n1e-08\\n)\\n[source]\\n[source]\\n¶\\nReturns cosine similarity between\\nx\\n1\\nx_1\\nx\\n1\\n\\u200b\\nand\\nx\\n2\\nx_2\\nx\\n2\\n\\u200b\\n, computed along\\ndim\\n.\\nsimilarity\\n=\\nx\\n1\\n⋅\\nx\\n2\\nmax\\n\\u2061\\n(\\n∥\\nx\\n1\\n∥\\n2\\n⋅\\n∥\\nx\\n2\\n∥\\n2\\n,\\nϵ\\n)\\n.\\n\\\\text{similarity} = \\\\dfrac{x_1 \\\\cdot x_2}{\\\\max(\\\\Vert x_1 \\\\Vert _2 \\\\cdot \\\\Vert x_2 \\\\Vert _2, \\\\epsilon)}.\\nsimilarity\\n=\\nmax\\n(\\n∥\\nx\\n1\\n\\u200b\\n∥\\n2\\n\\u200b\\n⋅\\n∥\\nx\\n2\\n\\u200b\\n∥\\n2\\n\\u200b\\n,\\nϵ\\n)\\nx\\n1\\n\\u200b\\n⋅\\nx\\n2\\n\\u200b\\n\\u200b\\n.\\nParameters\\ndim\\n(\\nint\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='x\\n1\\n\\u200b\\n⋅\\nx\\n2\\n\\u200b\\n\\u200b\\n.\\nParameters\\ndim\\n(\\nint\\n,\\noptional\\n) – Dimension where cosine similarity is computed. Default: 1\\neps\\n(\\nfloat\\n,\\noptional\\n) – Small value to avoid division by zero.\\nDefault: 1e-8\\nShape:\\nInput1:\\n(\\n∗\\n1\\n,\\nD\\n,\\n∗\\n2\\n)\\n(\\\\ast_1, D, \\\\ast_2)\\n(\\n∗\\n1\\n\\u200b\\n,\\nD\\n,\\n∗\\n2\\n\\u200b\\n)\\nwhere D is at position\\ndim\\nInput2:\\n(\\n∗\\n1\\n,\\nD\\n,\\n∗\\n2\\n)\\n(\\\\ast_1, D, \\\\ast_2)\\n(\\n∗\\n1\\n\\u200b\\n,\\nD\\n,\\n∗\\n2\\n\\u200b\\n)\\n, same number of dimensions as x1, matching x1 size at dimension\\ndim\\n,\\nand broadcastable with x1 at other dimensions.\\nOutput:\\n(\\n∗\\n1\\n,\\n∗\\n2'),\n",
       " Document(metadata={}, page_content='Output:\\n(\\n∗\\n1\\n,\\n∗\\n2\\n)\\n(\\\\ast_1, \\\\ast_2)\\n(\\n∗\\n1\\n\\u200b\\n,\\n∗\\n2\\n\\u200b\\n)\\nExamples::\\n>>>\\ninput1\\n=\\ntorch\\n.\\nrandn\\n(\\n100\\n,\\n128\\n)\\n>>>\\ninput2\\n=\\ntorch\\n.\\nrandn\\n(\\n100\\n,\\n128\\n)\\n>>>\\ncos\\n=\\nnn\\n.\\nCosineSimilarity\\n(\\ndim\\n=\\n1\\n,\\neps\\n=\\n1e-6\\n)\\n>>>\\noutput\\n=\\ncos\\n(\\ninput1\\n,\\ninput2\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content='PairwiseDistance\\n¶\\nclass\\ntorch.nn.\\nPairwiseDistance\\n(\\np\\n=\\n2.0\\n,\\neps\\n=\\n1e-06\\n,\\nkeepdim\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nComputes the pairwise distance between input vectors, or between columns of input matrices.\\nDistances are computed using\\np\\n-norm, with constant\\neps\\nadded to avoid division by zero\\nif\\np\\nis negative, i.e.:\\nd\\ni\\ns\\nt\\n(\\nx\\n,\\ny\\n)\\n=\\n∥\\nx\\n−\\ny\\n+\\nϵ\\ne\\n∥\\np\\n,\\n\\\\mathrm{dist}\\\\left(x, y\\\\right) = \\\\left\\\\Vert x-y + \\\\epsilon e \\\\right\\\\Vert_p,\\ndist\\n(\\nx\\n,\\ny\\n)\\n=\\n∥\\nx\\n−\\ny\\n+\\nϵe\\n∥\\np\\n\\u200b\\n,\\nwhere\\ne\\ne\\ne'),\n",
       " Document(metadata={}, page_content='dist\\n(\\nx\\n,\\ny\\n)\\n=\\n∥\\nx\\n−\\ny\\n+\\nϵe\\n∥\\np\\n\\u200b\\n,\\nwhere\\ne\\ne\\ne\\nis the vector of ones and the\\np\\n-norm is given by.\\n∥\\nx\\n∥\\np\\n=\\n(\\n∑\\ni\\n=\\n1\\nn\\n∣\\nx\\ni\\n∣\\np\\n)\\n1\\n/\\np\\n.\\n\\\\Vert x \\\\Vert _p = \\\\left( \\\\sum_{i=1}^n  \\\\vert x_i \\\\vert ^ p \\\\right) ^ {1/p}.\\n∥\\nx\\n∥\\np\\n\\u200b\\n=\\n(\\ni\\n=\\n1\\n∑\\nn\\n\\u200b\\n∣\\nx\\ni\\n\\u200b\\n∣\\np\\n)\\n1/\\np\\n.\\nParameters\\np\\n(\\nreal\\n,\\noptional\\n) – the norm degree. Can be negative. Default: 2\\neps\\n(\\nfloat\\n,\\noptional\\n) – Small value to avoid division by zero.\\nDefault: 1e-6\\nkeepdim\\n(\\nbool\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='Default: 1e-6\\nkeepdim\\n(\\nbool\\n,\\noptional\\n) – Determines whether or not to keep the vector dimension.\\nDefault: False\\nShape:\\nInput1:\\n(\\nN\\n,\\nD\\n)\\n(N, D)\\n(\\nN\\n,\\nD\\n)\\nor\\n(\\nD\\n)\\n(D)\\n(\\nD\\n)\\nwhere\\nN = batch dimension\\nand\\nD = vector dimension\\nInput2:\\n(\\nN\\n,\\nD\\n)\\n(N, D)\\n(\\nN\\n,\\nD\\n)\\nor\\n(\\nD\\n)\\n(D)\\n(\\nD\\n)\\n, same shape as the Input1\\nOutput:\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\n)\\n()\\n(\\n)\\nbased on input dimension.\\nIf\\nkeepdim\\nis\\nTrue\\n, then\\n(\\nN\\n,\\n1\\n)\\n(N, 1)\\n(\\nN\\n,\\n1\\n)\\nor\\n(\\n1\\n)\\n(1)\\n(\\n1\\n)\\nbased on input dimension.\\nExamples::\\n>>>\\npdist\\n=\\nnn\\n.'),\n",
       " Document(metadata={}, page_content='Examples::\\n>>>\\npdist\\n=\\nnn\\n.\\nPairwiseDistance\\n(\\np\\n=\\n2\\n)\\n>>>\\ninput1\\n=\\ntorch\\n.\\nrandn\\n(\\n100\\n,\\n128\\n)\\n>>>\\ninput2\\n=\\ntorch\\n.\\nrandn\\n(\\n100\\n,\\n128\\n)\\n>>>\\noutput\\n=\\npdist\\n(\\ninput1\\n,\\ninput2\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"L1Loss\\n¶\\nclass\\ntorch.nn.\\nL1Loss\\n(\\nsize_average\\n=\\nNone\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n)\\n[source]\\n[source]\\n¶\\nCreates a criterion that measures the mean absolute error (MAE) between each element in\\nthe input\\nx\\nx\\nx\\nand target\\ny\\ny\\ny\\n.\\nThe unreduced (i.e. with\\nreduction\\nset to\\n'none'\\n) loss can be described as:\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n,\\n…\\n,\\nl\\nN\\n}\\n⊤\\n,\\nl\\nn\\n=\\n∣\\nx\\nn\\n−\\ny\\nn\\n∣\\n,\\n\\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = \\\\left| x_n - y_n \\\\right|,\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n\\u200b\\n}\\n⊤\\n,\\nl\"),\n",
       " Document(metadata={}, page_content=\"ℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n\\u200b\\n}\\n⊤\\n,\\nl\\nn\\n\\u200b\\n=\\n∣\\nx\\nn\\n\\u200b\\n−\\ny\\nn\\n\\u200b\\n∣\\n,\\nwhere\\nN\\nN\\nN\\nis the batch size. If\\nreduction\\nis not\\n'none'\\n(default\\n'mean'\\n), then:\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nmean\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘mean’;\\nsum\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\\\ell(x, y) =\\n\\\\begin{cases}\\n    \\\\operatorname{mean}(L), & \\\\text{if reduction} = \\\\text{`mean';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  & \\\\text{if reduction} = \\\\text{`sum'.}\\n\\\\end{cases}\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nmean\\n(\\nL\\n)\\n,\\nsum\\n(\\nL\\n)\\n,\\n\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\\nif\\xa0reduction\\n=\"),\n",
       " Document(metadata={}, page_content=\"(\\nL\\n)\\n,\\n\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\u200b\\nx\\nx\\nx\\nand\\ny\\ny\\ny\\nare tensors of arbitrary shapes with a total\\nof\\nN\\nN\\nN\\nelements each.\\nThe sum operation still operates over all the elements, and divides by\\nN\\nN\\nN\\n.\\nThe division by\\nN\\nN\\nN\\ncan be avoided if one sets\\nreduction\\n=\\n'sum'\\n.\\nSupports real-valued and complex-valued inputs.\\nParameters\\nsize_average\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\"),\n",
       " Document(metadata={}, page_content='some losses, there are multiple elements per sample. If the field\\nsize_average\\nis set to\\nFalse\\n, the losses are instead summed for each minibatch. Ignored\\nwhen\\nreduce\\nis\\nFalse\\n. Default:\\nTrue\\nreduce\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non\\nsize_average\\n. When\\nreduce\\nis\\nFalse\\n, returns a loss per\\nbatch element instead and ignores\\nsize_average\\n. Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional'),\n",
       " Document(metadata={}, page_content=\". Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction will be applied,\\n'mean'\\n: the sum of the output will be divided by the number of\\nelements in the output,\\n'sum'\\n: the output will be summed. Note:\\nsize_average\\nand\\nreduce\\nare in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override\\nreduction\\n. Default:\\n'mean'\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\"),\n",
       " Document(metadata={}, page_content=\"Shape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nTarget:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nOutput: scalar. If\\nreduction\\nis\\n'none'\\n, then\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nloss\\n=\\nnn\\n.\\nL1Loss\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n)\\n>>>\\noutput\\n=\\nloss\\n(\\ninput\\n,\\ntarget\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\"),\n",
       " Document(metadata={}, page_content='Built with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"MSELoss\\n¶\\nclass\\ntorch.nn.\\nMSELoss\\n(\\nsize_average\\n=\\nNone\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n)\\n[source]\\n[source]\\n¶\\nCreates a criterion that measures the mean squared error (squared L2 norm) between\\neach element in the input\\nx\\nx\\nx\\nand target\\ny\\ny\\ny\\n.\\nThe unreduced (i.e. with\\nreduction\\nset to\\n'none'\\n) loss can be described as:\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n,\\n…\\n,\\nl\\nN\\n}\\n⊤\\n,\\nl\\nn\\n=\\n(\\nx\\nn\\n−\\ny\\nn\\n)\\n2\\n,\\n\\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = \\\\left( x_n - y_n \\\\right)^2,\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\"),\n",
       " Document(metadata={}, page_content=\"ℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n\\u200b\\n}\\n⊤\\n,\\nl\\nn\\n\\u200b\\n=\\n(\\nx\\nn\\n\\u200b\\n−\\ny\\nn\\n\\u200b\\n)\\n2\\n,\\nwhere\\nN\\nN\\nN\\nis the batch size. If\\nreduction\\nis not\\n'none'\\n(default\\n'mean'\\n), then:\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nmean\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘mean’;\\nsum\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\\\ell(x, y) =\\n\\\\begin{cases}\\n    \\\\operatorname{mean}(L), &  \\\\text{if reduction} = \\\\text{`mean';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  &  \\\\text{if reduction} = \\\\text{`sum'.}\\n\\\\end{cases}\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nmean\\n(\\nL\\n)\\n,\\nsum\\n(\\nL\\n)\\n,\\n\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\"),\n",
       " Document(metadata={}, page_content=\"mean\\n(\\nL\\n)\\n,\\nsum\\n(\\nL\\n)\\n,\\n\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\u200b\\nx\\nx\\nx\\nand\\ny\\ny\\ny\\nare tensors of arbitrary shapes with a total\\nof\\nN\\nN\\nN\\nelements each.\\nThe mean operation still operates over all the elements, and divides by\\nN\\nN\\nN\\n.\\nThe division by\\nN\\nN\\nN\\ncan be avoided if one sets\\nreduction\\n=\\n'sum'\\n.\\nParameters\\nsize_average\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\"),\n",
       " Document(metadata={}, page_content='some losses, there are multiple elements per sample. If the field\\nsize_average\\nis set to\\nFalse\\n, the losses are instead summed for each minibatch. Ignored\\nwhen\\nreduce\\nis\\nFalse\\n. Default:\\nTrue\\nreduce\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non\\nsize_average\\n. When\\nreduce\\nis\\nFalse\\n, returns a loss per\\nbatch element instead and ignores\\nsize_average\\n. Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional'),\n",
       " Document(metadata={}, page_content=\". Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction will be applied,\\n'mean'\\n: the sum of the output will be divided by the number of\\nelements in the output,\\n'sum'\\n: the output will be summed. Note:\\nsize_average\\nand\\nreduce\\nare in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override\\nreduction\\n. Default:\\n'mean'\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\"),\n",
       " Document(metadata={}, page_content='Shape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nTarget:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nExamples:\\n>>>\\nloss\\n=\\nnn\\n.\\nMSELoss\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n)\\n>>>\\noutput\\n=\\nloss\\n(\\ninput\\n,\\ntarget\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"CrossEntropyLoss\\n¶\\nclass\\ntorch.nn.\\nCrossEntropyLoss\\n(\\nweight\\n=\\nNone\\n,\\nsize_average\\n=\\nNone\\n,\\nignore_index\\n=\\n-100\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n,\\nlabel_smoothing\\n=\\n0.0\\n)\\n[source]\\n[source]\\n¶\\nThis criterion computes the cross entropy loss between input logits\\nand target.\\nIt is useful when training a classification problem with\\nC\\nclasses.\\nIf provided, the optional argument\\nweight\\nshould be a 1D\\nTensor\\nassigning weight to each of the classes.\"),\n",
       " Document(metadata={}, page_content='Tensor\\nassigning weight to each of the classes.\\nThis is particularly useful when you have an unbalanced training set.\\nThe\\ninput\\nis expected to contain the unnormalized logits for each class (which do\\nnot\\nneed\\nto be positive or sum to 1, in general).\\ninput\\nhas to be a Tensor of size\\n(\\nC\\n)\\n(C)\\n(\\nC\\n)\\nfor unbatched input,\\n(\\nm\\ni\\nn\\ni\\nb\\na\\nt\\nc\\nh\\n,\\nC\\n)\\n(minibatch, C)\\n(\\nminiba\\nt\\nc\\nh\\n,\\nC\\n)\\nor\\n(\\nm\\ni\\nn\\ni\\nb\\na\\nt\\nc\\nh\\n,\\nC\\n,\\nd\\n1\\n,\\nd\\n2\\n,\\n.\\n.\\n.\\n,\\nd\\nK\\n)\\n(minibatch, C, d_1, d_2, ..., d_K)\\n(\\nminiba\\nt\\nc\\nh\\n,\\nC\\n,\\nd\\n1\\n\\u200b'),\n",
       " Document(metadata={}, page_content='(\\nminiba\\nt\\nc\\nh\\n,\\nC\\n,\\nd\\n1\\n\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\\n...\\n,\\nd\\nK\\n\\u200b\\n)\\nwith\\nK\\n≥\\n1\\nK \\\\geq 1\\nK\\n≥\\n1\\nfor the\\nK\\n-dimensional case. The last being useful for higher dimension inputs, such\\nas computing cross entropy loss per-pixel for 2D images.\\nThe\\ntarget\\nthat this criterion expects should contain either:\\nClass indices in the range\\n[\\n0\\n,\\nC\\n)\\n[0, C)\\n[\\n0\\n,\\nC\\n)\\nwhere\\nC\\nC\\nC\\nis the number of classes; if\\nignore_index\\nis specified, this loss also accepts this class index (this index'),\n",
       " Document(metadata={}, page_content=\"may not necessarily be in the class range). The unreduced (i.e. with\\nreduction\\nset to\\n'none'\\n) loss for this case can be described as:\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n,\\n…\\n,\\nl\\nN\\n}\\n⊤\\n,\\nl\\nn\\n=\\n−\\nw\\ny\\nn\\nlog\\n\\u2061\\nexp\\n\\u2061\\n(\\nx\\nn\\n,\\ny\\nn\\n)\\n∑\\nc\\n=\\n1\\nC\\nexp\\n\\u2061\\n(\\nx\\nn\\n,\\nc\\n)\\n⋅\\n1\\n{\\ny\\nn\\n≠\\nignore_index\\n}\\n\\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - w_{y_n} \\\\log \\\\frac{\\\\exp(x_{n,y_n})}{\\\\sum_{c=1}^C \\\\exp(x_{n,c})}\\n\\\\cdot \\\\mathbb{1}\\\\{y_n \\\\not= \\\\text{ignore\\\\_index}\\\\}\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n\\u200b\\n}\\n⊤\\n,\\nl\\nn\\n\\u200b\\n=\\n−\\nw\\ny\"),\n",
       " Document(metadata={}, page_content=\"y\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n\\u200b\\n}\\n⊤\\n,\\nl\\nn\\n\\u200b\\n=\\n−\\nw\\ny\\nn\\n\\u200b\\n\\u200b\\nlo\\ng\\n∑\\nc\\n=\\n1\\nC\\n\\u200b\\nexp\\n(\\nx\\nn\\n,\\nc\\n\\u200b\\n)\\nexp\\n(\\nx\\nn\\n,\\ny\\nn\\n\\u200b\\n\\u200b\\n)\\n\\u200b\\n⋅\\n1\\n{\\ny\\nn\\n\\u200b\\n\\ue020\\n=\\nignore_index\\n}\\nwhere\\nx\\nx\\nx\\nis the input,\\ny\\ny\\ny\\nis the target,\\nw\\nw\\nw\\nis the weight,\\nC\\nC\\nC\\nis the number of classes, and\\nN\\nN\\nN\\nspans the minibatch dimension as well as\\nd\\n1\\n,\\n.\\n.\\n.\\n,\\nd\\nk\\nd_1, ..., d_k\\nd\\n1\\n\\u200b\\n,\\n...\\n,\\nd\\nk\\n\\u200b\\nfor the\\nK\\n-dimensional case. If\\nreduction\\nis not\\n'none'\\n(default\\n'mean'\\n), then\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\n∑\\nn\\n=\\n1\\nN\\n1\\n∑\\nn\\n=\\n1\\nN\\nw\\ny\\nn\\n⋅\\n1\\n{\\ny\\nn\\n≠\\nignore_index\"),\n",
       " Document(metadata={}, page_content=\"=\\n1\\nN\\n1\\n∑\\nn\\n=\\n1\\nN\\nw\\ny\\nn\\n⋅\\n1\\n{\\ny\\nn\\n≠\\nignore_index\\n}\\nl\\nn\\n,\\nif\\xa0reduction\\n=\\n‘mean’;\\n∑\\nn\\n=\\n1\\nN\\nl\\nn\\n,\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\\\ell(x, y) = \\\\begin{cases}\\n    \\\\sum_{n=1}^N \\\\frac{1}{\\\\sum_{n=1}^N w_{y_n} \\\\cdot \\\\mathbb{1}\\\\{y_n \\\\not= \\\\text{ignore\\\\_index}\\\\}} l_n, &\\n     \\\\text{if reduction} = \\\\text{`mean';}\\\\\\\\\\n      \\\\sum_{n=1}^N l_n,  &\\n      \\\\text{if reduction} = \\\\text{`sum'.}\\n  \\\\end{cases}\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\n∑\\nn\\n=\\n1\\nN\\n\\u200b\\n∑\\nn\\n=\\n1\\nN\\n\\u200b\\nw\\ny\\nn\\n\\u200b\\n\\u200b\\n⋅\\n1\\n{\\ny\\nn\\n\\u200b\\n\\ue020\\n=\\nignore_index\\n}\\n1\\n\\u200b\\nl\\nn\\n\\u200b\\n,\\n∑\\nn\\n=\\n1\\nN\\n\\u200b\\nl\\nn\\n\\u200b\\n,\\n\\u200b\"),\n",
       " Document(metadata={}, page_content=\"ignore_index\\n}\\n1\\n\\u200b\\nl\\nn\\n\\u200b\\n,\\n∑\\nn\\n=\\n1\\nN\\n\\u200b\\nl\\nn\\n\\u200b\\n,\\n\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\u200b\\nNote that this case is equivalent to applying\\nLogSoftmax\\non an input, followed by\\nNLLLoss\\n.\\nProbabilities for each class; useful when labels beyond a single class per minibatch item\\nare required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\\nreduction\\nset to\\n'none'\\n) loss for this case can be described as:\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n,\\n…\\n,\\nl\\nN\\n}\\n⊤\\n,\\nl\\nn\\n=\\n−\\n∑\\nc\\n=\\n1\\nC\\nw\\nc\\nlog\"),\n",
       " Document(metadata={}, page_content='=\\n{\\nl\\n1\\n,\\n…\\n,\\nl\\nN\\n}\\n⊤\\n,\\nl\\nn\\n=\\n−\\n∑\\nc\\n=\\n1\\nC\\nw\\nc\\nlog\\n\\u2061\\nexp\\n\\u2061\\n(\\nx\\nn\\n,\\nc\\n)\\n∑\\ni\\n=\\n1\\nC\\nexp\\n\\u2061\\n(\\nx\\nn\\n,\\ni\\n)\\ny\\nn\\n,\\nc\\n\\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - \\\\sum_{c=1}^C w_c \\\\log \\\\frac{\\\\exp(x_{n,c})}{\\\\sum_{i=1}^C \\\\exp(x_{n,i})} y_{n,c}\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n\\u200b\\n}\\n⊤\\n,\\nl\\nn\\n\\u200b\\n=\\n−\\nc\\n=\\n1\\n∑\\nC\\n\\u200b\\nw\\nc\\n\\u200b\\nlo\\ng\\n∑\\ni\\n=\\n1\\nC\\n\\u200b\\nexp\\n(\\nx\\nn\\n,\\ni\\n\\u200b\\n)\\nexp\\n(\\nx\\nn\\n,\\nc\\n\\u200b\\n)\\n\\u200b\\ny\\nn\\n,\\nc\\n\\u200b\\nwhere\\nx\\nx\\nx\\nis the input,\\ny\\ny\\ny\\nis the target,\\nw\\nw\\nw\\nis the weight,\\nC\\nC\\nC\\nis the number of classes, and\\nN\\nN\\nN'),\n",
       " Document(metadata={}, page_content=\"C\\nC\\nC\\nis the number of classes, and\\nN\\nN\\nN\\nspans the minibatch dimension as well as\\nd\\n1\\n,\\n.\\n.\\n.\\n,\\nd\\nk\\nd_1, ..., d_k\\nd\\n1\\n\\u200b\\n,\\n...\\n,\\nd\\nk\\n\\u200b\\nfor the\\nK\\n-dimensional case. If\\nreduction\\nis not\\n'none'\\n(default\\n'mean'\\n), then\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\n∑\\nn\\n=\\n1\\nN\\nl\\nn\\nN\\n,\\nif\\xa0reduction\\n=\\n‘mean’;\\n∑\\nn\\n=\\n1\\nN\\nl\\nn\\n,\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\\\ell(x, y) = \\\\begin{cases}\\n    \\\\frac{\\\\sum_{n=1}^N l_n}{N}, &\\n     \\\\text{if reduction} = \\\\text{`mean';}\\\\\\\\\\n      \\\\sum_{n=1}^N l_n,  &\\n      \\\\text{if reduction} = \\\\text{`sum'.}\\n  \\\\end{cases}\\nℓ\"),\n",
       " Document(metadata={}, page_content='\\\\end{cases}\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nN\\n∑\\nn\\n=\\n1\\nN\\n\\u200b\\nl\\nn\\n\\u200b\\n\\u200b\\n,\\n∑\\nn\\n=\\n1\\nN\\n\\u200b\\nl\\nn\\n\\u200b\\n,\\n\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\u200b\\nNote\\nThe performance of this criterion is generally better when\\ntarget\\ncontains class\\nindices, as this allows for optimized computation. Consider providing\\ntarget\\nas\\nclass probabilities only when a single class label per minibatch item is too restrictive.\\nParameters\\nweight\\n(\\nTensor\\n,\\noptional\\n) – a manual rescaling weight given to each class.'),\n",
       " Document(metadata={}, page_content='If given, has to be a Tensor of size\\nC\\nand floating point dtype\\nsize_average\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field\\nsize_average\\nis set to\\nFalse\\n, the losses are instead summed for each minibatch. Ignored\\nwhen\\nreduce\\nis\\nFalse\\n. Default:\\nTrue\\nignore_index\\n(\\nint\\n,\\noptional\\n) – Specifies a target value that is ignored'),\n",
       " Document(metadata={}, page_content=') – Specifies a target value that is ignored\\nand does not contribute to the input gradient. When\\nsize_average\\nis\\nTrue\\n, the loss is averaged over non-ignored targets. Note that\\nignore_index\\nis only applicable when the target contains class indices.\\nreduce\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non\\nsize_average\\n. When\\nreduce\\nis\\nFalse\\n, returns a loss per\\nbatch element instead and ignores'),\n",
       " Document(metadata={}, page_content=\"batch element instead and ignores\\nsize_average\\n. Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction will\\nbe applied,\\n'mean'\\n: the weighted mean of the output is taken,\\n'sum'\\n: the output will be summed. Note:\\nsize_average\\nand\\nreduce\\nare in the process of being deprecated, and in\\nthe meantime, specifying either of those two args will override\\nreduction\\n. Default:\\n'mean'\\nlabel_smoothing\\n(\\nfloat\\n,\\noptional\"),\n",
       " Document(metadata={}, page_content=\"'mean'\\nlabel_smoothing\\n(\\nfloat\\n,\\noptional\\n) – A float in [0.0, 1.0]. Specifies the amount\\nof smoothing when computing the loss, where 0.0 means no smoothing. The targets\\nbecome a mixture of the original ground truth and a uniform distribution as described in\\nRethinking the Inception Architecture for Computer Vision\\n. Default:\\n0.0\\n0.0\\n0.0\\n.\\nShape:\\nInput: Shape\\n(\\nC\\n)\\n(C)\\n(\\nC\\n)\\n,\\n(\\nN\\n,\\nC\\n)\\n(N, C)\\n(\\nN\\n,\\nC\\n)\\nor\\n(\\nN\\n,\\nC\\n,\\nd\\n1\\n,\\nd\\n2\\n,\\n.\\n.\\n.\\n,\\nd\\nK\\n)\\n(N, C, d_1, d_2, ..., d_K)\\n(\\nN\\n,\\nC\\n,\\nd\\n1\\n\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\"),\n",
       " Document(metadata={}, page_content='(\\nN\\n,\\nC\\n,\\nd\\n1\\n\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\\n...\\n,\\nd\\nK\\n\\u200b\\n)\\nwith\\nK\\n≥\\n1\\nK \\\\geq 1\\nK\\n≥\\n1\\nin the case of\\nK\\n-dimensional loss.\\nTarget: If containing class indices, shape\\n(\\n)\\n()\\n(\\n)\\n,\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\nN\\n,\\nd\\n1\\n,\\nd\\n2\\n,\\n.\\n.\\n.\\n,\\nd\\nK\\n)\\n(N, d_1, d_2, ..., d_K)\\n(\\nN\\n,\\nd\\n1\\n\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\\n...\\n,\\nd\\nK\\n\\u200b\\n)\\nwith\\nK\\n≥\\n1\\nK \\\\geq 1\\nK\\n≥\\n1\\nin the case of K-dimensional loss where each value should be between\\n[\\n0\\n,\\nC\\n)\\n[0, C)\\n[\\n0\\n,\\nC\\n)\\n.\\nIf containing class probabilities, same shape as the input and each value should be between\\n[\\n0\\n,\\n1\\n]'),\n",
       " Document(metadata={}, page_content='[\\n0\\n,\\n1\\n]\\n[0, 1]\\n[\\n0\\n,\\n1\\n]\\n.\\nOutput: If reduction is ‘none’, shape\\n(\\n)\\n()\\n(\\n)\\n,\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\nN\\n,\\nd\\n1\\n,\\nd\\n2\\n,\\n.\\n.\\n.\\n,\\nd\\nK\\n)\\n(N, d_1, d_2, ..., d_K)\\n(\\nN\\n,\\nd\\n1\\n\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\\n...\\n,\\nd\\nK\\n\\u200b\\n)\\nwith\\nK\\n≥\\n1\\nK \\\\geq 1\\nK\\n≥\\n1\\nin the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\\nwhere:\\nC\\n=\\nnumber\\xa0of\\xa0classes\\nN\\n=\\nbatch\\xa0size\\n\\\\begin{aligned}\\n    C ={} & \\\\text{number of classes} \\\\\\\\\\n    N ={} & \\\\text{batch size} \\\\\\\\\\n\\\\end{aligned}\\nC\\n=\\nN\\n=\\n\\u200b\\nnumber\\xa0of\\xa0classes\\nbatch\\xa0size\\n\\u200b'),\n",
       " Document(metadata={}, page_content='C\\n=\\nN\\n=\\n\\u200b\\nnumber\\xa0of\\xa0classes\\nbatch\\xa0size\\n\\u200b\\nExamples:\\n>>>\\n# Example of target with class indices\\n>>>\\nloss\\n=\\nnn\\n.\\nCrossEntropyLoss\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nempty\\n(\\n3\\n,\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n.\\nrandom_\\n(\\n5\\n)\\n>>>\\noutput\\n=\\nloss\\n(\\ninput\\n,\\ntarget\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\n>>>\\n>>>\\n# Example of target with class probabilities\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n)\\n.\\nsoftmax\\n(\\ndim\\n=\\n1\\n)\\n>>>'),\n",
       " Document(metadata={}, page_content='torch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n)\\n.\\nsoftmax\\n(\\ndim\\n=\\n1\\n)\\n>>>\\noutput\\n=\\nloss\\n(\\ninput\\n,\\ntarget\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"CTCLoss\\n¶\\nclass\\ntorch.nn.\\nCTCLoss\\n(\\nblank\\n=\\n0\\n,\\nreduction\\n=\\n'mean'\\n,\\nzero_infinity\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nThe Connectionist Temporal Classification loss.\\nCalculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the\\nprobability of possible alignments of input to target, producing a loss value which is differentiable\\nwith respect to each input node. The alignment of input to target is assumed to be “many-to-one”, which\"),\n",
       " Document(metadata={}, page_content=\"limits the length of the target sequence such that it must be\\n≤\\n\\\\leq\\n≤\\nthe input length.\\nParameters\\nblank\\n(\\nint\\n,\\noptional\\n) – blank label. Default\\n0\\n0\\n0\\n.\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction will be applied,\\n'mean'\\n: the output losses will be divided by the target lengths and\\nthen the mean over the batch is taken,\\n'sum'\\n: the output losses will be summed.\\nDefault:\\n'mean'\\nzero_infinity\\n(\\nbool\\n,\\noptional\"),\n",
       " Document(metadata={}, page_content=\"Default:\\n'mean'\\nzero_infinity\\n(\\nbool\\n,\\noptional\\n) – Whether to zero infinite losses and the associated gradients.\\nDefault:\\nFalse\\nInfinite losses mainly occur when the inputs are too short\\nto be aligned to the targets.\\nShape:\\nLog_probs: Tensor of size\\n(\\nT\\n,\\nN\\n,\\nC\\n)\\n(T, N, C)\\n(\\nT\\n,\\nN\\n,\\nC\\n)\\nor\\n(\\nT\\n,\\nC\\n)\\n(T, C)\\n(\\nT\\n,\\nC\\n)\\n,\\nwhere\\nT\\n=\\ninput\\xa0length\\nT = \\\\text{input length}\\nT\\n=\\ninput\\xa0length\\n,\\nN\\n=\\nbatch\\xa0size\\nN = \\\\text{batch size}\\nN\\n=\\nbatch\\xa0size\\n, and\\nC\\n=\\nnumber\\xa0of\\xa0classes\\xa0(including\\xa0blank)\"),\n",
       " Document(metadata={}, page_content=', and\\nC\\n=\\nnumber\\xa0of\\xa0classes\\xa0(including\\xa0blank)\\nC = \\\\text{number of classes (including blank)}\\nC\\n=\\nnumber\\xa0of\\xa0classes\\xa0(including\\xa0blank)\\n.\\nThe logarithmized probabilities of the outputs (e.g. obtained with\\ntorch.nn.functional.log_softmax()\\n).\\nTargets: Tensor of size\\n(\\nN\\n,\\nS\\n)\\n(N, S)\\n(\\nN\\n,\\nS\\n)\\nor\\n(\\nsum\\n\\u2061\\n(\\ntarget_lengths\\n)\\n)\\n(\\\\operatorname{sum}(\\\\text{target\\\\_lengths}))\\n(\\nsum\\n(\\ntarget_lengths\\n))\\n,\\nwhere\\nN\\n=\\nbatch\\xa0size\\nN = \\\\text{batch size}\\nN\\n=\\nbatch\\xa0size\\nand\\nS\\n=\\nmax\\xa0target\\xa0length,\\xa0if\\xa0shape\\xa0is\\n(\\nN\\n,\\nS'),\n",
       " Document(metadata={}, page_content='and\\nS\\n=\\nmax\\xa0target\\xa0length,\\xa0if\\xa0shape\\xa0is\\n(\\nN\\n,\\nS\\n)\\nS = \\\\text{max target length, if shape is } (N, S)\\nS\\n=\\nmax\\xa0target\\xa0length,\\xa0if\\xa0shape\\xa0is\\n(\\nN\\n,\\nS\\n)\\n.\\nIt represents the target sequences. Each element in the target\\nsequence is a class index. And the target index cannot be blank (default=0).\\nIn the\\n(\\nN\\n,\\nS\\n)\\n(N, S)\\n(\\nN\\n,\\nS\\n)\\nform, targets are padded to the\\nlength of the longest sequence, and stacked.\\nIn the\\n(\\nsum\\n\\u2061\\n(\\ntarget_lengths\\n)\\n)\\n(\\\\operatorname{sum}(\\\\text{target\\\\_lengths}))\\n(\\nsum\\n('),\n",
       " Document(metadata={}, page_content='(\\nsum\\n(\\ntarget_lengths\\n))\\nform,\\nthe targets are assumed to be un-padded and\\nconcatenated within 1 dimension.\\nInput_lengths: Tuple or tensor of size\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\n)\\n()\\n(\\n)\\n,\\nwhere\\nN\\n=\\nbatch\\xa0size\\nN = \\\\text{batch size}\\nN\\n=\\nbatch\\xa0size\\n. It represents the lengths of the\\ninputs (must each be\\n≤\\nT\\n\\\\leq T\\n≤\\nT\\n). And the lengths are specified\\nfor each sequence to achieve masking under the assumption that sequences\\nare padded to equal lengths.\\nTarget_lengths: Tuple or tensor of size\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)'),\n",
       " Document(metadata={}, page_content='(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\n)\\n()\\n(\\n)\\n,\\nwhere\\nN\\n=\\nbatch\\xa0size\\nN = \\\\text{batch size}\\nN\\n=\\nbatch\\xa0size\\n. It represents lengths of the targets.\\nLengths are specified for each sequence to achieve masking under the\\nassumption that sequences are padded to equal lengths. If target shape is\\n(\\nN\\n,\\nS\\n)\\n(N,S)\\n(\\nN\\n,\\nS\\n)\\n, target_lengths are effectively the stop index\\ns\\nn\\ns_n\\ns\\nn\\n\\u200b\\nfor each target sequence, such that\\ntarget_n\\n=\\ntargets[n,0:s_n]\\nfor\\neach target in a batch. Lengths must each be\\n≤\\nS\\n\\\\leq S\\n≤\\nS'),\n",
       " Document(metadata={}, page_content=\"≤\\nS\\n\\\\leq S\\n≤\\nS\\nIf the targets are given as a 1d tensor that is the concatenation of individual\\ntargets, the target_lengths must add up to the total length of the tensor.\\nOutput: scalar if\\nreduction\\nis\\n'mean'\\n(default) or\\n'sum'\\n. If\\nreduction\\nis\\n'none'\\n, then\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nif input is batched or\\n(\\n)\\n()\\n(\\n)\\nif input is unbatched, where\\nN\\n=\\nbatch\\xa0size\\nN = \\\\text{batch size}\\nN\\n=\\nbatch\\xa0size\\n.\\nExamples:\\n>>>\\n# Target are to be padded\\n>>>\\nT\\n=\\n50\\n# Input sequence length\\n>>>\\nC\\n=\\n20\"),\n",
       " Document(metadata={}, page_content='>>>\\nT\\n=\\n50\\n# Input sequence length\\n>>>\\nC\\n=\\n20\\n# Number of classes (including blank)\\n>>>\\nN\\n=\\n16\\n# Batch size\\n>>>\\nS\\n=\\n30\\n# Target sequence length of longest target in batch (padding length)\\n>>>\\nS_min\\n=\\n10\\n# Minimum target length, for demonstration purposes\\n>>>\\n>>>\\n# Initialize random batch of input vectors, for *size = (T,N,C)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\nT\\n,\\nN\\n,\\nC\\n)\\n.\\nlog_softmax\\n(\\n2\\n)\\n.\\ndetach\\n()\\n.\\nrequires_grad_\\n()\\n>>>\\n>>>\\n# Initialize random batch of targets (0 = blank, 1:C = classes)\\n>>>'),\n",
       " Document(metadata={}, page_content='>>>\\ntarget\\n=\\ntorch\\n.\\nrandint\\n(\\nlow\\n=\\n1\\n,\\nhigh\\n=\\nC\\n,\\nsize\\n=\\n(\\nN\\n,\\nS\\n),\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\n>>>\\ninput_lengths\\n=\\ntorch\\n.\\nfull\\n(\\nsize\\n=\\n(\\nN\\n,),\\nfill_value\\n=\\nT\\n,\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\ntarget_lengths\\n=\\ntorch\\n.\\nrandint\\n(\\nlow\\n=\\nS_min\\n,\\nhigh\\n=\\nS\\n,\\nsize\\n=\\n(\\nN\\n,),\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\nctc_loss\\n=\\nnn\\n.\\nCTCLoss\\n()\\n>>>\\nloss\\n=\\nctc_loss\\n(\\ninput\\n,\\ntarget\\n,\\ninput_lengths\\n,\\ntarget_lengths\\n)\\n>>>\\nloss\\n.\\nbackward\\n()\\n>>>\\n>>>\\n>>>\\n# Target are to be un-padded\\n>>>\\nT\\n=\\n50\\n# Input sequence length\\n>>>\\nC\\n='),\n",
       " Document(metadata={}, page_content='>>>\\nT\\n=\\n50\\n# Input sequence length\\n>>>\\nC\\n=\\n20\\n# Number of classes (including blank)\\n>>>\\nN\\n=\\n16\\n# Batch size\\n>>>\\n>>>\\n# Initialize random batch of input vectors, for *size = (T,N,C)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\nT\\n,\\nN\\n,\\nC\\n)\\n.\\nlog_softmax\\n(\\n2\\n)\\n.\\ndetach\\n()\\n.\\nrequires_grad_\\n()\\n>>>\\ninput_lengths\\n=\\ntorch\\n.\\nfull\\n(\\nsize\\n=\\n(\\nN\\n,),\\nfill_value\\n=\\nT\\n,\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\n>>>\\n# Initialize random batch of targets (0 = blank, 1:C = classes)\\n>>>\\ntarget_lengths\\n=\\ntorch\\n.\\nrandint\\n(\\nlow\\n=\\n1\\n,\\nhigh\\n=\\nT\\n,\\nsize'),\n",
       " Document(metadata={}, page_content='=\\ntorch\\n.\\nrandint\\n(\\nlow\\n=\\n1\\n,\\nhigh\\n=\\nT\\n,\\nsize\\n=\\n(\\nN\\n,),\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandint\\n(\\nlow\\n=\\n1\\n,\\nhigh\\n=\\nC\\n,\\nsize\\n=\\n(\\nsum\\n(\\ntarget_lengths\\n),),\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\nctc_loss\\n=\\nnn\\n.\\nCTCLoss\\n()\\n>>>\\nloss\\n=\\nctc_loss\\n(\\ninput\\n,\\ntarget\\n,\\ninput_lengths\\n,\\ntarget_lengths\\n)\\n>>>\\nloss\\n.\\nbackward\\n()\\n>>>\\n>>>\\n>>>\\n# Target are to be un-padded and unbatched (effectively N=1)\\n>>>\\nT\\n=\\n50\\n# Input sequence length\\n>>>\\nC\\n=\\n20\\n# Number of classes (including blank)\\n>>>\\n>>>'),\n",
       " Document(metadata={}, page_content='20\\n# Number of classes (including blank)\\n>>>\\n>>>\\n# Initialize random batch of input vectors, for *size = (T,C)\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\nT\\n,\\nC\\n)\\n.\\nlog_softmax\\n(\\n1\\n)\\n.\\ndetach\\n()\\n.\\nrequires_grad_\\n()\\n>>>\\ninput_lengths\\n=\\ntorch\\n.\\ntensor\\n(\\nT\\n,\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\n>>>\\n# Initialize random batch of targets (0 = blank, 1:C = classes)\\n>>>\\ntarget_lengths\\n=\\ntorch\\n.\\nrandint\\n(\\nlow\\n=\\n1\\n,\\nhigh\\n=\\nT\\n,\\nsize\\n=\\n(),\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandint\\n(\\nlow\\n=\\n1\\n,\\nhigh\\n=\\nC\\n,\\nsize\\n=\\n('),\n",
       " Document(metadata={}, page_content='=\\ntorch\\n.\\nrandint\\n(\\nlow\\n=\\n1\\n,\\nhigh\\n=\\nC\\n,\\nsize\\n=\\n(\\ntarget_lengths\\n,),\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n>>>\\nctc_loss\\n=\\nnn\\n.\\nCTCLoss\\n()\\n>>>\\nloss\\n=\\nctc_loss\\n(\\ninput\\n,\\ntarget\\n,\\ninput_lengths\\n,\\ntarget_lengths\\n)\\n>>>\\nloss\\n.\\nbackward\\n()\\nReference:\\nA. Graves et al.: Connectionist Temporal Classification:\\nLabelling Unsegmented Sequence Data with Recurrent Neural Networks:\\nhttps://www.cs.toronto.edu/~graves/icml_2006.pdf\\nNote\\nIn order to use CuDNN, the following must be satisfied:\\ntargets\\nmust be'),\n",
       " Document(metadata={}, page_content='targets\\nmust be\\nin concatenated format, all\\ninput_lengths\\nmust be\\nT\\n.\\nb\\nl\\na\\nn\\nk\\n=\\n0\\nblank=0\\nb\\nl\\nank\\n=\\n0\\n,\\ntarget_lengths\\n≤\\n256\\n\\\\leq 256\\n≤\\n256\\n, the integer arguments must be of\\ndtype\\ntorch.int32\\n.\\nThe regular implementation uses the (more common in PyTorch)\\ntorch.long\\ndtype.\\nNote\\nIn some circumstances when using the CUDA backend with CuDNN, this operator\\nmay select a nondeterministic algorithm to increase performance. If this is'),\n",
       " Document(metadata={}, page_content='undesirable, you can try to make the operation deterministic (potentially at\\na performance cost) by setting\\ntorch.backends.cudnn.deterministic\\n=\\nTrue\\n.\\nPlease see the notes on\\nReproducibility\\nfor background.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"NLLLoss\\n¶\\nclass\\ntorch.nn.\\nNLLLoss\\n(\\nweight\\n=\\nNone\\n,\\nsize_average\\n=\\nNone\\n,\\nignore_index\\n=\\n-100\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n)\\n[source]\\n[source]\\n¶\\nThe negative log likelihood loss. It is useful to train a classification\\nproblem with\\nC\\nclasses.\\nIf provided, the optional argument\\nweight\\nshould be a 1D Tensor assigning\\nweight to each of the classes. This is particularly useful when you have an\\nunbalanced training set.\\nThe\\ninput\\ngiven through a forward call is expected to contain\"),\n",
       " Document(metadata={}, page_content='log-probabilities of each class.\\ninput\\nhas to be a Tensor of size either\\n(\\nm\\ni\\nn\\ni\\nb\\na\\nt\\nc\\nh\\n,\\nC\\n)\\n(minibatch, C)\\n(\\nminiba\\nt\\nc\\nh\\n,\\nC\\n)\\nor\\n(\\nm\\ni\\nn\\ni\\nb\\na\\nt\\nc\\nh\\n,\\nC\\n,\\nd\\n1\\n,\\nd\\n2\\n,\\n.\\n.\\n.\\n,\\nd\\nK\\n)\\n(minibatch, C, d_1, d_2, ..., d_K)\\n(\\nminiba\\nt\\nc\\nh\\n,\\nC\\n,\\nd\\n1\\n\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\\n...\\n,\\nd\\nK\\n\\u200b\\n)\\nwith\\nK\\n≥\\n1\\nK \\\\geq 1\\nK\\n≥\\n1\\nfor the\\nK\\n-dimensional case. The latter is useful for\\nhigher dimension inputs, such as computing NLL loss per-pixel for 2D images.'),\n",
       " Document(metadata={}, page_content='Obtaining log-probabilities in a neural network is easily achieved by\\nadding a\\nLogSoftmax\\nlayer in the last layer of your network.\\nYou may use\\nCrossEntropyLoss\\ninstead, if you prefer not to add an extra\\nlayer.\\nThe\\ntarget\\nthat this loss expects should be a class index in the range\\n[\\n0\\n,\\nC\\n−\\n1\\n]\\n[0, C-1]\\n[\\n0\\n,\\nC\\n−\\n1\\n]\\nwhere\\nC = number of classes\\n; if\\nignore_index\\nis specified, this loss also accepts\\nthis class index (this index may not necessarily be in the class range).\\nThe unreduced (i.e. with'),\n",
       " Document(metadata={}, page_content=\"The unreduced (i.e. with\\nreduction\\nset to\\n'none'\\n) loss can be described as:\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n,\\n…\\n,\\nl\\nN\\n}\\n⊤\\n,\\nl\\nn\\n=\\n−\\nw\\ny\\nn\\nx\\nn\\n,\\ny\\nn\\n,\\nw\\nc\\n=\\nweight\\n[\\nc\\n]\\n⋅\\n1\\n{\\nc\\n≠\\nignore_index\\n}\\n,\\n\\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - w_{y_n} x_{n,y_n}, \\\\quad\\nw_{c} = \\\\text{weight}[c] \\\\cdot \\\\mathbb{1}\\\\{c \\\\not= \\\\text{ignore\\\\_index}\\\\},\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n\\u200b\\n}\\n⊤\\n,\\nl\\nn\\n\\u200b\\n=\\n−\\nw\\ny\\nn\\n\\u200b\\n\\u200b\\nx\\nn\\n,\\ny\\nn\\n\\u200b\\n\\u200b\\n,\\nw\\nc\\n\\u200b\\n=\\nweight\\n[\\nc\\n]\\n⋅\\n1\\n{\\nc\\n\\ue020\\n=\\nignore_index\\n}\\n,\\nwhere\\nx\\nx\\nx\"),\n",
       " Document(metadata={}, page_content=\"[\\nc\\n]\\n⋅\\n1\\n{\\nc\\n\\ue020\\n=\\nignore_index\\n}\\n,\\nwhere\\nx\\nx\\nx\\nis the input,\\ny\\ny\\ny\\nis the target,\\nw\\nw\\nw\\nis the weight, and\\nN\\nN\\nN\\nis the batch size. If\\nreduction\\nis not\\n'none'\\n(default\\n'mean'\\n), then\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\n∑\\nn\\n=\\n1\\nN\\n1\\n∑\\nn\\n=\\n1\\nN\\nw\\ny\\nn\\nl\\nn\\n,\\nif\\xa0reduction\\n=\\n‘mean’;\\n∑\\nn\\n=\\n1\\nN\\nl\\nn\\n,\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\\\ell(x, y) = \\\\begin{cases}\\n    \\\\sum_{n=1}^N \\\\frac{1}{\\\\sum_{n=1}^N w_{y_n}} l_n, &\\n    \\\\text{if reduction} = \\\\text{`mean';}\\\\\\\\\\n    \\\\sum_{n=1}^N l_n,  &\\n    \\\\text{if reduction} = \\\\text{`sum'.}\\n\\\\end{cases}\\nℓ\\n(\"),\n",
       " Document(metadata={}, page_content='\\\\end{cases}\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\n∑\\nn\\n=\\n1\\nN\\n\\u200b\\n∑\\nn\\n=\\n1\\nN\\n\\u200b\\nw\\ny\\nn\\n\\u200b\\n\\u200b\\n1\\n\\u200b\\nl\\nn\\n\\u200b\\n,\\n∑\\nn\\n=\\n1\\nN\\n\\u200b\\nl\\nn\\n\\u200b\\n,\\n\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\u200b\\nParameters\\nweight\\n(\\nTensor\\n,\\noptional\\n) – a manual rescaling weight given to each\\nclass. If given, it has to be a Tensor of size\\nC\\n. Otherwise, it is\\ntreated as if having all ones.\\nsize_average\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default,\\nthe losses are averaged over each loss element in the batch. Note that for'),\n",
       " Document(metadata={}, page_content='some losses, there are multiple elements per sample. If the field\\nsize_average\\nis set to\\nFalse\\n, the losses are instead summed for each minibatch. Ignored\\nwhen\\nreduce\\nis\\nFalse\\n. Default:\\nNone\\nignore_index\\n(\\nint\\n,\\noptional\\n) – Specifies a target value that is ignored\\nand does not contribute to the input gradient. When\\nsize_average\\nis\\nTrue\\n, the loss is averaged over\\nnon-ignored targets.\\nreduce\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default, the'),\n",
       " Document(metadata={}, page_content=\") – Deprecated (see\\nreduction\\n). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non\\nsize_average\\n. When\\nreduce\\nis\\nFalse\\n, returns a loss per\\nbatch element instead and ignores\\nsize_average\\n. Default:\\nNone\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction will\\nbe applied,\\n'mean'\\n: the weighted mean of the output is taken,\\n'sum'\\n: the output will be summed. Note:\\nsize_average\"),\n",
       " Document(metadata={}, page_content=\": the output will be summed. Note:\\nsize_average\\nand\\nreduce\\nare in the process of being deprecated, and in\\nthe meantime, specifying either of those two args will override\\nreduction\\n. Default:\\n'mean'\\nShape::\\nInput:\\n(\\nN\\n,\\nC\\n)\\n(N, C)\\n(\\nN\\n,\\nC\\n)\\nor\\n(\\nC\\n)\\n(C)\\n(\\nC\\n)\\n, where\\nC = number of classes\\n,\\nN = batch size\\n, or\\n(\\nN\\n,\\nC\\n,\\nd\\n1\\n,\\nd\\n2\\n,\\n.\\n.\\n.\\n,\\nd\\nK\\n)\\n(N, C, d_1, d_2, ..., d_K)\\n(\\nN\\n,\\nC\\n,\\nd\\n1\\n\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\\n...\\n,\\nd\\nK\\n\\u200b\\n)\\nwith\\nK\\n≥\\n1\\nK \\\\geq 1\\nK\\n≥\\n1\\nin the case of\\nK\\n-dimensional loss.\\nTarget:\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\"),\n",
       " Document(metadata={}, page_content=\"K\\n-dimensional loss.\\nTarget:\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\n)\\n()\\n(\\n)\\n, where each value is\\n0\\n≤\\ntargets\\n[\\ni\\n]\\n≤\\nC\\n−\\n1\\n0 \\\\leq \\\\text{targets}[i] \\\\leq C-1\\n0\\n≤\\ntargets\\n[\\ni\\n]\\n≤\\nC\\n−\\n1\\n, or\\n(\\nN\\n,\\nd\\n1\\n,\\nd\\n2\\n,\\n.\\n.\\n.\\n,\\nd\\nK\\n)\\n(N, d_1, d_2, ..., d_K)\\n(\\nN\\n,\\nd\\n1\\n\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\\n...\\n,\\nd\\nK\\n\\u200b\\n)\\nwith\\nK\\n≥\\n1\\nK \\\\geq 1\\nK\\n≥\\n1\\nin the case of\\nK-dimensional loss.\\nOutput: If\\nreduction\\nis\\n'none'\\n, shape\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\nN\\n,\\nd\\n1\\n,\\nd\\n2\\n,\\n.\\n.\\n.\\n,\\nd\\nK\\n)\\n(N, d_1, d_2, ..., d_K)\\n(\\nN\\n,\\nd\\n1\\n\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\\n...\\n,\\nd\\nK\\n\\u200b\\n)\\nwith\\nK\\n≥\\n1\\nK \\\\geq 1\\nK\\n≥\"),\n",
       " Document(metadata={}, page_content='\\u200b\\n,\\nd\\n2\\n\\u200b\\n,\\n...\\n,\\nd\\nK\\n\\u200b\\n)\\nwith\\nK\\n≥\\n1\\nK \\\\geq 1\\nK\\n≥\\n1\\nin the case of K-dimensional loss.\\nOtherwise, scalar.\\nExamples:\\n>>>\\nlog_softmax\\n=\\nnn\\n.\\nLogSoftmax\\n(\\ndim\\n=\\n1\\n)\\n>>>\\nloss_fn\\n=\\nnn\\n.\\nNLLLoss\\n()\\n>>>\\n# input to NLLLoss is of size N x C = 3 x 5\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\n# each element in target must have 0 <= value < C\\n>>>\\ntarget\\n=\\ntorch\\n.\\ntensor\\n([\\n1\\n,\\n0\\n,\\n4\\n])\\n>>>\\nloss\\n=\\nloss_fn\\n(\\nlog_softmax\\n(\\ninput\\n),\\ntarget\\n)\\n>>>\\nloss\\n.\\nbackward\\n()\\n>>>\\n>>>\\n>>>'),\n",
       " Document(metadata={}, page_content='),\\ntarget\\n)\\n>>>\\nloss\\n.\\nbackward\\n()\\n>>>\\n>>>\\n>>>\\n# 2D loss example (used, for example, with image inputs)\\n>>>\\nN\\n,\\nC\\n=\\n5\\n,\\n4\\n>>>\\nloss_fn\\n=\\nnn\\n.\\nNLLLoss\\n()\\n>>>\\ndata\\n=\\ntorch\\n.\\nrandn\\n(\\nN\\n,\\n16\\n,\\n10\\n,\\n10\\n)\\n>>>\\nconv\\n=\\nnn\\n.\\nConv2d\\n(\\n16\\n,\\nC\\n,\\n(\\n3\\n,\\n3\\n))\\n>>>\\nlog_softmax\\n=\\nnn\\n.\\nLogSoftmax\\n(\\ndim\\n=\\n1\\n)\\n>>>\\n# output of conv forward is of shape [N, C, 8, 8]\\n>>>\\noutput\\n=\\nlog_softmax\\n(\\nconv\\n(\\ndata\\n))\\n>>>\\n# each element in target must have 0 <= value < C\\n>>>\\ntarget\\n=\\ntorch\\n.\\nempty\\n(\\nN\\n,\\n8\\n,\\n8\\n,\\ndtype\\n=\\ntorch\\n.'),\n",
       " Document(metadata={}, page_content='=\\ntorch\\n.\\nempty\\n(\\nN\\n,\\n8\\n,\\n8\\n,\\ndtype\\n=\\ntorch\\n.\\nlong\\n)\\n.\\nrandom_\\n(\\n0\\n,\\nC\\n)\\n>>>\\n# input to NLLLoss is of size N x C x height (8) x width (8)\\n>>>\\nloss\\n=\\nloss_fn\\n(\\noutput\\n,\\ntarget\\n)\\n>>>\\nloss\\n.\\nbackward\\n()\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"PoissonNLLLoss\\n¶\\nclass\\ntorch.nn.\\nPoissonNLLLoss\\n(\\nlog_input\\n=\\nTrue\\n,\\nfull\\n=\\nFalse\\n,\\nsize_average\\n=\\nNone\\n,\\neps\\n=\\n1e-08\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n)\\n[source]\\n[source]\\n¶\\nNegative log likelihood loss with Poisson distribution of target.\\nThe loss can be described as:\\ntarget\\n∼\\nP\\no\\ni\\ns\\ns\\no\\nn\\n(\\ninput\\n)\\nloss\\n(\\ninput\\n,\\ntarget\\n)\\n=\\ninput\\n−\\ntarget\\n∗\\nlog\\n\\u2061\\n(\\ninput\\n)\\n+\\nlog\\n\\u2061\\n(\\ntarget!\\n)\\n\\\\text{target} \\\\sim \\\\mathrm{Poisson}(\\\\text{input})\"),\n",
       " Document(metadata={}, page_content='\\\\text{loss}(\\\\text{input}, \\\\text{target}) = \\\\text{input} - \\\\text{target} * \\\\log(\\\\text{input})\\n                            + \\\\log(\\\\text{target!})\\ntarget\\n∼\\nPoisson\\n(\\ninput\\n)\\nloss\\n(\\ninput\\n,\\ntarget\\n)\\n=\\ninput\\n−\\ntarget\\n∗\\nlo\\ng\\n(\\ninput\\n)\\n+\\nlo\\ng\\n(\\ntarget!\\n)\\nThe last term can be omitted or approximated with Stirling formula. The\\napproximation is used for target values more than 1. For targets less or\\nequal to 1 zeros are added to the loss.\\nParameters\\nlog_input\\n(\\nbool\\n,\\noptional\\n) – if\\nTrue'),\n",
       " Document(metadata={}, page_content='log_input\\n(\\nbool\\n,\\noptional\\n) – if\\nTrue\\nthe loss is computed as\\nexp\\n\\u2061\\n(\\ninput\\n)\\n−\\ntarget\\n∗\\ninput\\n\\\\exp(\\\\text{input}) - \\\\text{target}*\\\\text{input}\\nexp\\n(\\ninput\\n)\\n−\\ntarget\\n∗\\ninput\\n, if\\nFalse\\nthe loss is\\ninput\\n−\\ntarget\\n∗\\nlog\\n\\u2061\\n(\\ninput\\n+\\neps\\n)\\n\\\\text{input} - \\\\text{target}*\\\\log(\\\\text{input}+\\\\text{eps})\\ninput\\n−\\ntarget\\n∗\\nlo\\ng\\n(\\ninput\\n+\\neps\\n)\\n.\\nfull\\n(\\nbool\\n,\\noptional\\n) –\\nwhether to compute full loss, i. e. to add the\\nStirling approximation term\\ntarget\\n∗\\nlog\\n\\u2061\\n(\\ntarget\\n)\\n−\\ntarget\\n+\\n0.5\\n∗\\nlog\\n\\u2061\\n(\\n2\\nπ'),\n",
       " Document(metadata={}, page_content='∗\\nlog\\n\\u2061\\n(\\ntarget\\n)\\n−\\ntarget\\n+\\n0.5\\n∗\\nlog\\n\\u2061\\n(\\n2\\nπ\\ntarget\\n)\\n.\\n\\\\text{target}*\\\\log(\\\\text{target}) - \\\\text{target} + 0.5 * \\\\log(2\\\\pi\\\\text{target}).\\ntarget\\n∗\\nlo\\ng\\n(\\ntarget\\n)\\n−\\ntarget\\n+\\n0.5\\n∗\\nlo\\ng\\n(\\n2\\nπ\\ntarget\\n)\\n.\\nsize_average\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field\\nsize_average\\nis set to\\nFalse'),\n",
       " Document(metadata={}, page_content='size_average\\nis set to\\nFalse\\n, the losses are instead summed for each minibatch. Ignored\\nwhen\\nreduce\\nis\\nFalse\\n. Default:\\nTrue\\neps\\n(\\nfloat\\n,\\noptional\\n) – Small value to avoid evaluation of\\nlog\\n\\u2061\\n(\\n0\\n)\\n\\\\log(0)\\nlo\\ng\\n(\\n0\\n)\\nwhen\\nlog_input\\n=\\nFalse\\n. Default: 1e-8\\nreduce\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non\\nsize_average\\n. When\\nreduce\\nis\\nFalse\\n, returns a loss per'),\n",
       " Document(metadata={}, page_content=\". When\\nreduce\\nis\\nFalse\\n, returns a loss per\\nbatch element instead and ignores\\nsize_average\\n. Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction will be applied,\\n'mean'\\n: the sum of the output will be divided by the number of\\nelements in the output,\\n'sum'\\n: the output will be summed. Note:\\nsize_average\\nand\\nreduce\\nare in the process of being deprecated, and in the meantime,\"),\n",
       " Document(metadata={}, page_content=\"specifying either of those two args will override\\nreduction\\n. Default:\\n'mean'\\nExamples:\\n>>>\\nloss\\n=\\nnn\\n.\\nPoissonNLLLoss\\n()\\n>>>\\nlog_input\\n=\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n2\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n2\\n)\\n>>>\\noutput\\n=\\nloss\\n(\\nlog_input\\n,\\ntarget\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nTarget:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nOutput: scalar by default. If\\nreduction\\nis\\n'none'\\n, then\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n,\"),\n",
       " Document(metadata={}, page_content=\"reduction\\nis\\n'none'\\n, then\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n,\\nthe same shape as the input.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.\"),\n",
       " Document(metadata={}, page_content=\"GaussianNLLLoss\\n¶\\nclass\\ntorch.nn.\\nGaussianNLLLoss\\n(\\n*\\n,\\nfull\\n=\\nFalse\\n,\\neps\\n=\\n1e-06\\n,\\nreduction\\n=\\n'mean'\\n)\\n[source]\\n[source]\\n¶\\nGaussian negative log likelihood loss.\\nThe targets are treated as samples from Gaussian distributions with\\nexpectations and variances predicted by the neural network. For a\\ntarget\\ntensor modelled as having Gaussian distribution with a tensor\\nof expectations\\ninput\\nand a tensor of positive variances\\nvar\\nthe loss is:\\nloss\\n=\\n1\\n2\\n(\\nlog\\n\\u2061\\n(\\nmax\\n(\\nvar\\n,\\neps\\n)\\n)\\n+\\n(\\ninput\\n−\"),\n",
       " Document(metadata={}, page_content='=\\n1\\n2\\n(\\nlog\\n\\u2061\\n(\\nmax\\n(\\nvar\\n,\\neps\\n)\\n)\\n+\\n(\\ninput\\n−\\ntarget\\n)\\n2\\nmax\\n(\\nvar\\n,\\neps\\n)\\n)\\n+\\nconst.\\n\\\\text{loss} = \\\\frac{1}{2}\\\\left(\\\\log\\\\left(\\\\text{max}\\\\left(\\\\text{var},\\n\\\\ \\\\text{eps}\\\\right)\\\\right) + \\\\frac{\\\\left(\\\\text{input} - \\\\text{target}\\\\right)^2}\\n{\\\\text{max}\\\\left(\\\\text{var}, \\\\ \\\\text{eps}\\\\right)}\\\\right) + \\\\text{const.}\\nloss\\n=\\n2\\n1\\n\\u200b\\n(\\nlo\\ng\\n(\\nmax\\n(\\nvar\\n,\\neps\\n)\\n)\\n+\\nmax\\n(\\nvar\\n,\\neps\\n)\\n(\\ninput\\n−\\ntarget\\n)\\n2\\n\\u200b\\n)\\n+\\nconst.\\nwhere\\neps\\nis used for stability. By default, the constant term of'),\n",
       " Document(metadata={}, page_content='the loss function is omitted unless\\nfull\\nis\\nTrue\\n. If\\nvar\\nis not the same\\nsize as\\ninput\\n(due to a homoscedastic assumption), it must either have a final dimension\\nof 1 or have one fewer dimension (with all other sizes being the same) for correct broadcasting.\\nParameters\\nfull\\n(\\nbool\\n,\\noptional\\n) – include the constant term in the loss\\ncalculation. Default:\\nFalse\\n.\\neps\\n(\\nfloat\\n,\\noptional\\n) – value used to clamp\\nvar\\n(see note below), for\\nstability. Default: 1e-6.\\nreduction\\n(\\nstr\\n,\\noptional'),\n",
       " Document(metadata={}, page_content=\"reduction\\n(\\nstr\\n,\\noptional\\n) – specifies the reduction to apply to the\\noutput:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction\\nwill be applied,\\n'mean'\\n: the output is the average of all batch\\nmember losses,\\n'sum'\\n: the output is the sum of all batch member\\nlosses. Default:\\n'mean'\\n.\\nShape:\\nInput:\\n(\\nN\\n,\\n∗\\n)\\n(N, *)\\n(\\nN\\n,\\n∗\\n)\\nor\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\nwhere\\n∗\\n*\\n∗\\nmeans any number of additional\\ndimensions\\nTarget:\\n(\\nN\\n,\\n∗\\n)\\n(N, *)\\n(\\nN\\n,\\n∗\\n)\\nor\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\"),\n",
       " Document(metadata={}, page_content=\"(\\nN\\n,\\n∗\\n)\\n(N, *)\\n(\\nN\\n,\\n∗\\n)\\nor\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input, or same shape as the input\\nbut with one dimension equal to 1 (to allow for broadcasting)\\nVar:\\n(\\nN\\n,\\n∗\\n)\\n(N, *)\\n(\\nN\\n,\\n∗\\n)\\nor\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input, or same shape as the input but\\nwith one dimension equal to 1, or same shape as the input but with one fewer\\ndimension (to allow for broadcasting), or a scalar value\\nOutput: scalar if\\nreduction\\nis\\n'mean'\\n(default) or\\n'sum'\\n. If\\nreduction\\nis\\n'none'\\n, then\\n(\\nN\\n,\"),\n",
       " Document(metadata={}, page_content=\"'sum'\\n. If\\nreduction\\nis\\n'none'\\n, then\\n(\\nN\\n,\\n∗\\n)\\n(N, *)\\n(\\nN\\n,\\n∗\\n)\\n, same\\nshape as the input\\nExamples::\\n>>>\\nloss\\n=\\nnn\\n.\\nGaussianNLLLoss\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n2\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n2\\n)\\n>>>\\nvar\\n=\\ntorch\\n.\\nones\\n(\\n5\\n,\\n2\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n# heteroscedastic\\n>>>\\noutput\\n=\\nloss\\n(\\ninput\\n,\\ntarget\\n,\\nvar\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\n>>>\\nloss\\n=\\nnn\\n.\\nGaussianNLLLoss\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n2\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandn\\n(\\n5\"),\n",
       " Document(metadata={}, page_content='=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandn\\n(\\n5\\n,\\n2\\n)\\n>>>\\nvar\\n=\\ntorch\\n.\\nones\\n(\\n5\\n,\\n1\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n# homoscedastic\\n>>>\\noutput\\n=\\nloss\\n(\\ninput\\n,\\ntarget\\n,\\nvar\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\nNote\\nThe clamping of\\nvar\\nis ignored with respect to autograd, and so the\\ngradients are unaffected by it.\\nReference:\\nNix, D. A. and Weigend, A. S., “Estimating the mean and variance of the\\ntarget probability distribution”, Proceedings of 1994 IEEE International'),\n",
       " Document(metadata={}, page_content='Conference on Neural Networks (ICNN’94), Orlando, FL, USA, 1994, pp. 55-60\\nvol.1, doi: 10.1109/ICNN.1994.374138.\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"KLDivLoss\\n¶\\nclass\\ntorch.nn.\\nKLDivLoss\\n(\\nsize_average\\n=\\nNone\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n,\\nlog_target\\n=\\nFalse\\n)\\n[source]\\n[source]\\n¶\\nThe Kullback-Leibler divergence loss.\\nFor tensors of the same shape\\ny\\npred\\n,\\ny\\ntrue\\ny_{\\\\text{pred}},\\\\ y_{\\\\text{true}}\\ny\\npred\\n\\u200b\\n,\\ny\\ntrue\\n\\u200b\\n,\\nwhere\\ny\\npred\\ny_{\\\\text{pred}}\\ny\\npred\\n\\u200b\\nis the\\ninput\\nand\\ny\\ntrue\\ny_{\\\\text{true}}\\ny\\ntrue\\n\\u200b\\nis the\\ntarget\\n, we define the\\npointwise KL-divergence\\nas\\nL\\n(\\ny\\npred\\n,\\ny\\ntrue\\n)\\n=\\ny\\ntrue\\n⋅\\nlog\\n\\u2061\\ny\\ntrue\\ny\\npred\\n=\\ny\\ntrue\\n⋅\\n(\\nlog\\n\\u2061\\ny\"),\n",
       " Document(metadata={}, page_content='y\\ntrue\\n⋅\\nlog\\n\\u2061\\ny\\ntrue\\ny\\npred\\n=\\ny\\ntrue\\n⋅\\n(\\nlog\\n\\u2061\\ny\\ntrue\\n−\\nlog\\n\\u2061\\ny\\npred\\n)\\nL(y_{\\\\text{pred}},\\\\ y_{\\\\text{true}})\\n    = y_{\\\\text{true}} \\\\cdot \\\\log \\\\frac{y_{\\\\text{true}}}{y_{\\\\text{pred}}}\\n    = y_{\\\\text{true}} \\\\cdot (\\\\log y_{\\\\text{true}} - \\\\log y_{\\\\text{pred}})\\nL\\n(\\ny\\npred\\n\\u200b\\n,\\ny\\ntrue\\n\\u200b\\n)\\n=\\ny\\ntrue\\n\\u200b\\n⋅\\nlo\\ng\\ny\\npred\\n\\u200b\\ny\\ntrue\\n\\u200b\\n\\u200b\\n=\\ny\\ntrue\\n\\u200b\\n⋅\\n(\\nlo\\ng\\ny\\ntrue\\n\\u200b\\n−\\nlo\\ng\\ny\\npred\\n\\u200b\\n)\\nTo avoid underflow issues when computing this quantity, this loss expects the argument\\ninput\\nin the log-space. The argument\\ntarget'),\n",
       " Document(metadata={}, page_content='input\\nin the log-space. The argument\\ntarget\\nmay also be provided in the\\nlog-space if\\nlog_target\\n= True\\n.\\nTo summarise, this function is roughly equivalent to computing\\nif\\nnot\\nlog_target\\n:\\n# default\\nloss_pointwise\\n=\\ntarget\\n*\\n(\\ntarget\\n.\\nlog\\n()\\n-\\ninput\\n)\\nelse\\n:\\nloss_pointwise\\n=\\ntarget\\n.\\nexp\\n()\\n*\\n(\\ntarget\\n-\\ninput\\n)\\nand then reducing this result depending on the argument\\nreduction\\nas\\nif\\nreduction\\n==\\n\"mean\"\\n:\\n# default\\nloss\\n=\\nloss_pointwise\\n.\\nmean\\n()\\nelif\\nreduction\\n==\\n\"batchmean\"\\n:'),\n",
       " Document(metadata={}, page_content='.\\nmean\\n()\\nelif\\nreduction\\n==\\n\"batchmean\"\\n:\\n# mathematically correct\\nloss\\n=\\nloss_pointwise\\n.\\nsum\\n()\\n/\\ninput\\n.\\nsize\\n(\\n0\\n)\\nelif\\nreduction\\n==\\n\"sum\"\\n:\\nloss\\n=\\nloss_pointwise\\n.\\nsum\\n()\\nelse\\n:\\n# reduction == \"none\"\\nloss\\n=\\nloss_pointwise\\nNote\\nAs all the other losses in PyTorch, this function expects the first argument,\\ninput\\n, to be the output of the model (e.g. the neural network)\\nand the second,\\ntarget\\n, to be the observations in the dataset.\\nThis differs from the standard mathematical notation\\nK\\nL\\n(\\nP'),\n",
       " Document(metadata={}, page_content='K\\nL\\n(\\nP\\n∣\\n∣\\nQ\\n)\\nKL(P\\\\ ||\\\\ Q)\\nK\\nL\\n(\\nP\\n∣∣\\nQ\\n)\\nwhere\\nP\\nP\\nP\\ndenotes the distribution of the observations and\\nQ\\nQ\\nQ\\ndenotes the model.\\nWarning\\nreduction\\n= “mean”\\ndoesn’t return the true KL divergence value, please use\\nreduction\\n= “batchmean”\\nwhich aligns with the mathematical definition.\\nParameters\\nsize_average\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default,\\nthe losses are averaged over each loss element in the batch. Note that for'),\n",
       " Document(metadata={}, page_content='some losses, there are multiple elements per sample. If the field\\nsize_average\\nis set to\\nFalse\\n, the losses are instead summed for each minibatch. Ignored\\nwhen\\nreduce\\nis\\nFalse\\n. Default:\\nTrue\\nreduce\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non\\nsize_average\\n. When\\nreduce\\nis\\nFalse\\n, returns a loss per\\nbatch element instead and ignores\\nsize_average\\n. Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional'),\n",
       " Document(metadata={}, page_content='. Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output. Default:\\n“mean”\\nlog_target\\n(\\nbool\\n,\\noptional\\n) – Specifies whether\\ntarget\\nis the log space. Default:\\nFalse\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nTarget:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nOutput: scalar by default. If\\nreduction\\nis\\n‘none’\\n, then\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n,\\nsame shape as the input.\\nExamples::\\n>>>\\nkl_loss\\n=\\nnn\\n.\\nKLDivLoss\\n(\\nreduction\\n=\\n\"batchmean\"\\n)'),\n",
       " Document(metadata={}, page_content='=\\nnn\\n.\\nKLDivLoss\\n(\\nreduction\\n=\\n\"batchmean\"\\n)\\n>>>\\n# input should be a distribution in the log space\\n>>>\\ninput\\n=\\nF\\n.\\nlog_softmax\\n(\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n5\\n,\\nrequires_grad\\n=\\nTrue\\n),\\ndim\\n=\\n1\\n)\\n>>>\\n# Sample a batch of distributions. Usually this would come from the dataset\\n>>>\\ntarget\\n=\\nF\\n.\\nsoftmax\\n(\\ntorch\\n.\\nrand\\n(\\n3\\n,\\n5\\n),\\ndim\\n=\\n1\\n)\\n>>>\\noutput\\n=\\nkl_loss\\n(\\ninput\\n,\\ntarget\\n)\\n>>>\\nkl_loss\\n=\\nnn\\n.\\nKLDivLoss\\n(\\nreduction\\n=\\n\"batchmean\"\\n,\\nlog_target\\n=\\nTrue\\n)\\n>>>\\nlog_target\\n=\\nF\\n.\\nlog_softmax\\n(\\ntorch\\n.\\nrand\\n(\\n3\\n,'),\n",
       " Document(metadata={}, page_content='log_target\\n=\\nF\\n.\\nlog_softmax\\n(\\ntorch\\n.\\nrand\\n(\\n3\\n,\\n5\\n),\\ndim\\n=\\n1\\n)\\n>>>\\noutput\\n=\\nkl_loss\\n(\\ninput\\n,\\nlog_target\\n)\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"BCELoss\\n¶\\nclass\\ntorch.nn.\\nBCELoss\\n(\\nweight\\n=\\nNone\\n,\\nsize_average\\n=\\nNone\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n)\\n[source]\\n[source]\\n¶\\nCreates a criterion that measures the Binary Cross Entropy between the target and\\nthe input probabilities:\\nThe unreduced (i.e. with\\nreduction\\nset to\\n'none'\\n) loss can be described as:\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n,\\n…\\n,\\nl\\nN\\n}\\n⊤\\n,\\nl\\nn\\n=\\n−\\nw\\nn\\n[\\ny\\nn\\n⋅\\nlog\\n\\u2061\\nx\\nn\\n+\\n(\\n1\\n−\\ny\\nn\\n)\\n⋅\\nlog\\n\\u2061\\n(\\n1\\n−\\nx\\nn\\n)\\n]\\n,\\n\\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\"),\n",
       " Document(metadata={}, page_content=\",\\n\\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - w_n \\\\left[ y_n \\\\cdot \\\\log x_n + (1 - y_n) \\\\cdot \\\\log (1 - x_n) \\\\right],\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n\\u200b\\n}\\n⊤\\n,\\nl\\nn\\n\\u200b\\n=\\n−\\nw\\nn\\n\\u200b\\n[\\ny\\nn\\n\\u200b\\n⋅\\nlo\\ng\\nx\\nn\\n\\u200b\\n+\\n(\\n1\\n−\\ny\\nn\\n\\u200b\\n)\\n⋅\\nlo\\ng\\n(\\n1\\n−\\nx\\nn\\n\\u200b\\n)\\n]\\n,\\nwhere\\nN\\nN\\nN\\nis the batch size. If\\nreduction\\nis not\\n'none'\\n(default\\n'mean'\\n), then\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nmean\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘mean’;\\nsum\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\\\ell(x, y) = \\\\begin{cases}\"),\n",
       " Document(metadata={}, page_content=\"if\\xa0reduction\\n=\\n‘sum’.\\n\\\\ell(x, y) = \\\\begin{cases}\\n    \\\\operatorname{mean}(L), & \\\\text{if reduction} = \\\\text{`mean';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  & \\\\text{if reduction} = \\\\text{`sum'.}\\n\\\\end{cases}\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nmean\\n(\\nL\\n)\\n,\\nsum\\n(\\nL\\n)\\n,\\n\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\u200b\\nThis is used for measuring the error of a reconstruction in for example\\nan auto-encoder. Note that the targets\\ny\\ny\\ny\\nshould be numbers\\nbetween 0 and 1.\\nNotice that if\\nx\\nn\\nx_n\\nx\\nn\\n\\u200b\"),\n",
       " Document(metadata={}, page_content='between 0 and 1.\\nNotice that if\\nx\\nn\\nx_n\\nx\\nn\\n\\u200b\\nis either 0 or 1, one of the log terms would be\\nmathematically undefined in the above loss equation. PyTorch chooses to set\\nlog\\n\\u2061\\n(\\n0\\n)\\n=\\n−\\n∞\\n\\\\log (0) = -\\\\infty\\nlo\\ng\\n(\\n0\\n)\\n=\\n−\\n∞\\n, since\\nlim\\n\\u2061\\nx\\n→\\n0\\nlog\\n\\u2061\\n(\\nx\\n)\\n=\\n−\\n∞\\n\\\\lim_{x\\\\to 0} \\\\log (x) = -\\\\infty\\nlim\\nx\\n→\\n0\\n\\u200b\\nlo\\ng\\n(\\nx\\n)\\n=\\n−\\n∞\\n.\\nHowever, an infinite term in the loss equation is not desirable for several reasons.\\nFor one, if either\\ny\\nn\\n=\\n0\\ny_n = 0\\ny\\nn\\n\\u200b\\n=\\n0\\nor\\n(\\n1\\n−\\ny\\nn\\n)\\n=\\n0\\n(1 - y_n) = 0\\n(\\n1\\n−\\ny\\nn'),\n",
       " Document(metadata={}, page_content='\\u200b\\n=\\n0\\nor\\n(\\n1\\n−\\ny\\nn\\n)\\n=\\n0\\n(1 - y_n) = 0\\n(\\n1\\n−\\ny\\nn\\n\\u200b\\n)\\n=\\n0\\n, then we would be\\nmultiplying 0 with infinity. Secondly, if we have an infinite loss value, then\\nwe would also have an infinite term in our gradient, since\\nlim\\n\\u2061\\nx\\n→\\n0\\nd\\nd\\nx\\nlog\\n\\u2061\\n(\\nx\\n)\\n=\\n∞\\n\\\\lim_{x\\\\to 0} \\\\frac{d}{dx} \\\\log (x) = \\\\infty\\nlim\\nx\\n→\\n0\\n\\u200b\\nd\\nx\\nd\\n\\u200b\\nlo\\ng\\n(\\nx\\n)\\n=\\n∞\\n.\\nThis would make BCELoss’s backward method nonlinear with respect to\\nx\\nn\\nx_n\\nx\\nn\\n\\u200b\\n,\\nand using it for things like linear regression would not be straight-forward.'),\n",
       " Document(metadata={}, page_content='Our solution is that BCELoss clamps its log function outputs to be greater than\\nor equal to -100. This way, we can always have a finite loss value and a linear\\nbackward method.\\nParameters\\nweight\\n(\\nTensor\\n,\\noptional\\n) – a manual rescaling weight given to the loss\\nof each batch element. If given, has to be a Tensor of size\\nnbatch\\n.\\nsize_average\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default,\\nthe losses are averaged over each loss element in the batch. Note that for'),\n",
       " Document(metadata={}, page_content='some losses, there are multiple elements per sample. If the field\\nsize_average\\nis set to\\nFalse\\n, the losses are instead summed for each minibatch. Ignored\\nwhen\\nreduce\\nis\\nFalse\\n. Default:\\nTrue\\nreduce\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non\\nsize_average\\n. When\\nreduce\\nis\\nFalse\\n, returns a loss per\\nbatch element instead and ignores\\nsize_average\\n. Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional'),\n",
       " Document(metadata={}, page_content=\". Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction will be applied,\\n'mean'\\n: the sum of the output will be divided by the number of\\nelements in the output,\\n'sum'\\n: the output will be summed. Note:\\nsize_average\\nand\\nreduce\\nare in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override\\nreduction\\n. Default:\\n'mean'\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\"),\n",
       " Document(metadata={}, page_content=\"Shape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nTarget:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nOutput: scalar. If\\nreduction\\nis\\n'none'\\n, then\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same\\nshape as input.\\nExamples:\\n>>>\\nm\\n=\\nnn\\n.\\nSigmoid\\n()\\n>>>\\nloss\\n=\\nnn\\n.\\nBCELoss\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\n2\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrand\\n(\\n3\\n,\\n2\\n,\\nrequires_grad\\n=\\nFalse\\n)\\n>>>\\noutput\\n=\\nloss\\n(\\nm\\n(\\ninput\\n),\\ntarget\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\nNext\\nPrevious\"),\n",
       " Document(metadata={}, page_content='target\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"BCEWithLogitsLoss\\n¶\\nclass\\ntorch.nn.\\nBCEWithLogitsLoss\\n(\\nweight\\n=\\nNone\\n,\\nsize_average\\n=\\nNone\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n,\\npos_weight\\n=\\nNone\\n)\\n[source]\\n[source]\\n¶\\nThis loss combines a\\nSigmoid\\nlayer and the\\nBCELoss\\nin one single\\nclass. This version is more numerically stable than using a plain\\nSigmoid\\nfollowed by a\\nBCELoss\\nas, by combining the operations into one layer,\\nwe take advantage of the log-sum-exp trick for numerical stability.\\nThe unreduced (i.e. with\\nreduction\\nset to\\n'none'\"),\n",
       " Document(metadata={}, page_content=\"The unreduced (i.e. with\\nreduction\\nset to\\n'none'\\n) loss can be described as:\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n,\\n…\\n,\\nl\\nN\\n}\\n⊤\\n,\\nl\\nn\\n=\\n−\\nw\\nn\\n[\\ny\\nn\\n⋅\\nlog\\n\\u2061\\nσ\\n(\\nx\\nn\\n)\\n+\\n(\\n1\\n−\\ny\\nn\\n)\\n⋅\\nlog\\n\\u2061\\n(\\n1\\n−\\nσ\\n(\\nx\\nn\\n)\\n)\\n]\\n,\\n\\\\ell(x, y) = L = \\\\{l_1,\\\\dots,l_N\\\\}^\\\\top, \\\\quad\\nl_n = - w_n \\\\left[ y_n \\\\cdot \\\\log \\\\sigma(x_n)\\n+ (1 - y_n) \\\\cdot \\\\log (1 - \\\\sigma(x_n)) \\\\right],\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\n=\\n{\\nl\\n1\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n\\u200b\\n}\\n⊤\\n,\\nl\\nn\\n\\u200b\\n=\\n−\\nw\\nn\\n\\u200b\\n[\\ny\\nn\\n\\u200b\\n⋅\\nlo\\ng\\nσ\\n(\\nx\\nn\\n\\u200b\\n)\\n+\\n(\\n1\\n−\\ny\\nn\\n\\u200b\\n)\\n⋅\\nlo\\ng\\n(\\n1\\n−\\nσ\\n(\\nx\\nn\\n\\u200b\\n))\\n]\\n,\\nwhere\\nN\\nN\\nN\"),\n",
       " Document(metadata={}, page_content=\"y\\nn\\n\\u200b\\n)\\n⋅\\nlo\\ng\\n(\\n1\\n−\\nσ\\n(\\nx\\nn\\n\\u200b\\n))\\n]\\n,\\nwhere\\nN\\nN\\nN\\nis the batch size. If\\nreduction\\nis not\\n'none'\\n(default\\n'mean'\\n), then\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nmean\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘mean’;\\nsum\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\\\ell(x, y) = \\\\begin{cases}\\n    \\\\operatorname{mean}(L), & \\\\text{if reduction} = \\\\text{`mean';}\\\\\\\\\\n    \\\\operatorname{sum}(L),  & \\\\text{if reduction} = \\\\text{`sum'.}\\n\\\\end{cases}\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nmean\\n(\\nL\\n)\\n,\\nsum\\n(\\nL\\n)\\n,\\n\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\u200b\"),\n",
       " Document(metadata={}, page_content='\\u200b\\nif\\xa0reduction\\n=\\n‘mean’;\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\u200b\\nThis is used for measuring the error of a reconstruction in for example\\nan auto-encoder. Note that the targets\\nt[i]\\nshould be numbers\\nbetween 0 and 1.\\nIt’s possible to trade off recall and precision by adding weights to positive examples.\\nIn the case of multi-label classification the loss can be described as:\\nℓ\\nc\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\nc\\n=\\n{\\nl\\n1\\n,\\nc\\n,\\n…\\n,\\nl\\nN\\n,\\nc\\n}\\n⊤\\n,\\nl\\nn\\n,\\nc\\n=\\n−\\nw\\nn\\n,\\nc\\n[\\np\\nc\\ny\\nn\\n,\\nc\\n⋅\\nlog\\n\\u2061\\nσ\\n(\\nx\\nn\\n,\\nc\\n)\\n+\\n(\\n1\\n−\\ny\\nn\\n,\\nc\\n)\\n⋅\\nlog\\n\\u2061\\n(\\n1\\n−'),\n",
       " Document(metadata={}, page_content='\\u2061\\nσ\\n(\\nx\\nn\\n,\\nc\\n)\\n+\\n(\\n1\\n−\\ny\\nn\\n,\\nc\\n)\\n⋅\\nlog\\n\\u2061\\n(\\n1\\n−\\nσ\\n(\\nx\\nn\\n,\\nc\\n)\\n)\\n]\\n,\\n\\\\ell_c(x, y) = L_c = \\\\{l_{1,c},\\\\dots,l_{N,c}\\\\}^\\\\top, \\\\quad\\nl_{n,c} = - w_{n,c} \\\\left[ p_c y_{n,c} \\\\cdot \\\\log \\\\sigma(x_{n,c})\\n+ (1 - y_{n,c}) \\\\cdot \\\\log (1 - \\\\sigma(x_{n,c})) \\\\right],\\nℓ\\nc\\n\\u200b\\n(\\nx\\n,\\ny\\n)\\n=\\nL\\nc\\n\\u200b\\n=\\n{\\nl\\n1\\n,\\nc\\n\\u200b\\n,\\n…\\n,\\nl\\nN\\n,\\nc\\n\\u200b\\n}\\n⊤\\n,\\nl\\nn\\n,\\nc\\n\\u200b\\n=\\n−\\nw\\nn\\n,\\nc\\n\\u200b\\n[\\np\\nc\\n\\u200b\\ny\\nn\\n,\\nc\\n\\u200b\\n⋅\\nlo\\ng\\nσ\\n(\\nx\\nn\\n,\\nc\\n\\u200b\\n)\\n+\\n(\\n1\\n−\\ny\\nn\\n,\\nc\\n\\u200b\\n)\\n⋅\\nlo\\ng\\n(\\n1\\n−\\nσ\\n(\\nx\\nn\\n,\\nc\\n\\u200b\\n))\\n]\\n,\\nwhere\\nc\\nc\\nc\\nis the class number (\\nc\\n>\\n1\\nc > 1\\nc\\n>\\n1'),\n",
       " Document(metadata={}, page_content='c\\nc\\nc\\nis the class number (\\nc\\n>\\n1\\nc > 1\\nc\\n>\\n1\\nfor multi-label binary classification,\\nc\\n=\\n1\\nc = 1\\nc\\n=\\n1\\nfor single-label binary classification),\\nn\\nn\\nn\\nis the number of the sample in the batch and\\np\\nc\\np_c\\np\\nc\\n\\u200b\\nis the weight of the positive answer for the class\\nc\\nc\\nc\\n.\\np\\nc\\n>\\n1\\np_c > 1\\np\\nc\\n\\u200b\\n>\\n1\\nincreases the recall,\\np\\nc\\n<\\n1\\np_c < 1\\np\\nc\\n\\u200b\\n<\\n1\\nincreases the precision.\\nFor example, if a dataset contains 100 positive and 300 negative examples of a single class,\\nthen\\npos_weight'),\n",
       " Document(metadata={}, page_content='then\\npos_weight\\nfor the class should be equal to\\n300\\n100\\n=\\n3\\n\\\\frac{300}{100}=3\\n100\\n300\\n\\u200b\\n=\\n3\\n.\\nThe loss would act as if the dataset contains\\n3\\n×\\n100\\n=\\n300\\n3\\\\times 100=300\\n3\\n×\\n100\\n=\\n300\\npositive examples.\\nExamples:\\n>>>\\ntarget\\n=\\ntorch\\n.\\nones\\n([\\n10\\n,\\n64\\n],\\ndtype\\n=\\ntorch\\n.\\nfloat32\\n)\\n# 64 classes, batch size = 10\\n>>>\\noutput\\n=\\ntorch\\n.\\nfull\\n([\\n10\\n,\\n64\\n],\\n1.5\\n)\\n# A prediction (logit)\\n>>>\\npos_weight\\n=\\ntorch\\n.\\nones\\n([\\n64\\n])\\n# All weights are equal to 1\\n>>>\\ncriterion\\n=\\ntorch\\n.\\nnn\\n.\\nBCEWithLogitsLoss\\n('),\n",
       " Document(metadata={}, page_content='>>>\\ncriterion\\n=\\ntorch\\n.\\nnn\\n.\\nBCEWithLogitsLoss\\n(\\npos_weight\\n=\\npos_weight\\n)\\n>>>\\ncriterion\\n(\\noutput\\n,\\ntarget\\n)\\n# -log(sigmoid(1.5))\\ntensor(0.20...)\\nIn the above example, the\\npos_weight\\ntensor’s elements correspond to the 64 distinct classes\\nin a multi-label binary classification scenario. Each element in\\npos_weight\\nis designed to adjust the\\nloss function based on the imbalance between negative and positive samples for the respective class.'),\n",
       " Document(metadata={}, page_content='This approach is useful in datasets with varying levels of class imbalance, ensuring that the loss\\ncalculation accurately accounts for the distribution in each class.\\nParameters\\nweight\\n(\\nTensor\\n,\\noptional\\n) – a manual rescaling weight given to the loss\\nof each batch element. If given, has to be a Tensor of size\\nnbatch\\n.\\nsize_average\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default,\\nthe losses are averaged over each loss element in the batch. Note that for'),\n",
       " Document(metadata={}, page_content='some losses, there are multiple elements per sample. If the field\\nsize_average\\nis set to\\nFalse\\n, the losses are instead summed for each minibatch. Ignored\\nwhen\\nreduce\\nis\\nFalse\\n. Default:\\nTrue\\nreduce\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non\\nsize_average\\n. When\\nreduce\\nis\\nFalse\\n, returns a loss per\\nbatch element instead and ignores\\nsize_average\\n. Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional'),\n",
       " Document(metadata={}, page_content=\". Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction will be applied,\\n'mean'\\n: the sum of the output will be divided by the number of\\nelements in the output,\\n'sum'\\n: the output will be summed. Note:\\nsize_average\\nand\\nreduce\\nare in the process of being deprecated, and in the meantime,\\nspecifying either of those two args will override\\nreduction\\n. Default:\\n'mean'\\npos_weight\\n(\\nTensor\\n,\\noptional\"),\n",
       " Document(metadata={}, page_content=\". Default:\\n'mean'\\npos_weight\\n(\\nTensor\\n,\\noptional\\n) – a weight of positive examples to be broadcasted with target.\\nMust be a tensor with equal size along the class dimension to the number of classes.\\nPay close attention to PyTorch’s broadcasting semantics in order to achieve the desired\\noperations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\\nsize [B, C, H, W] will apply different pos_weights to each element of the batch or\"),\n",
       " Document(metadata={}, page_content=\"[C, H, W] the same pos_weights across the batch. To apply the same positive weight\\nalong all spacial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\\nDefault:\\nNone\\nShape:\\nInput:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, where\\n∗\\n*\\n∗\\nmeans any number of dimensions.\\nTarget:\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same shape as the input.\\nOutput: scalar. If\\nreduction\\nis\\n'none'\\n, then\\n(\\n∗\\n)\\n(*)\\n(\\n∗\\n)\\n, same\\nshape as input.\\nExamples:\\n>>>\\nloss\\n=\\nnn\\n.\\nBCEWithLogitsLoss\\n()\\n>>>\\ninput\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\"),\n",
       " Document(metadata={}, page_content='=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nempty\\n(\\n3\\n)\\n.\\nrandom_\\n(\\n2\\n)\\n>>>\\noutput\\n=\\nloss\\n(\\ninput\\n,\\ntarget\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"MarginRankingLoss\\n¶\\nclass\\ntorch.nn.\\nMarginRankingLoss\\n(\\nmargin\\n=\\n0.0\\n,\\nsize_average\\n=\\nNone\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n)\\n[source]\\n[source]\\n¶\\nCreates a criterion that measures the loss given\\ninputs\\nx\\n1\\nx1\\nx\\n1\\n,\\nx\\n2\\nx2\\nx\\n2\\n, two 1D mini-batch or 0D\\nTensors\\n,\\nand a label 1D mini-batch or 0D\\nTensor\\ny\\ny\\ny\\n(containing 1 or -1).\\nIf\\ny\\n=\\n1\\ny = 1\\ny\\n=\\n1\\nthen it assumed the first input should be ranked higher\\n(have a larger value) than the second input, and vice-versa for\\ny\\n=\\n−\\n1\\ny = -1\\ny\\n=\\n−\\n1\\n.\"),\n",
       " Document(metadata={}, page_content='y\\n=\\n−\\n1\\ny = -1\\ny\\n=\\n−\\n1\\n.\\nThe loss function for each pair of samples in the mini-batch is:\\nloss\\n(\\nx\\n1\\n,\\nx\\n2\\n,\\ny\\n)\\n=\\nmax\\n\\u2061\\n(\\n0\\n,\\n−\\ny\\n∗\\n(\\nx\\n1\\n−\\nx\\n2\\n)\\n+\\nmargin\\n)\\n\\\\text{loss}(x1, x2, y) = \\\\max(0, -y * (x1 - x2) + \\\\text{margin})\\nloss\\n(\\nx\\n1\\n,\\nx\\n2\\n,\\ny\\n)\\n=\\nmax\\n(\\n0\\n,\\n−\\ny\\n∗\\n(\\nx\\n1\\n−\\nx\\n2\\n)\\n+\\nmargin\\n)\\nParameters\\nmargin\\n(\\nfloat\\n,\\noptional\\n) – Has a default value of\\n0\\n0\\n0\\n.\\nsize_average\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default,'),\n",
       " Document(metadata={}, page_content=') – Deprecated (see\\nreduction\\n). By default,\\nthe losses are averaged over each loss element in the batch. Note that for\\nsome losses, there are multiple elements per sample. If the field\\nsize_average\\nis set to\\nFalse\\n, the losses are instead summed for each minibatch. Ignored\\nwhen\\nreduce\\nis\\nFalse\\n. Default:\\nTrue\\nreduce\\n(\\nbool\\n,\\noptional\\n) – Deprecated (see\\nreduction\\n). By default, the\\nlosses are averaged or summed over observations for each minibatch depending\\non\\nsize_average\\n. When\\nreduce\\nis'),\n",
       " Document(metadata={}, page_content=\"on\\nsize_average\\n. When\\nreduce\\nis\\nFalse\\n, returns a loss per\\nbatch element instead and ignores\\nsize_average\\n. Default:\\nTrue\\nreduction\\n(\\nstr\\n,\\noptional\\n) – Specifies the reduction to apply to the output:\\n'none'\\n|\\n'mean'\\n|\\n'sum'\\n.\\n'none'\\n: no reduction will be applied,\\n'mean'\\n: the sum of the output will be divided by the number of\\nelements in the output,\\n'sum'\\n: the output will be summed. Note:\\nsize_average\\nand\\nreduce\\nare in the process of being deprecated, and in the meantime,\"),\n",
       " Document(metadata={}, page_content=\"specifying either of those two args will override\\nreduction\\n. Default:\\n'mean'\\nShape:\\nInput1:\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\n)\\n()\\n(\\n)\\nwhere\\nN\\nis the batch size.\\nInput2:\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\n)\\n()\\n(\\n)\\n, same shape as the Input1.\\nTarget:\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\nor\\n(\\n)\\n()\\n(\\n)\\n, same shape as the inputs.\\nOutput: scalar. If\\nreduction\\nis\\n'none'\\nand Input size is not\\n(\\n)\\n()\\n(\\n)\\n, then\\n(\\nN\\n)\\n(N)\\n(\\nN\\n)\\n.\\nExamples:\\n>>>\\nloss\\n=\\nnn\\n.\\nMarginRankingLoss\\n()\\n>>>\\ninput1\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ninput2\\n=\"),\n",
       " Document(metadata={}, page_content='.\\nrandn\\n(\\n3\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ninput2\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n,\\nrequires_grad\\n=\\nTrue\\n)\\n>>>\\ntarget\\n=\\ntorch\\n.\\nrandn\\n(\\n3\\n)\\n.\\nsign\\n()\\n>>>\\noutput\\n=\\nloss\\n(\\ninput1\\n,\\ninput2\\n,\\ntarget\\n)\\n>>>\\noutput\\n.\\nbackward\\n()\\nNext\\nPrevious\\n© Copyright 2024, PyTorch Contributors.\\nBuilt with\\nSphinx\\nusing a\\ntheme\\nprovided by\\nRead the Docs\\n.'),\n",
       " Document(metadata={}, page_content=\"HingeEmbeddingLoss\\n¶\\nclass\\ntorch.nn.\\nHingeEmbeddingLoss\\n(\\nmargin\\n=\\n1.0\\n,\\nsize_average\\n=\\nNone\\n,\\nreduce\\n=\\nNone\\n,\\nreduction\\n=\\n'mean'\\n)\\n[source]\\n[source]\\n¶\\nMeasures the loss given an input tensor\\nx\\nx\\nx\\nand a labels tensor\\ny\\ny\\ny\\n(containing 1 or -1).\\nThis is usually used for measuring whether two inputs are similar or\\ndissimilar, e.g. using the L1 pairwise distance as\\nx\\nx\\nx\\n, and is typically\\nused for learning nonlinear embeddings or semi-supervised learning.\\nThe loss function for\\nn\\nn\\nn\"),\n",
       " Document(metadata={}, page_content='The loss function for\\nn\\nn\\nn\\n-th sample in the mini-batch is\\nl\\nn\\n=\\n{\\nx\\nn\\n,\\nif\\ny\\nn\\n=\\n1\\n,\\nmax\\n\\u2061\\n{\\n0\\n,\\nm\\na\\nr\\ng\\ni\\nn\\n−\\nx\\nn\\n}\\n,\\nif\\ny\\nn\\n=\\n−\\n1\\n,\\nl_n = \\\\begin{cases}\\n    x_n, & \\\\text{if}\\\\; y_n = 1,\\\\\\\\\\n    \\\\max \\\\{0, margin - x_n\\\\}, & \\\\text{if}\\\\; y_n = -1,\\n\\\\end{cases}\\nl\\nn\\n\\u200b\\n=\\n{\\nx\\nn\\n\\u200b\\n,\\nmax\\n{\\n0\\n,\\nma\\nr\\ng\\nin\\n−\\nx\\nn\\n\\u200b\\n}\\n,\\n\\u200b\\nif\\ny\\nn\\n\\u200b\\n=\\n1\\n,\\nif\\ny\\nn\\n\\u200b\\n=\\n−\\n1\\n,\\n\\u200b\\nand the total loss functions is\\nℓ\\n(\\nx\\n,\\ny\\n)\\n=\\n{\\nmean\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘mean’;\\nsum\\n\\u2061\\n(\\nL\\n)\\n,\\nif\\xa0reduction\\n=\\n‘sum’.\\n\\\\ell(x, y) = \\\\begin{cases}'),\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phani\\AppData\\Local\\Temp\\ipykernel_37392\\4070198614.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Generate embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phani\\miniconda3\\envs\\rag_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Checkpoint for Vector Store\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "vectorstore.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved Vector Store\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embedding_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
